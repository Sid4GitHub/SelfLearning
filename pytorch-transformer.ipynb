{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Un8vCXATDgEW"
      },
      "outputs": [],
      "source": [
        "# CELL - 1\n",
        "import torch # Imports the PyTorch library, a popular deep learning framework.\n",
        "import torch.nn as nn # Imports the neural network module from PyTorch for building layers.\n",
        "import torch.optim as optim # Imports optimization algorithms like Adam, SGD, etc.\n",
        "import torch.nn.functional as F # Imports functional layers like activation functions, pooling, etc.\n",
        "import pandas as pd # Imports pandas for data manipulation and analysis.\n",
        "import numpy as np # Imports numpy for numerical operations, especially array manipulation.\n",
        "import math # Imports the math module for mathematical functions.\n",
        "import matplotlib.pyplot as plt # Imports matplotlib for plotting and visualization.\n",
        "from torch.utils.data import Dataset, DataLoader # Imports tools for creating custom datasets and efficient data loading.\n",
        "from collections import Counter # Imports Counter for counting hashable objects.\n",
        "import pickle # Imports pickle for serializing and de-serializing Python object structures.\n",
        "import os # Imports the os module for interacting with the operating system, like file paths.\n",
        "from tqdm import tqdm # Imports tqdm for displaying progress bars during loops.\n",
        "import time # Imports the time module for time-related functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "639lJhOkDhN7",
        "outputId": "486fe14b-52b1-43fa-d2e7-1ccdce7c95e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Configuration: {'vocab_size': 20000, 'd_model': 512, 'dff': 2048, 'num_heads': 8, 'num_encoder_layers': 6, 'num_decoder_layers': 6, 'dropout_rate': 0.1, 'max_length': 200, 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 300, 'apply_early_stop': True, 'patience': 3, 'max_sentences': 39000, 'device': 'cuda'}\n"
          ]
        }
      ],
      "source": [
        "# CELL - 2\n",
        "# Configuration dictionary - all hyperparameters in one place\n",
        "CONFIG = {\n",
        "    'vocab_size': 18000,        # Vocabulary size for both source and target\n",
        "    'd_model': 512,             # Model dimension (embedding size)\n",
        "    'dff': 2048,                # Feed-forward network dimension\n",
        "    'num_heads': 8,             # Number of attention heads\n",
        "    'num_encoder_layers': 6,     # Number of encoder layers\n",
        "    'num_decoder_layers': 6,     # Number of decoder layers\n",
        "    'dropout_rate': 0.1,        # Dropout rate\n",
        "    'max_length': 200,          # Maximum sequence length\n",
        "    'batch_size': 32,           # Batch size for training\n",
        "    'learning_rate': 0.0001,    # Learning rate\n",
        "    'epochs': 300,               # Number of training epochs\n",
        "    'apply_early_stop': True,\n",
        "    'patience': 3,              # Early stopping patience\n",
        "    'max_sentences': 39000,    # Maximum number of sentences to read from CSV\n",
        "    #'device': 'mps' if torch.backends.mps.is_available() else 'cpu'  # Use MPS on Mac M4\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n",
        "}\n",
        "\n",
        "print(f\"Using device: {CONFIG['device']}\")\n",
        "print(f\"Configuration: {CONFIG}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3iSA66kTD52V"
      },
      "outputs": [],
      "source": [
        "# CELL - 3\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Vocabulary class to handle word-to-index and index-to-word mappings\n",
        "    This is essential for converting text to numbers that the model can process\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n",
        "        self.word_count = Counter() #  The Counter object is a specialized dictionary subclass for counting hashable object\n",
        "\n",
        "    def build_vocab(self, sentences, max_vocab_size):\n",
        "        \"\"\"Build vocabulary from list of sentences\"\"\"\n",
        "        # Count word frequencies\n",
        "        for sentence in sentences:\n",
        "            self.word_count.update(sentence.split())\n",
        "\n",
        "        # Get most common words\n",
        "        most_common = self.word_count.most_common(max_vocab_size - 4)  # -4 for special tokens\n",
        "\n",
        "        # Add to vocabulary\n",
        "        for word, _ in most_common:\n",
        "            if word not in self.word2idx:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        \"\"\"Convert sentence to list of indices\"\"\"\n",
        "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in sentence.split()]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert list of indices back to sentence\"\"\"\n",
        "        return ' '.join([self.idx2word.get(idx, '<UNK>') for idx in indices])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hAYqsYnxEDKF"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Encoding adds position information to embeddings\n",
        "    Since transformers don't have inherent position awareness like RNNs,\n",
        "    we need to explicitly add position information\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_length=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1) #\n",
        "\n",
        "        # Create div_term for sine and cosine functions\n",
        "        # We can implement Original paper uses 1 / (10000^(2*i/d_model))\n",
        "        # This is equivalent to exp( -(2*i/d_model) * log(10000) )this directly: (a^x=e^(ln(a)))\n",
        "        # torch.arange(0, d_model, 2).float()  = [0., 2., 4., ..., dmodel - 1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even positions and cosine to odd positions\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # select all rows, and select columns starting from index 0, going to the end, with a step of 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension and register as buffer (not a parameter)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        # its shape becomes [1, max_length, d_model].\n",
        "         #This is done to add a \"batch\" dimension, even though the positional encoding is typically applied to each item in a batch identically.\n",
        "         #The subsequent .transpose(0, 1) then changes the shape to [max_length, 1, d_model].\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Add positional encoding to input embeddings\"\"\"\n",
        "        return x + self.pe[:x.size(0), :] # pos_encoding[:seq_len] slices the positional encodings to match the input length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F9Nj3y_WKJU1"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention mechanism - the core of the transformer\n",
        "    It allows the model to attend to different parts of the sequence simultaneously\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Dimension of each head\n",
        "\n",
        "        # Linear transformations for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Compute scaled dot-product attention\n",
        "        Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
        "        \"\"\"\n",
        "        # K tensor has a shape of [batch_size, num_heads, sequence_length, d_k]\n",
        "        # K.transpose(-2, -1) is swapping the second-to-last dimension (sequence_length) and the last dimension (d_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (for padding or future positions)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax\n",
        "        #applying softmax along the last dimension means that for each query position, each head and each sequence\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear transformations and split into heads\n",
        "        # (batch_size, seq_len, d_model) → (batch_size, seq_len, num_heads, d_k) → (batch_size, num_heads, seq_len, d_k)\n",
        "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply attention\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Concatenate heads\n",
        "        # (batch_size, num_heads, seq_len, d_k) → (batch_size, seq_len, num_heads, d_k) → (batch_size, seq_len, d_model)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear transformation\n",
        "        output = self.W_o(attention_output)\n",
        "        return output\n",
        "\n",
        "# Linear weights (W_q, W_k, W_v, W_o) are [d_model, d_model],\n",
        "# while Q, K, and V before attention are [batch_size, num_heads, sequence_length, d_k],\n",
        "# the final output is [batch_size, sequence_length, d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qnHpzdI3RBS4"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feed-Forward Network\n",
        "    Two linear transformations with ReLU activation in between\n",
        "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, dff):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, dff)\n",
        "        self.linear2 = nn.Linear(dff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bj95JmGORZ1y"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Encoder Layer containing:\n",
        "    1. Multi-head self-attention\n",
        "    2. Residual connection + Layer normalization\n",
        "    3. Feed-forward network\n",
        "    4. Residual connection + Layer normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForwardNetwork(d_model, dff)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-head attention + residual connection + layer norm\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Residual connection\n",
        "\n",
        "        # Feed-forward network + residual connection + layer norm\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8cIN5EbeR4jv"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Encoder consisting of:\n",
        "    1. Input embedding\n",
        "    2. Positional encoding\n",
        "    3. Stack of encoder layers\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dff, max_length, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_length)\n",
        "        self.enc_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # Embedding + positional encoding\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)  # Scale embeddings\n",
        "        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through encoder layers\n",
        "        for enc_layer in self.enc_layers:\n",
        "            x = enc_layer(x, mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tvs4P8filA39"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Decoder Layer containing:\n",
        "    1. Multi-head self-attention (masked)\n",
        "    2. Residual connection + Layer normalization\n",
        "    3. Multi-head cross-attention with encoder output\n",
        "    4. Residual connection + Layer normalization\n",
        "    5. Feed-forward network\n",
        "    6. Residual connection + Layer normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Self-attention\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Cross-attention\n",
        "        self.ffn = FeedForwardNetwork(d_model, dff)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        # Masked self-attention + residual connection + layer norm\n",
        "        attn1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        # Cross-attention + residual connection + layer norm\n",
        "        attn2 = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "        # Feed-forward network + residual connection + layer norm\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "        return out3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nTnWTeNvZP9m"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Decoder consisting of:\n",
        "    1. Target embedding\n",
        "    2. Positional encoding\n",
        "    3. Stack of decoder layers\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dff, max_length, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_length)\n",
        "        self.dec_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # Embedding + positional encoding\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        for dec_layer in self.dec_layers:\n",
        "            x = dec_layer(x, enc_output, look_ahead_mask, padding_mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rCeMQZD-ZhkN"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer model for sequence-to-sequence translation\n",
        "    Combines encoder and decoder with final linear layer for vocabulary prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads,\n",
        "                 dff, max_length, dropout_rate):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads,\n",
        "                              dff, max_length, dropout_rate)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads,\n",
        "                              dff, max_length, dropout_rate)\n",
        "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.device = torch.device(CONFIG['device'])\n",
        "\n",
        "    def create_padding_mask(self, seq):\n",
        "        \"\"\"Create padding mask to ignore padding tokens\"\"\"\n",
        "        return (seq != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    def create_look_ahead_mask(self, size):\n",
        "        \"\"\"Create look-ahead mask to prevent seeing future tokens during training\"\"\"\n",
        "        mask = torch.tril(torch.ones(size, size)).bool()\n",
        "        return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    def forward(self, src, tgt, training=True):\n",
        "        # Create masks\n",
        "        src_mask = self.create_padding_mask(src)\n",
        "\n",
        "        \"\"\"\n",
        "        if training:\n",
        "            tgt_seq_len = tgt.size(1)\n",
        "            look_ahead_mask = self.create_look_ahead_mask(tgt_seq_len).to(self.device)\n",
        "            tgt_padding_mask = self.create_padding_mask(tgt).to(self.device)\n",
        "            combined_mask = torch.max(look_ahead_mask, ~tgt_padding_mask)\n",
        "        else:\n",
        "            #combined_mask = None\n",
        "            tgt_seq_len = tgt.size(1)\n",
        "            look_ahead_mask = self.create_look_ahead_mask(tgt_seq_len).to(self.device)\n",
        "            combined_mask = look_ahead_mask\n",
        "        \"\"\"\n",
        "\n",
        "        # Create target padding mask\n",
        "        tgt_padding_mask = self.create_padding_mask(tgt).to(self.device)\n",
        "\n",
        "        # Create look-ahead mask for target sequence\n",
        "        tgt_seq_len = tgt.size(1)\n",
        "        look_ahead_mask = self.create_look_ahead_mask(tgt_seq_len).to(self.device)\n",
        "\n",
        "        # Combine look-ahead and padding masks for decoder self-attention\n",
        "        combined_mask = torch.logical_and(look_ahead_mask, tgt_padding_mask)\n",
        "\n",
        "        # Encoder\n",
        "        enc_output = self.encoder(src, src_mask)\n",
        "\n",
        "        # Decoder\n",
        "        dec_output = self.decoder(tgt, enc_output, combined_mask, src_mask)\n",
        "\n",
        "        # Final linear layer\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Vyn-08aOiVGN"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset class for handling translation data\n",
        "    Efficiently loads and processes data in batches\n",
        "    \"\"\"\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_length):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sentence = self.src_sentences[idx]\n",
        "        tgt_sentence = self.tgt_sentences[idx]\n",
        "\n",
        "        # Encode sentences\n",
        "        src_encoded = self.src_vocab.encode(src_sentence)\n",
        "        #tgt_encoded = [self.tgt_vocab.word2idx['<SOS>']] + self.tgt_vocab.encode(tgt_sentence) + [self.tgt_vocab.word2idx['<EOS>']]\n",
        "        tgt_encoded_ip = [self.tgt_vocab.word2idx['<SOS>']] + self.tgt_vocab.encode(tgt_sentence)\n",
        "        tgt_encoded_op = self.tgt_vocab.encode(tgt_sentence) + [self.tgt_vocab.word2idx['<EOS>']]\n",
        "\n",
        "\n",
        "        # Truncate if too long\n",
        "        if len(src_encoded) > self.max_length:\n",
        "            src_encoded = src_encoded[:self.max_length]\n",
        "        if len(tgt_encoded_ip) > self.max_length:\n",
        "            tgt_encoded_ip = tgt_encoded_ip[:self.max_length]\n",
        "        if len(tgt_encoded_op) > self.max_length:\n",
        "            tgt_encoded_op = tgt_encoded_op[:self.max_length]\n",
        "\n",
        "\n",
        "        # Pad sequences\n",
        "        src_padded = src_encoded + [0] * (self.max_length - len(src_encoded))\n",
        "        tgt_encoded_ip = tgt_encoded_ip + [0] * (self.max_length - len(tgt_encoded_ip))\n",
        "        tgt_encoded_op = tgt_encoded_op + [0] * (self.max_length - len(tgt_encoded_op))\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_padded, dtype=torch.long),\n",
        "            'tgt_ip': torch.tensor(tgt_encoded_ip, dtype=torch.long),\n",
        "            'tgt_op': torch.tensor(tgt_encoded_op, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HPPw6BLoimOi"
      },
      "outputs": [],
      "source": [
        "def load_data_efficiently(file_name, max_sentences):\n",
        "    \"\"\"\n",
        "    Efficiently load data from CSV file in chunks to handle large datasets\n",
        "    This prevents memory issues with very large files\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {file_name}...\")\n",
        "\n",
        "    # Read data in chunks to handle large files\n",
        "    chunk_size = 10000\n",
        "    english_sentences = []\n",
        "    bengali_sentences = []\n",
        "\n",
        "    try:\n",
        "        # Read CSV in chunks\n",
        "        chunk_iter = pd.read_csv(file_name, chunksize=chunk_size)\n",
        "        total_loaded = 0\n",
        "\n",
        "        for chunk in chunk_iter:\n",
        "            if total_loaded >= max_sentences:\n",
        "                break\n",
        "\n",
        "            # Filter out NaN values and empty strings\n",
        "            chunk = chunk.dropna()\n",
        "            chunk = chunk[chunk['en'].str.len() > 0]\n",
        "            chunk = chunk[chunk['bn'].str.len() > 0]\n",
        "\n",
        "            # Add to lists\n",
        "            remaining = max_sentences - total_loaded\n",
        "            chunk_to_add = min(len(chunk), remaining)\n",
        "\n",
        "            english_sentences.extend(chunk['en'].iloc[:chunk_to_add].tolist())\n",
        "            bengali_sentences.extend(chunk['bn'].iloc[:chunk_to_add].tolist())\n",
        "\n",
        "            total_loaded += chunk_to_add\n",
        "            print(f\"Loaded {total_loaded} sentences...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return [], []\n",
        "\n",
        "    print(f\"Successfully loaded {len(english_sentences)} sentence pairs\")\n",
        "    #print(f\"English sentences: {english_sentences[:5]}\")\n",
        "    #print(f\"Bengali sentences: {bengali_sentences[:5]}\")\n",
        "    return english_sentences, bengali_sentences\n",
        "\n",
        "def create_data_loaders(english_sentences, bengali_sentences, src_vocab, tgt_vocab, config):\n",
        "    \"\"\"Create train and validation data loaders\"\"\"\n",
        "    # Split data into train and validation\n",
        "    split_idx = int(0.9 * len(english_sentences))\n",
        "\n",
        "    train_src = english_sentences[:split_idx]\n",
        "    train_tgt = bengali_sentences[:split_idx]\n",
        "    val_src = english_sentences[split_idx:]\n",
        "    val_tgt = bengali_sentences[split_idx:]\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TranslationDataset(train_src, train_tgt, src_vocab, tgt_vocab, config['max_length'])\n",
        "    val_dataset = TranslationDataset(val_src, val_tgt, src_vocab, tgt_vocab, config['max_length'])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YaDzgI3Hi0vu"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stopping to prevent overfitting\n",
        "    Stops training when validation loss stops improving\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=5, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "def train_model(model, train_loader, val_loader, config):\n",
        "    \"\"\"\n",
        "    Train the transformer model with early stopping\n",
        "    \"\"\"\n",
        "    device = torch.device(config['device'])\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(patience=config['patience'])\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(config['epochs']):\n",
        "\n",
        "        time_elapse = time.time() - start_time\n",
        "        print(f\"Time elapsed: {time_elapse // 60:.0f}m {time_elapse % 60:.0f}s\")\n",
        "\n",
        "        if time_elapse > 1 * (60 * 60):\n",
        "            print(\"Time limit exceeded. Stopping training.\")\n",
        "            break\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "\n",
        "        # Training loop\n",
        "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
        "            src = batch['src'].to(device)\n",
        "            tgt_ip = batch['tgt_ip'].to(device)\n",
        "            tgt_op = batch['tgt_op'].to(device)\n",
        "\n",
        "            #print(\"\\nsrc:\", src)\n",
        "            #print(\"tgt_input:\", tgt_ip)\n",
        "            #print(\"tgt_real:\", tgt_op)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(src, tgt_ip, training=True)\n",
        "\n",
        "            #print(\"\\npredictions:\", predictions)\n",
        "\n",
        "            #print(\"\")\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(predictions.reshape(-1, predictions.size(-1)), tgt_op.reshape(-1))\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                src = batch['src'].to(device)\n",
        "\n",
        "                tgt_ip = batch['tgt_ip'].to(device)\n",
        "                tgt_op = batch['tgt_op'].to(device)\n",
        "\n",
        "                predictions = model(src, tgt_ip, training=True)\n",
        "                loss = criterion(predictions.reshape(-1, predictions.size(-1)), tgt_op.reshape(-1))\n",
        "                epoch_val_loss += loss.item()\n",
        "\n",
        "        #print(\"len(train_loader): \", len(train_loader))\n",
        "        #print(\"len(val_loader):\", len(val_loader))\n",
        "\n",
        "        # Average losses\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['epochs']}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if config['apply_early_stop'] :\n",
        "          # Early stopping check\n",
        "          if early_stopping(avg_val_loss):\n",
        "              print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "              break\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hCD988qJjjZa"
      },
      "outputs": [],
      "source": [
        "class TranslationInference:\n",
        "    \"\"\"\n",
        "    Inference class for translating sentences using trained model\n",
        "    \"\"\"\n",
        "    def __init__(self, model, src_vocab, tgt_vocab, config):\n",
        "        self.model = model\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.config = config\n",
        "        self.device = torch.device(config['device'])\n",
        "        self.model.eval()\n",
        "\n",
        "    def translate(self, sentence, max_length=None):\n",
        "        \"\"\"\n",
        "        Translate a single English sentence to Bengali\n",
        "        \"\"\"\n",
        "        if max_length is None:\n",
        "            max_length = self.config['max_length']\n",
        "\n",
        "        # Encode source sentence\n",
        "        src_encoded = self.src_vocab.encode(sentence.lower().strip())\n",
        "        if len(src_encoded) > max_length:\n",
        "            src_encoded = src_encoded[:max_length]\n",
        "\n",
        "        # Pad source\n",
        "        src_padded = src_encoded + [0] * (max_length - len(src_encoded))\n",
        "        src_tensor = torch.tensor([src_padded], dtype=torch.long).to(self.device)\n",
        "\n",
        "        # Start with SOS token\n",
        "        tgt_input = [self.tgt_vocab.word2idx['<SOS>']]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                # Pad target input\n",
        "                tgt_padded = tgt_input + [0] * (max_length - len(tgt_input))\n",
        "                tgt_tensor = torch.tensor([tgt_padded], dtype=torch.long).to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                predictions = self.model(src_tensor, tgt_tensor, training=False)\n",
        "\n",
        "                # Get next token prediction\n",
        "                next_token_logits = predictions[0, len(tgt_input)-1, :]\n",
        "                next_token = torch.argmax(next_token_logits).item()\n",
        "\n",
        "                # Add to target input\n",
        "                tgt_input.append(next_token)\n",
        "\n",
        "                # Stop if EOS token is generated\n",
        "                if next_token == self.tgt_vocab.word2idx['<EOS>']:\n",
        "                    break\n",
        "\n",
        "        # Decode the result (excluding SOS and EOS tokens)\n",
        "        result_tokens = tgt_input[1:-1] if tgt_input[-1] == self.tgt_vocab.word2idx['<EOS>'] else tgt_input[1:]\n",
        "        translated_sentence = self.tgt_vocab.decode(result_tokens)\n",
        "\n",
        "        return translated_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSpasyaBkbyy",
        "outputId": "c0229505-ab58-43ff-fb18-6c38dae8a691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== English to Bengali Transformer Translation ===\n",
            "Configuration: {'vocab_size': 20000, 'd_model': 512, 'dff': 2048, 'num_heads': 8, 'num_encoder_layers': 6, 'num_decoder_layers': 6, 'dropout_rate': 0.1, 'max_length': 200, 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 300, 'apply_early_stop': True, 'patience': 3, 'max_sentences': 39000, 'device': 'cuda'}\n",
            "Loading data from english_to_bangla.csv...\n",
            "Loaded 10000 sentences...\n",
            "Loaded 20000 sentences...\n",
            "Loaded 30000 sentences...\n",
            "Loaded 39000 sentences...\n",
            "Successfully loaded 39000 sentence pairs\n",
            "Building vocabularies...\n",
            "Source vocabulary size: 8827\n",
            "Target vocabulary size: 17322\n",
            "Training samples: 35100\n",
            "Validation samples: 3900\n",
            "Creating transformer model...\n",
            "Total parameters: 66,376,106\n",
            "Trainable parameters: 66,376,106\n",
            "Starting training...\n",
            "Time elapsed: 0m 0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 1097/1097 [12:12<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "Train Loss: 4.8673, Val Loss: 4.0921\n",
            "Time elapsed: 12m 42s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2:  44%|████▍     | 487/1097 [05:29<06:51,  1.48it/s]"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Main function to orchestrate the entire training and inference process\n",
        "\"\"\"\n",
        "print(\"=== English to Bengali Transformer Translation ===\")\n",
        "print(f\"Configuration: {CONFIG}\")\n",
        "\n",
        "# Load data\n",
        "file_name = 'english_to_bangla.csv'\n",
        "english_sentences, bengali_sentences = load_data_efficiently(file_name, CONFIG['max_sentences'])\n",
        "\n",
        "if len(english_sentences) == 0:\n",
        "    print(\"No data loaded. Please check the CSV file.\")\n",
        "\n",
        "\n",
        "# Build vocabularies\n",
        "print(\"Building vocabularies...\")\n",
        "src_vocab = Vocabulary()\n",
        "tgt_vocab = Vocabulary()\n",
        "\n",
        "src_vocab.build_vocab(english_sentences, CONFIG['vocab_size'])\n",
        "tgt_vocab.build_vocab(bengali_sentences, CONFIG['vocab_size'])\n",
        "\n",
        "print(f\"Source vocabulary size: {len(src_vocab)}\")\n",
        "print(f\"Target vocabulary size: {len(tgt_vocab)}\")\n",
        "\n",
        "#print(f\"Source vocabulary : {src_vocab.idx2word}\")\n",
        "#print(f\"Target vocabulary: {tgt_vocab.idx2word}\")\n",
        "\n",
        "#print(\"english_sentences:\", english_sentences)\n",
        "#print(\"bengali_sentences\", bengali_sentences)\n",
        "# Create data loaders\n",
        "train_loader, val_loader = create_data_loaders(\n",
        "    english_sentences, bengali_sentences, src_vocab, tgt_vocab, CONFIG\n",
        ")\n",
        "\n",
        "# Create model\n",
        "print(\"Creating transformer model...\")\n",
        "model = Transformer(\n",
        "    src_vocab_size=len(src_vocab),\n",
        "    tgt_vocab_size=len(tgt_vocab),\n",
        "    d_model=CONFIG['d_model'],\n",
        "    num_layers=CONFIG['num_encoder_layers'],  # Same for both encoder and decoder\n",
        "    num_heads=CONFIG['num_heads'],\n",
        "    dff=CONFIG['dff'],\n",
        "    max_length=CONFIG['max_length'],\n",
        "    dropout_rate=CONFIG['dropout_rate']\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Train model\n",
        "trained_model, train_losses, val_losses = train_model(model, train_loader, val_loader, CONFIG)\n",
        "\n",
        "# Save model and vocabularies\n",
        "print(\"Saving model and vocabularies...\")\n",
        "torch.save({\n",
        "    'model_state_dict': trained_model.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'src_vocab': src_vocab,\n",
        "    'tgt_vocab': tgt_vocab\n",
        "}, 'en_bn_transformer.pth')\n",
        "\n",
        "# Create inference object\n",
        "translator = TranslationInference(trained_model, src_vocab, tgt_vocab, CONFIG)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLuH0AkXktOM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Test translations\n",
        "print(\"\\n=== Testing Translations ===\")\n",
        "test_sentences = [\n",
        "    \"a child in a pink dress is climbing up a set of stairs in an entry way .\"\n",
        "    ,\"a girl going into a wooden building .\"\n",
        "    ,\"a dog is running in the snow\"\n",
        "    ,\"a dog running\"\n",
        "    ,\"Hello, how are you?\"\n",
        "    ,\"a man in an orange hat starring at something .\"\n",
        "    ,\"I love you.\"\n",
        "    ,\"a little girl climbing into a wooden playhouse .\"\n",
        "    ,\"What is your name?\"\n",
        "    ,\"two dogs of different breeds looking at each other on the road .\"\n",
        "    ,\"Good morning.\"\n",
        "    ,\"Thank you very much.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    translation = translator.translate(sentence)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"Bengali: {translation}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"Training and inference completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
