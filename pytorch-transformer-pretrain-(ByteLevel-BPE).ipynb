{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "ldPna17ZgDqw",
        "outputId": "820b8fba-35e9-4b7d-d2de-211b88d703d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-176b84ca-b74f-430f-89c1-757e66f263b1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-176b84ca-b74f-430f-89c1-757e66f263b1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving English to Bengali For Machine Translation Pre-Train.zip to English to Bengali For Machine Translation Pre-Train.zip\n",
            "Archive:  English to Bengali For Machine Translation Pre-Train.zip\n",
            "  inflating: english_to_bangla.csv   \n",
            "  inflating: EBook_of_The_Bhagavad-Gita_Bengali.txt  \n",
            "  inflating: EBook_of_The_Bhagavad-Gita_English.txt  \n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Assuming only one file is uploaded\n",
        "file_name = list(uploaded.keys())[0]\n",
        "!unzip 'English to Bengali For Machine Translation Pre-Train.zip'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVdYf9HkzrSm",
        "outputId": "1aadfdbd-e4f7-4d3b-9211-11609312683b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: huggingface\n",
            "Successfully installed huggingface-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL - 1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "import re  # For text normalization\n",
        "import os\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence as NormalizerSequence\n",
        "import tempfile\n",
        "\n",
        "# =============================================================\n",
        "# English-to-Bengali Transformer: Pretraining & Fine-tuning Flow\n",
        "#\n",
        "# This script implements a full pipeline for training a transformer-based\n",
        "# English-to-Bengali translation model. It includes:\n",
        "#   - Data loading and normalization\n",
        "#   - BPE (ByteLevel) tokenizer training (shared or separate)\n",
        "#   - Transformer model definition (encoder, decoder, MLM heads)\n",
        "#   - Pretraining on monolingual data (MLM for encoder/decoder)\n",
        "#   - Fine-tuning on parallel translation pairs\n",
        "#   - Saving/loading checkpoints, plotting metrics, and inference\n",
        "#\n",
        "# The code is modular and can be adapted for other language pairs.\n",
        "# =============================================================\n",
        "\n",
        "# Configuration dictionary - all hyperparameters in one place\n",
        "CONFIG = {\n",
        "    'vocab_size': 18000,        # Maximum vocabulary size for both source and target languages\n",
        "    'd_model': 512,             # Model dimension (embedding size for each token)\n",
        "    'dff': 2048,                # Feed-forward network dimension (hidden size in FFN)\n",
        "    'num_heads': 8,             # Number of attention heads in multi-head attention\n",
        "    'num_encoder_layers': 6,    # Number of encoder layers (stacked)\n",
        "    'num_decoder_layers': 6,    # Number of decoder layers (stacked)\n",
        "    'dropout_rate': 0.1,        # Dropout rate for regularization\n",
        "    'max_length': 200,          # Maximum sequence length for input/output\n",
        "    'batch_size': 64,           # Batch size for training\n",
        "    'pretrain_learning_rate': 0.0001,    # Learning rate for pre-training\n",
        "    'finetune_learning_rate': 0.00005,   # Learning rate for fine-tuning (lower)\n",
        "    'pretrain_epochs': 500,               # Number of pre-training epochs\n",
        "    'finetune_epochs': 500,              # Number of fine-tuning epochs\n",
        "    'apply_early_stop': False,           # Whether to use early stopping\n",
        "    'patience': 5,                      # Early stopping patience (epochs)\n",
        "    'english_file': 'EBook_of_The_Bhagavad-Gita_English.txt',       # Path to English monolingual data\n",
        "    'bengali_file': 'EBook_of_The_Bhagavad-Gita_Bengali.txt',       # Path to Bengali monolingual data\n",
        "    'translation_file': 'english_to_bangla.csv',                    # Path to parallel translation data\n",
        "    'max_pretrain_sentences': 39000,    # Maximum sentences for pre-training (for quick test)\n",
        "    'max_translation_pairs': 39,      # Maximum translation pairs for fine-tuning (for quick test)\n",
        "    'mask_probability': 0.15,            # Probability of masking tokens during pre-training\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
        "    'max_train_minutes': 45,           # Maximum allowed training time (in minutes) for each phase\n",
        "    'max_global_minutes': 100,          # Maximum allowed total wall-clock time (in minutes) for the whole script\n",
        "    'shared_bpe_vocab': False,          # If True, use a single shared BPE vocabulary for both languages [Shared vocabulary might not be optimal for very different languages]\n",
        "    'warmup_steps': 4000,              # Warm-up steps for Noam learning-rate schedule\n",
        "    'tqdm_disable' : False\n",
        "}\n",
        "\n",
        "print(f\"Using device: {CONFIG['device']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nptLrMO_g7b2",
        "outputId": "8a33750d-e1b9-4e55-bddf-b9d6289673c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL - 2\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    WHAT THIS CLASS DOES:\n",
        "    Imagine you have a sentence like \"The cat sat on the mat\"\n",
        "    Each word gets converted to a list of numbers (called embeddings)\n",
        "    But the AI doesn't know the ORDER of words - \"cat sat\" vs \"sat cat\" look the same!\n",
        "    This class adds special \"position codes\" to tell the AI which word came first, second, etc.\n",
        "\n",
        "    It's like adding invisible timestamps to each word so the AI knows their sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_length=5000):\n",
        "        # d_model = how many numbers represent each word (like 512 numbers per word)\n",
        "        # max_length = maximum sentence length we can handle (like 5000 words max)\n",
        "\n",
        "        # This is Python's way of calling the parent class constructor\n",
        "        # Similar to calling super() in Java or C#\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a big table to store our position codes\n",
        "        # INPUT DIMENSIONS: We want [max_length, d_model] - like a spreadsheet\n",
        "        # Each row = position signature for that word position (0th word, 1st word, etc.)\n",
        "        # Each column = one dimension of the position encoding\n",
        "        pe = torch.zeros(max_length, d_model)  # OUTPUT: [5000, 512] matrix of zeros\n",
        "\n",
        "        # Create a list of position numbers: [0, 1, 2, 3, 4, ...]\n",
        "        # torch.arange is like range() in Python but creates a special array type\n",
        "        # INPUT: start=0, end=max_length\n",
        "        # OUTPUT before unsqueeze: [5000] - 1D array [0, 1, 2, 3, ..., 4999]\n",
        "        # OUTPUT after unsqueeze(1): [5000, 1] - 2D column [[0], [1], [2], [3], ...]\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # This is the mathematical \"secret sauce\" - creates different wave frequencies\n",
        "        # Think of it like creating different radio frequencies for each dimension\n",
        "        # INPUT: start=0, end=d_model, step=2 (so we get 0, 2, 4, 6, ...)\n",
        "        # OUTPUT before exp: [256] array if d_model=512 (every even index)\n",
        "        # OUTPUT after exp: [256] array of decreasing values (like [1.0, 0.9, 0.8, ...])\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Now we fill in our position codes using sine and cosine waves\n",
        "        # Why waves? Because they create smooth, predictable patterns that repeat\n",
        "        # This gives each position a unique \"fingerprint\" of numbers\n",
        "\n",
        "        # For even-numbered columns (0, 2, 4, 6, ...), use sine wave\n",
        "        # pe[:, 0::2] is Python slice notation: \"all rows, every 2nd column starting from 0\"\n",
        "        # INPUT: position [5000, 1] * div_term [256] = [5000, 256] via broadcasting\n",
        "        # OUTPUT: pe[:, 0::2] gets filled with [5000, 256] sine values\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # For odd-numbered columns (1, 3, 5, 7, ...), use cosine wave\n",
        "        # pe[:, 1::2] is Python slice notation: \"all rows, every 2nd column starting from 1\"\n",
        "        # INPUT: position [5000, 1] * div_term [256] = [5000, 256] via broadcasting\n",
        "        # OUTPUT: pe[:, 1::2] gets filled with [5000, 256] cosine values\n",
        "        # Now pe is completely filled: [5000, 512] with alternating sine/cosine columns\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Reshape our data to work with batches of sentences\n",
        "        # INPUT: pe is [5000, 512] (max_length, d_model)\n",
        "        # After unsqueeze(0): [1, 5000, 512] - adds batch dimension at front\n",
        "        # After transpose(0,1): [5000, 1, 512] - swaps first two dimensions\n",
        "        # OUTPUT: pe is now [5000, 1, 512] (seq_length, batch_size, d_model)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        # This tells PyTorch: \"save this data with the model, but don't try to learn/change it\"\n",
        "        # It's like marking a variable as 'final' or 'const' - it's part of the model but fixed\n",
        "        # INPUT: pe [5000, 1, 512]\n",
        "        # OUTPUT: self.pe is now a registered buffer [5000, 1, 512]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        This method gets called when we actually use the class\n",
        "\n",
        "        INPUT DIMENSIONS:\n",
        "        x = the input sentence (each word already converted to numbers)\n",
        "        x has shape: [sequence_length, batch_size, d_model]\n",
        "        Example: [20, 32, 512] means 20 words, 32 sentences in batch, 512 numbers per word\n",
        "\n",
        "        What we do: take our pre-calculated position codes and ADD them to each word\n",
        "        It's like adding a unique \"position stamp\" to each word's numeric representation\n",
        "        \"\"\"\n",
        "        # Extract position encodings for just the length we need\n",
        "        # INPUT: self.pe is [5000, 1, 512] but we only need first x.size(0) positions\n",
        "        # x.size(0) = sequence_length from input (like 20 in our example)\n",
        "        # OUTPUT: self.pe[:x.size(0), :] is [20, 1, 512] - just the positions we need\n",
        "\n",
        "        # The + operation adds our position codes to each word's numbers\n",
        "        # INPUT: x [20, 32, 512] + self.pe[:20, :] [20, 1, 512]\n",
        "        # Broadcasting happens: [20, 1, 512] gets expanded to [20, 32, 512]\n",
        "        # OUTPUT: [20, 32, 512] - same shape as input but with position info added\n",
        "        # self.pe[:x.size(0), :] = self.pe[:x.size(0)] = self.pe[:x.size(0), :, :] --> slicing upto the max words a sentence is having\n",
        "        return x + self.pe[:x.size(0), :]"
      ],
      "metadata": {
        "id": "mRq8pMlQhIBn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL - 3\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    WHAT THIS CLASS DOES:\n",
        "    Imagine you're reading a sentence and trying to understand each word.\n",
        "    For the word \"bank\", you might look at nearby words to understand if it means:\n",
        "    - \"river bank\" (look at words like \"water\", \"river\")\n",
        "    - \"money bank\" (look at words like \"deposit\", \"loan\")\n",
        "\n",
        "    This is \"attention\" - figuring out which other words are important for understanding each word.\n",
        "    \"Multi-head\" means we do this multiple times in parallel, like having multiple people\n",
        "    each focus on different types of relationships (grammar, meaning, context, etc.)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        # d_model = how many numbers represent each word (like 512)\n",
        "        # num_heads = how many parallel attention mechanisms (like 8)\n",
        "\n",
        "        # Call parent constructor (like super() in Java/C#)\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Safety check: d_model must be divisible by num_heads\n",
        "        # We'll split the word representation equally among heads\n",
        "        # Example: 512 dimensions ÷ 8 heads = 64 dimensions per head\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model      # Total dimensions (512)\n",
        "        self.num_heads = num_heads  # Number of attention heads (8)\n",
        "        self.d_k = d_model // num_heads  # Dimensions per head (64)\n",
        "\n",
        "        # Create 4 \"projection\" matrices - think of them as different colored glasses\n",
        "        # Each one transforms the input to focus on different aspects\n",
        "        # nn.Linear is like a matrix multiplication + optional bias\n",
        "        # INPUT for each: [batch_size, seq_len, d_model]\n",
        "        # OUTPUT for each: [batch_size, seq_len, d_model]\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)  # Query projection: \"what am I looking for?\"\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)  # Key projection: \"what do I represent?\"\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)  # Value projection: \"what information do I carry?\"\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)  # Output projection: \"combine all heads\"\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        This is the core attention mechanism - like a search engine!\n",
        "\n",
        "        INPUT DIMENSIONS:\n",
        "        Q (queries): [batch_size, num_heads, seq_len, d_k] - \"what each position is looking for\"\n",
        "        K (keys): [batch_size, num_heads, seq_len, d_k] - \"what each position offers\"\n",
        "        V (values): [batch_size, num_heads, seq_len, d_k] - \"actual information at each position\"\n",
        "        mask: optional [batch_size, seq_len, seq_len] - which positions to ignore\n",
        "\n",
        "        Think of it like a library:\n",
        "        - Q = your search query\n",
        "        - K = book titles/keywords\n",
        "        - V = actual book content\n",
        "        - We find books whose titles match your query, then return their content\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Calculate similarity scores between queries and keys\n",
        "        # Q: [batch_size, num_heads, seq_len, d_k]\n",
        "        # K.transpose(-2, -1): [batch_size, num_heads, d_k, seq_len] (flip last 2 dimensions)\n",
        "        # torch.matmul: [batch_size, num_heads, seq_len, d_k] × [batch_size, num_heads, d_k, seq_len]\n",
        "        # OUTPUT: scores [batch_size, num_heads, seq_len, seq_len]\n",
        "        # scores[i,j,a,b] = how much position 'a' should pay attention to position 'b'\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Step 2: Apply mask if provided (optional)\n",
        "        # Mask is used to ignore certain positions (like padding tokens)\n",
        "        # INPUT: scores [batch_size, num_heads, seq_len, seq_len]\n",
        "        # OUTPUT: scores with -1e9 (very negative) where mask == 0\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Step 3: Convert scores to probabilities using softmax\n",
        "        # This ensures attention weights sum to 1 for each query position\n",
        "        # INPUT: scores [batch_size, num_heads, seq_len, seq_len]\n",
        "        # OUTPUT: attention_weights [batch_size, num_heads, seq_len, seq_len]\n",
        "        # Each row sums to 1.0 (probability distribution)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Step 4: Use attention weights to get weighted average of values\n",
        "        # attention_weights: [batch_size, num_heads, seq_len, seq_len]\n",
        "        # V: [batch_size, num_heads, seq_len, d_k]\n",
        "        # OUTPUT: [batch_size, num_heads, seq_len, d_k]\n",
        "        # For each position, we get a weighted combination of all value vectors\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Main forward pass - processes input through multi-head attention\n",
        "\n",
        "        INPUT DIMENSIONS:\n",
        "        query: [batch_size, seq_len, d_model] - typically the same as key/value for self-attention\n",
        "        key: [batch_size, seq_len, d_model] - what we're searching through\n",
        "        value: [batch_size, seq_len, d_model] - the actual information\n",
        "        mask: optional masking ; [batch_size, seq_len, seq_len]\n",
        "\n",
        "        Example: batch_size=32, seq_len=20, d_model=512, num_heads=8\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size for reshaping operations\n",
        "        batch_size = query.size(0)  # Example: 32\n",
        "\n",
        "        # Step 1: Project inputs through learned transformations and split into heads\n",
        "        # Think of this as creating multiple \"views\" of the same data\n",
        "\n",
        "        # Apply query projection and reshape for multi-head processing\n",
        "        # INPUT: query [32, 20, 512]\n",
        "        # After W_q: [32, 20, 512] (linear transformation)\n",
        "        # After view: [32, 20, 8, 64] (split into 8 heads of 64 dims each)\n",
        "        # So self.W_q(query) = \"transform the input into a specialized query representation using learned weights\".[matrix multiplication]\n",
        "        # self.W_q(query) → [32, 20, 512] (batch_size, seq_len, d_model).view(batch_size, -1, self.num_heads, self.d_k) → [32, 20, 8, 64]\n",
        "        # What view does:\n",
        "        ## Takes the 512 features and splits them into 8 groups of 64\n",
        "        ## It's like taking a deck of 512 cards and organizing them into 8 piles of 64 cards each\n",
        "        ## The -1 means \"figure out this dimension automatically\" (it becomes 20 in this case)\n",
        "        # After transpose: [32, 8, 20, 64] (move heads to dimension 1)\n",
        "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply key projection and reshape (same process)\n",
        "        # INPUT: key [32, 20, 512] → OUTPUT: [32, 8, 20, 64]\n",
        "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply value projection and reshape (same process)\n",
        "        # INPUT: value [32, 20, 512] → OUTPUT: [32, 8, 20, 64]\n",
        "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Step 2: Apply attention mechanism to each head in parallel\n",
        "        # INPUT: Q, K, V each [32, 8, 20, 64]\n",
        "        # OUTPUT: attention_output [32, 8, 20, 64], attention_weights [32, 8, 20, 20]\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Step 3: Concatenate all heads back together\n",
        "        # INPUT: attention_output [32, 8, 20, 64]\n",
        "        # After transpose(1,2): [32, 20, 8, 64] (move sequence length back to position 1)\n",
        "        # After contiguous(): ensures memory is laid out properly for view operation\n",
        "        # After view: [32, 20, 512] (flatten the 8×64 back to 512)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.d_model)\n",
        "\n",
        "        # Step 4: Apply final linear transformation\n",
        "        # This lets the model learn how to best combine information from all heads\n",
        "        # INPUT: attention_output [32, 20, 512]\n",
        "        # OUTPUT: [32, 20, 512] (final result)\n",
        "        output = self.W_o(attention_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "N7T83f7Ur69_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL - 4\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    WHAT THIS CLASS DOES:\n",
        "    Think of this as a \"thinking layer\" that processes each word independently.\n",
        "    After attention figures out which words are related, this layer lets each word\n",
        "    \"think\" about what it learned from attending to other words.\n",
        "\n",
        "    It's like having a conversation where:\n",
        "    1. First you listen to everyone (attention phase)\n",
        "    2. Then you individually process what you heard (this feed-forward phase)\n",
        "\n",
        "    The architecture is simple but powerful:\n",
        "    - Expand: Give each word more \"thinking space\" (more numbers to work with)\n",
        "    - Process: Apply non-linear thinking (ReLU activation)\n",
        "    - Compress: Condense the thinking back to original size\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, dff):\n",
        "        # d_model = word representation size (like 512 numbers per word)\n",
        "        # dff = internal \"thinking\" size (like 2048 - typically 4x larger than d_model)\n",
        "\n",
        "        # Call parent constructor (like super() in Java/C#)\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "        # Create two linear transformation layers\n",
        "        # nn.Linear(input_size, output_size) creates a learnable matrix transformation\n",
        "        # It's like: output = input × weight_matrix + bias\n",
        "\n",
        "        # Layer 1: Expand from d_model to dff (give more thinking space)\n",
        "        # INPUT dimensions: [batch_size, seq_len, d_model]\n",
        "        # OUTPUT dimensions: [batch_size, seq_len, dff]\n",
        "        # Example: [32, 20, 512] → [32, 20, 2048]\n",
        "        self.linear1 = nn.Linear(d_model, dff)\n",
        "\n",
        "        # Layer 2: Compress from dff back to d_model (condense thinking)\n",
        "        # INPUT dimensions: [batch_size, seq_len, dff]\n",
        "        # OUTPUT dimensions: [batch_size, seq_len, d_model]\n",
        "        # Example: [32, 20, 2048] → [32, 20, 512]\n",
        "        self.linear2 = nn.Linear(dff, d_model)\n",
        "\n",
        "        # ReLU activation function: f(x) = max(0, x)\n",
        "        # This introduces non-linearity - without it, the two linear layers\n",
        "        # would just be equivalent to one linear layer (linear combinations of linear = linear)\n",
        "        # ReLU lets the network learn complex, non-linear patterns\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Process input through the feed-forward network\n",
        "\n",
        "        INPUT DIMENSIONS:\n",
        "        x: [batch_size, seq_len, d_model]\n",
        "        Example: [32, 20, 512] meaning 32 sentences, 20 words each, 512 numbers per word\n",
        "\n",
        "        This processes each word position independently - no interaction between words\n",
        "        (that's what attention layers are for)\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Expand the representation (give more thinking space)\n",
        "        # INPUT: x [32, 20, 512]\n",
        "        # OPERATION: matrix multiply with learned weights [512, 2048] + bias [2048]\n",
        "        # OUTPUT: [32, 20, 2048] - each word now has 2048 numbers instead of 512\n",
        "        expanded = self.linear1(x)\n",
        "\n",
        "        # Step 2: Apply ReLU activation (non-linear thinking)\n",
        "        # INPUT: expanded [32, 20, 2048] (can have negative values)\n",
        "        # OPERATION: f(x) = max(0, x) - set all negative values to 0\n",
        "        # OUTPUT: [32, 20, 2048] (all values >= 0)\n",
        "        # This allows the network to learn complex patterns and make decisions\n",
        "        activated = self.relu(expanded)\n",
        "\n",
        "        # Step 3: Compress back to original size (condense the thinking)\n",
        "        # INPUT: activated [32, 20, 2048]\n",
        "        # OPERATION: matrix multiply with learned weights [2048, 512] + bias [512]\n",
        "        # OUTPUT: [32, 20, 512] - back to original word representation size\n",
        "        final_output = self.linear2(activated)\n",
        "\n",
        "        # The one-liner version of the above three steps:\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "        # INTUITION:\n",
        "        # This is like asking each word to \"think harder\" about what it learned\n",
        "        # from attention, then summarize that thinking back to its original form.\n",
        "        # The expansion→activation→compression allows for complex non-linear processing\n",
        "        # that wouldn't be possible with just linear transformations."
      ],
      "metadata": {
        "id": "Yd5Jv6z3mJEU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL - 5\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    WHAT THIS CLASS DOES:\n",
        "    This is like one complete \"conversation round\" in a group discussion.\n",
        "    In each round, everyone:\n",
        "    1. Listens to everyone else and updates their understanding (attention)\n",
        "    2. Thinks individually about what they learned (feed-forward)\n",
        "    3. Keeps some of their original thoughts (residual connections)\n",
        "    4. Normalizes their thinking to stay balanced (layer normalization)\n",
        "    5. Occasionally \"zones out\" to prevent overthinking (dropout)\n",
        "\n",
        "    Multiple encoder layers are stacked to create deeper understanding,\n",
        "    like having multiple rounds of discussion where insights build up.\n",
        "\n",
        "    The architecture follows: Input → Attention → Add & Norm → FeedForward → Add & Norm → Output\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        # d_model = word representation size (like 512)\n",
        "        # num_heads = number of attention heads (like 8)\n",
        "        # dff = feed-forward internal size (like 2048)\n",
        "        # dropout_rate = probability of \"zoning out\" (like 0.1 = 10% chance)\n",
        "\n",
        "        # Call parent constructor\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Component 1: Multi-head attention (the \"listening\" phase)\n",
        "        # INPUT/OUTPUT: [batch_size, seq_len, d_model]\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Component 2: Feed-forward network (the \"individual thinking\" phase)\n",
        "        # INPUT/OUTPUT: [batch_size, seq_len, d_model]\n",
        "        self.ffn = FeedForwardNetwork(d_model, dff)\n",
        "\n",
        "        # Component 3: Layer normalization (keeps values in reasonable range)\n",
        "        # Think of this like \"staying calm and balanced\" after each processing step\n",
        "        # INPUT/OUTPUT: [batch_size, seq_len, d_model] (same shape, just normalized)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)  # After attention\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)  # After feed-forward\n",
        "\n",
        "        # Component 4: Dropout (randomly \"forget\" some information to prevent overfitting)\n",
        "        # This is like occasionally zoning out to avoid overthinking\n",
        "        # INPUT/OUTPUT: [batch_size, seq_len, d_model] (same shape, some values → 0)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)  # After attention\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)  # After feed-forward\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Process input through one encoder layer\n",
        "\n",
        "        INPUT DIMENSIONS:\n",
        "        x: [batch_size, seq_len, d_model]\n",
        "        Example: [32, 20, 512] - 32 sentences, 20 words each, 512 numbers per word\n",
        "        mask: optional [batch_size, seq_len, seq_len] - which positions to ignore\n",
        "\n",
        "        OUTPUT DIMENSIONS:\n",
        "        [batch_size, seq_len, d_model] - same shape as input but with richer representations\n",
        "        \"\"\"\n",
        "\n",
        "        # PHASE 1: ATTENTION (Listen to everyone)\n",
        "        # =====================================\n",
        "\n",
        "        # Step 1: Apply multi-head self-attention\n",
        "        # INPUT: x [32, 20, 512] used as query, key, AND value (self-attention)\n",
        "        # OUTPUT: attn_output [32, 20, 512] - each word updated based on all words\n",
        "        # This is where words \"listen\" to each other and update their understanding\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "\n",
        "        # Step 2: Apply dropout (randomly zero out some values during training)\n",
        "        # INPUT: attn_output [32, 20, 512]\n",
        "        # OUTPUT: attn_output [32, 20, 512] (some values randomly set to 0)\n",
        "        # This prevents the model from becoming too dependent on specific patterns\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "\n",
        "        # Step 3: Add & Norm (residual connection + layer normalization)\n",
        "        # INPUT: x [32, 20, 512] + attn_output [32, 20, 512]\n",
        "        # First ADD: [32, 20, 512] (combines original input with attention output)\n",
        "        # Then NORMALIZE: [32, 20, 512] (keeps values in reasonable range)\n",
        "        #\n",
        "        # WHY ADD THE ORIGINAL INPUT?\n",
        "        # - Helps with gradient flow (technical: prevents vanishing gradients)\n",
        "        # - Keeps some original information (don't lose what you already knew)\n",
        "        # - Like saying \"keep your original thoughts but add what you learned\"\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # PHASE 2: FEED-FORWARD (Individual thinking)\n",
        "        # ==========================================\n",
        "\n",
        "        # Step 4: Apply feed-forward network (individual processing)\n",
        "        # INPUT: out1 [32, 20, 512]\n",
        "        # OUTPUT: ffn_output [32, 20, 512] - each word processed independently\n",
        "        # This is where each word \"thinks\" about what it learned from attention\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "        # Step 5: Apply dropout again\n",
        "        # INPUT: ffn_output [32, 20, 512]\n",
        "        # OUTPUT: ffn_output [32, 20, 512] (some values randomly set to 0)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "\n",
        "        # Step 6: Add & Norm again (second residual connection)\n",
        "        # INPUT: out1 [32, 20, 512] + ffn_output [32, 20, 512]\n",
        "        # First ADD: [32, 20, 512] (combines attention output with feed-forward output)\n",
        "        # Then NORMALIZE: [32, 20, 512] (final normalization)\n",
        "        #\n",
        "        # Again, we keep the previous state and add the new learning\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        # FINAL OUTPUT: [32, 20, 512] - same shape as input but with enriched representations\n",
        "        return out2\n",
        "\n",
        "        # INTUITION:\n",
        "        # Input words → Listen to each other → Keep original + learned info →\n",
        "        # Think individually → Keep previous + new thoughts → Output richer representations\n",
        "        #\n",
        "        # Each layer builds upon the previous understanding, like multiple rounds\n",
        "        # of discussion where insights accumulate and deepen."
      ],
      "metadata": {
        "id": "Fk7QdvM-mN53"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The encoder stack: input embedding + positional encoding + N encoder layers.\n",
        "\n",
        "    This is like a text processing pipeline that converts words into mathematical\n",
        "    representations and then refines them through multiple layers of analysis.\n",
        "    Think of it as a sophisticated text analyzer that builds understanding by\n",
        "    looking at words in context with each other.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dff, max_length, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Step 1: Word-to-Vector Converter\n",
        "        # Takes word IDs (integers) and converts them to dense vectors\n",
        "        # Input: integers in range [0, vocab_size-1]\n",
        "        # Output: vectors of size d_model (e.g., 512 dimensions)\n",
        "        # Like a lookup table: word_id -> dense_vector\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Step 2: Position Information Injector\n",
        "        # Adds \"where am I in the sentence?\" information to each word vector\n",
        "        # Input: vectors of size d_model\n",
        "        # Output: same vectors but now with position info mixed in\n",
        "        # This helps the model understand word order (crucial for meaning)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        # Step 3: Stack of Processing Layers\n",
        "        # Creates a list of identical processing units (like assembly line stations)\n",
        "        # Each layer refines the understanding by looking at relationships between words\n",
        "        # Input to each layer: vectors of size d_model\n",
        "        # Output from each layer: refined vectors of same size d_model\n",
        "        self.enc_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "            for _ in range(num_layers)  # Fixed syntax error: was \"for * in range(num*layers)\"\n",
        "        ])\n",
        "\n",
        "        # Step 4: Regularization (Prevents Overfitting)\n",
        "        # Randomly sets some vector elements to zero during training\n",
        "        # This forces the model to be more robust and not rely on specific patterns\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Process a batch of sentences through the encoder pipeline.\n",
        "\n",
        "        Input dimensions: x has shape [batch_size, sequence_length]\n",
        "        - batch_size: how many sentences we're processing at once\n",
        "        - sequence_length: how many words in each sentence\n",
        "        - Each element is an integer (word ID from vocabulary)\n",
        "\n",
        "        Output dimensions: [batch_size, sequence_length, d_model]\n",
        "        - Same batch_size and sequence_length as input\n",
        "        - Each word now represented as a d_model-dimensional vector\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the length of our input sequences\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # STEP 1: Convert word IDs to dense vectors\n",
        "        # Input: [batch_size, seq_len] - integers\n",
        "        # Output: [batch_size, seq_len, d_model] - floating point vectors\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        # Note: multiplication by sqrt(d_model) is a scaling trick from the original paper\n",
        "        # It helps with training stability by preventing embeddings from being too small\n",
        "\n",
        "        # STEP 2: Add positional information\n",
        "        # The transpose operations are needed because PositionalEncoding expects\n",
        "        # sequence length as the first dimension, but we have batch first\n",
        "        # Input: [batch_size, seq_len, d_model]\n",
        "        # After first transpose: [seq_len, batch_size, d_model]\n",
        "        # After pos_encoding: [seq_len, batch_size, d_model] (with position info added)\n",
        "        # After second transpose: [batch_size, seq_len, d_model] (back to batch first)\n",
        "        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)\n",
        "\n",
        "        # STEP 3: Apply dropout for regularization\n",
        "        # Randomly zero out some elements during training\n",
        "        # Shape remains: [batch_size, seq_len, d_model]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # STEP 4: Process through all encoder layers\n",
        "        # Each layer takes [batch_size, seq_len, d_model] and outputs the same shape\n",
        "        # But the vectors become more refined with each layer\n",
        "        # Think of it as multiple rounds of \"looking at context and updating understanding\"\n",
        "        for enc_layer in self.enc_layers:\n",
        "            x = enc_layer(x, mask)\n",
        "            # mask parameter: optional tensor that tells the model which positions to ignore\n",
        "            # (useful for padding tokens or future positions in some applications)\n",
        "\n",
        "        # Final output: [batch_size, seq_len, d_model]\n",
        "        # Each word in each sentence now has a rich vector representation\n",
        "        # that captures both the word's meaning and its contextual relationships\n",
        "        return x"
      ],
      "metadata": {
        "id": "9KdJ34MAmQ_e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single decoder layer: masked self-attention + encoder-decoder attention + feed-forward + normalization.\n",
        "\n",
        "    This is like a sophisticated text generator that works in three stages:\n",
        "    1. Looks at what it has written so far (masked self-attention)\n",
        "    2. Considers the input context from the encoder (cross-attention)\n",
        "    3. Processes and refines the information (feed-forward network)\n",
        "\n",
        "    Think of it as a writer who:\n",
        "    - Reviews their draft so far (but can't peek ahead)\n",
        "    - Considers the source material/context\n",
        "    - Refines their writing based on both\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Component 1: Self-Attention (Masked)\n",
        "        # Lets each position look at previous positions in the output sequence\n",
        "        # \"What have I written so far that's relevant to what I'm writing now?\"\n",
        "        # Input/Output: [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Component 2: Cross-Attention (Encoder-Decoder)\n",
        "        # Lets the decoder attend to the encoder's output\n",
        "        # \"What parts of the input are relevant to what I'm generating now?\"\n",
        "        # Input: decoder=[batch_size, target_seq_len, d_model], encoder=[batch_size, source_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, d_model]\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Component 3: Feed-Forward Network\n",
        "        # Non-linear transformation to process the attended information\n",
        "        # \"Now let me think about all this information and refine it\"\n",
        "        # Input/Output: [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n",
        "        self.ffn = FeedForwardNetwork(d_model, dff)\n",
        "\n",
        "        # Normalization Layers (3 of them, one after each major component)\n",
        "        # These stabilize training by normalizing the data distribution\n",
        "        # Think of them as \"data cleaners\" that keep values in a reasonable range\n",
        "        # Input/Output: [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)  # After masked self-attention\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)  # After cross-attention\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)  # After feed-forward\n",
        "\n",
        "        # Dropout Layers (3 of them, one after each major component)\n",
        "        # Randomly zero out some elements during training to prevent overfitting\n",
        "        # \"Add some randomness to make the model more robust\"\n",
        "        # Input/Output: [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)  # After masked self-attention\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)  # After cross-attention\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)  # After feed-forward\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        \"\"\"\n",
        "        Process target sequence through the decoder layer.\n",
        "\n",
        "        Input dimensions:\n",
        "        - x: [batch_size, target_seq_len, d_model] - what we're generating so far\n",
        "        - enc_output: [batch_size, source_seq_len, d_model] - encoder's understanding of input\n",
        "        - look_ahead_mask: prevents looking at future tokens (maintains causality)\n",
        "        - padding_mask: ignores padding tokens in encoder output\n",
        "\n",
        "        Output dimensions: [batch_size, target_seq_len, d_model]\n",
        "        - Same shape as input x, but with refined representations\n",
        "        \"\"\"\n",
        "\n",
        "        # STAGE 1: Masked Self-Attention\n",
        "        # \"Let me look at what I've written so far (but not peek ahead)\"\n",
        "        # Input: x=[batch_size, target_seq_len, d_model]\n",
        "        # Query, Key, Value all come from x (self-attention)\n",
        "        # look_ahead_mask prevents positions from seeing future positions\n",
        "        # Output: [batch_size, target_seq_len, d_model]\n",
        "        attn1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        # Input/Output: [batch_size, target_seq_len, d_model]\n",
        "        attn1 = self.dropout1(attn1)\n",
        "\n",
        "        # Add & Normalize (Residual Connection + Layer Normalization)\n",
        "        # x + attn1: adds original input to attention output (residual connection)\n",
        "        # This helps with gradient flow and allows the model to learn incremental changes\n",
        "        # Input: x=[batch_size, target_seq_len, d_model], attn1=[batch_size, target_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, d_model]\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        # STAGE 2: Cross-Attention (Encoder-Decoder Attention)\n",
        "        # \"Now let me consider the input context for what I'm generating\"\n",
        "        # Query comes from decoder (out1), Key and Value come from encoder (enc_output)\n",
        "        # Input: out1=[batch_size, target_seq_len, d_model], enc_output=[batch_size, source_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, d_model]\n",
        "        attn2 = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        # Input/Output: [batch_size, target_seq_len, d_model]\n",
        "        attn2 = self.dropout2(attn2)\n",
        "\n",
        "        # Add & Normalize (Residual Connection + Layer Normalization)\n",
        "        # Input: out1=[batch_size, target_seq_len, d_model], attn2=[batch_size, target_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, d_model]\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "        # STAGE 3: Feed-Forward Network\n",
        "        # \"Let me process and refine all this information\"\n",
        "        # Non-linear transformation to add modeling capacity\n",
        "        # Input/Output: [batch_size, target_seq_len, d_model]\n",
        "        ffn_output = self.ffn(out2)\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        # Input/Output: [batch_size, target_seq_len, d_model]\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "\n",
        "        # Final Add & Normalize (Residual Connection + Layer Normalization)\n",
        "        # Input: out2=[batch_size, target_seq_len, d_model], ffn_output=[batch_size, target_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, d_model]\n",
        "        out3 = self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "        # Final output: [batch_size, target_seq_len, d_model]\n",
        "        # Each position now has a refined representation that considers:\n",
        "        # 1. Previous positions in the target sequence\n",
        "        # 2. Relevant parts of the source sequence\n",
        "        # 3. Non-linear processing of the combined information\n",
        "        return out3"
      ],
      "metadata": {
        "id": "9mkMe2gGmTSD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The decoder stack: target embedding + positional encoding + N decoder layers.\n",
        "\n",
        "    This is the \"text generation\" part of the transformer - it takes what you want to generate\n",
        "    (like a translation target or response) and produces rich representations by:\n",
        "    1. Converting target words to vectors (embedding)\n",
        "    2. Adding position information\n",
        "    3. Processing through multiple decoder layers that look at both the target and encoder output\n",
        "\n",
        "    Think of it as a sophisticated auto-complete system that considers both:\n",
        "    - What you've typed so far (target sequence)\n",
        "    - The context/source material (encoder output)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dff, max_length, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Step 1: Target Word-to-Vector Converter\n",
        "        # Takes target word IDs (what we want to generate) and converts to dense vectors\n",
        "        # Input: integers in range [0, vocab_size-1] (target word IDs)\n",
        "        # Output: vectors of size d_model (e.g., 512 dimensions)\n",
        "        # Like a lookup table: target_word_id -> dense_vector\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Step 2: Position Information Injector\n",
        "        # Adds \"where am I in the target sequence?\" information to each word vector\n",
        "        # Input: vectors of size d_model\n",
        "        # Output: same vectors but now with position info mixed in\n",
        "        # This helps the model understand the order of words being generated\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        # Step 3: Stack of Decoder Processing Layers\n",
        "        # Creates a list of identical decoder units (like assembly line stations)\n",
        "        # Each layer refines understanding by looking at:\n",
        "        # - Previous words in target sequence (masked self-attention)\n",
        "        # - Relevant parts of encoder output (cross-attention)\n",
        "        # Input to each layer: vectors of size d_model\n",
        "        # Output from each layer: refined vectors of same size d_model\n",
        "        self.dec_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "            for _ in range(num_layers)  # Fixed syntax error: was \"for * in range(num*layers)\"\n",
        "        ])\n",
        "\n",
        "        # Step 4: Regularization (Prevents Overfitting)\n",
        "        # Randomly sets some vector elements to zero during training\n",
        "        # This forces the model to be more robust and not rely on specific patterns\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        \"\"\"\n",
        "        Process target sequence through the decoder pipeline.\n",
        "\n",
        "        Input dimensions:\n",
        "        - x: [batch_size, target_seq_len] - target sequence (what we want to generate)\n",
        "        - enc_output: [batch_size, source_seq_len, d_model] - encoder's understanding of input\n",
        "        - look_ahead_mask: prevents decoder from seeing future target words (maintains causality)\n",
        "        - padding_mask: tells decoder which encoder positions to ignore (padding tokens)\n",
        "\n",
        "        Output dimensions: [batch_size, target_seq_len, d_model]\n",
        "        - Same batch_size and target_seq_len as input x\n",
        "        - Each target word now represented as a d_model-dimensional vector\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the length of our target sequences\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # STEP 1: Convert target word IDs to dense vectors\n",
        "        # Input: [batch_size, target_seq_len] - integers (target word IDs)\n",
        "        # Output: [batch_size, target_seq_len, d_model] - floating point vectors\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        # Note: multiplication by sqrt(d_model) is a scaling trick from the original paper\n",
        "        # It helps with training stability by preventing embeddings from being too small\n",
        "\n",
        "        # STEP 2: Add positional information\n",
        "        # The transpose operations are needed because PositionalEncoding expects\n",
        "        # sequence length as the first dimension, but we have batch first\n",
        "        # Input: [batch_size, target_seq_len, d_model]\n",
        "        # After first transpose: [target_seq_len, batch_size, d_model]\n",
        "        # After pos_encoding: [target_seq_len, batch_size, d_model] (with position info added)\n",
        "        # After second transpose: [batch_size, target_seq_len, d_model] (back to batch first)\n",
        "        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)\n",
        "\n",
        "        # STEP 3: Apply dropout for regularization\n",
        "        # Randomly zero out some elements during training\n",
        "        # Shape remains: [batch_size, target_seq_len, d_model]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # STEP 4: Process through all decoder layers\n",
        "        # Each layer takes [batch_size, target_seq_len, d_model] and outputs the same shape\n",
        "        # But the vectors become more refined with each layer\n",
        "        # Each layer performs three operations:\n",
        "        # 1. Masked self-attention: \"What have I generated so far?\"\n",
        "        # 2. Cross-attention: \"What's relevant from the input?\"\n",
        "        # 3. Feed-forward: \"Let me process this information\"\n",
        "        for dec_layer in self.dec_layers:\n",
        "            x = dec_layer(x, enc_output, look_ahead_mask, padding_mask)\n",
        "            # x: current target representations [batch_size, target_seq_len, d_model]\n",
        "            # enc_output: encoder's input understanding [batch_size, source_seq_len, d_model]\n",
        "            # look_ahead_mask: prevents seeing future target words\n",
        "            # padding_mask: ignores padding in encoder output\n",
        "\n",
        "        # Final output: [batch_size, target_seq_len, d_model]\n",
        "        # Each target word now has a rich vector representation that captures:\n",
        "        # 1. The word's meaning and position in target sequence\n",
        "        # 2. Its relationship to previous target words\n",
        "        # 3. Its relationship to relevant parts of the source sequence\n",
        "        # 4. Non-linear processing of all this combined information\n",
        "        return x"
      ],
      "metadata": {
        "id": "s-yK1rISmVUl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer model for sequence-to-sequence translation.\n",
        "    Now supports separate numbers of layers for encoder and decoder.\n",
        "    - Encoder handles source language.\n",
        "    - Decoder handles target language.\n",
        "    - Includes separate MLM heads for pretraining.\n",
        "\n",
        "    This is the complete translation system that combines:\n",
        "    1. An encoder that understands the source language (e.g., English)\n",
        "    2. A decoder that generates the target language (e.g., Bengali)\n",
        "    3. Additional heads for pretraining on individual languages\n",
        "\n",
        "    Think of it as a sophisticated translation pipeline:\n",
        "    - Encoder: \"I understand what this English text means\"\n",
        "    - Decoder: \"Based on that understanding, here's the Bengali translation\"\n",
        "    - MLM heads: \"I can also learn individual languages separately\"\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model,\n",
        "                 num_encoder_layers, num_decoder_layers, num_heads,\n",
        "                 dff, max_length, dropout_rate):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # --- Main Translation Components ---\n",
        "\n",
        "        # Source Language Processor (Encoder)\n",
        "        # Takes source language tokens and creates rich representations\n",
        "        # Input: [batch_size, source_seq_len] (source word IDs)\n",
        "        # Output: [batch_size, source_seq_len, d_model] (source understanding vectors)\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_encoder_layers, num_heads,\n",
        "                               dff, max_length, dropout_rate)\n",
        "\n",
        "        # Target Language Generator (Decoder)\n",
        "        # Takes target language tokens and encoder output, generates target representations\n",
        "        # Input: target=[batch_size, target_seq_len], encoder_out=[batch_size, source_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, d_model] (target understanding vectors)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_decoder_layers, num_heads,\n",
        "                               dff, max_length, dropout_rate)\n",
        "\n",
        "        # Final Translation Head\n",
        "        # Converts decoder representations to target vocabulary probabilities\n",
        "        # Input: [batch_size, target_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, tgt_vocab_size] (word probabilities)\n",
        "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        # Device configuration for GPU/CPU processing\n",
        "        self.device = torch.device(CONFIG['device'])\n",
        "\n",
        "        # --- Pretraining Components ---\n",
        "\n",
        "        # Masked Language Model Head for Source Language (e.g., English)\n",
        "        # Used during pretraining to learn source language patterns\n",
        "        # Input: [batch_size, seq_len, d_model] (encoder output)\n",
        "        # Output: [batch_size, seq_len, src_vocab_size] (source word probabilities)\n",
        "        self.mlm_head_src = nn.Linear(d_model, src_vocab_size)\n",
        "\n",
        "        # Masked Language Model Head for Target Language (e.g., Bengali)\n",
        "        # Used during pretraining to learn target language patterns\n",
        "        # Input: [batch_size, seq_len, d_model] (decoder output)\n",
        "        # Output: [batch_size, seq_len, tgt_vocab_size] (target word probabilities)\n",
        "        self.mlm_head_tgt = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def create_padding_mask(self, seq):\n",
        "        \"\"\"\n",
        "        Create padding mask to ignore padding tokens.\n",
        "\n",
        "        Think of this like preparing a list of sentences for processing, where some sentences\n",
        "        are shorter than others. To handle them efficiently in batches, we add \"fake\" words\n",
        "        (padding tokens, marked as 0) to make all sentences the same length.\n",
        "\n",
        "        This function creates a \"mask\" - essentially a map that tells us which parts of each\n",
        "        sentence are real words and which parts are just padding that should be ignored.\n",
        "\n",
        "        Example:\n",
        "        - Original sentences: [\"Hello world\", \"Hi there friend\", \"Hey\"]\n",
        "        - After padding: [\"Hello world 0\", \"Hi there friend\", \"Hey 0 0\"]\n",
        "        - The mask marks which positions contain real words vs padding zeros\n",
        "\n",
        "        Args:\n",
        "            seq: Input tensor with shape [batch_size, seq_len]\n",
        "                - batch_size: how many sentences we're processing at once\n",
        "                - seq_len: maximum length of sentences (after padding)\n",
        "                - Values: actual token IDs for words, 0 for padding\n",
        "\n",
        "        Returns:\n",
        "            Mask tensor with shape [batch_size, 1, 1, seq_len]\n",
        "            - True where there are real tokens, False where there's padding\n",
        "            - Extra dimensions (1, 1) are added for neural network compatibility\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Compare each position to 0 (padding token)\n",
        "        # (seq != 0) creates a boolean tensor: True for real tokens, False for padding\n",
        "        # Input shape: [batch_size, seq_len] → Output shape: [batch_size, seq_len]\n",
        "        mask = (seq != 0)\n",
        "\n",
        "        # Step 2: Add first extra dimension for attention mechanism compatibility\n",
        "        # .unsqueeze(1) adds a dimension at position 1\n",
        "        # Shape: [batch_size, seq_len] → [batch_size, 1, seq_len]\n",
        "        mask = mask.unsqueeze(1)\n",
        "\n",
        "        # Step 3: Add second extra dimension for attention mechanism compatibility\n",
        "        # .unsqueeze(2) adds a dimension at position 2\n",
        "        # Shape: [batch_size, 1, seq_len] → [batch_size, 1, 1, seq_len]\n",
        "        mask = mask.unsqueeze(2)\n",
        "\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def create_look_ahead_mask(self, size):\n",
        "        \"\"\"\n",
        "        Create look-ahead mask to prevent seeing future tokens during training.\n",
        "\n",
        "        Think of this like taking a multiple-choice test where you're supposed to predict\n",
        "        the next word in a sentence. To make it fair, you should only see the words that\n",
        "        come BEFORE the word you're trying to predict, not the words that come AFTER.\n",
        "\n",
        "        This creates a \"triangular curtain\" that blocks the model from cheating by looking\n",
        "        at future words. For each position, it can only see itself and previous positions.\n",
        "\n",
        "        Example for size=4:\n",
        "        Position 0 can see: [0]           → mask row: [True,  False, False, False]\n",
        "        Position 1 can see: [0, 1]       → mask row: [True,  True,  False, False]\n",
        "        Position 2 can see: [0, 1, 2]    → mask row: [True,  True,  True,  False]\n",
        "        Position 3 can see: [0, 1, 2, 3] → mask row: [True,  True,  True,  True ]\n",
        "\n",
        "        Args:\n",
        "            size: Integer - length of the sequence (how many words/tokens)\n",
        "\n",
        "        Returns:\n",
        "            Mask tensor with shape [1, 1, size, size]\n",
        "            - True means \"can see this position\"\n",
        "            - False means \"block this position\"\n",
        "            - Extra dimensions (1, 1) are for neural network compatibility\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Create a matrix of all ones with shape [size, size]\n",
        "        # This is like creating a blank grid where every cell is filled\n",
        "        # Input: size (integer) → Output shape: [size, size]\n",
        "        ones_matrix = torch.ones(size, size)\n",
        "\n",
        "        # Step 2: Keep only the lower triangle (including diagonal)\n",
        "        # torch.tril = \"triangle lower\" - keeps bottom-left triangle, zeros out top-right\n",
        "        # This creates the \"staircase\" pattern where each row can see more positions\n",
        "        # Input shape: [size, size] → Output shape: [size, size]\n",
        "        triangular_matrix = torch.tril(ones_matrix)\n",
        "\n",
        "        # Step 3: Convert to boolean values (True/False instead of 1.0/0.0)\n",
        "        # Neural networks work better with explicit boolean masks\n",
        "        # Input shape: [size, size] → Output shape: [size, size]\n",
        "        mask = triangular_matrix.bool()\n",
        "\n",
        "        # Step 4: Add first extra dimension for batch processing\n",
        "        # .unsqueeze(0) adds a dimension at the beginning\n",
        "        # Shape: [size, size] → [1, size, size]\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        # Step 5: Add second extra dimension for attention mechanism compatibility\n",
        "        # .unsqueeze(0) adds another dimension at the beginning\n",
        "        # Shape: [1, size, size] → [1, 1, size, size]\n",
        "        mask = mask.unsqueeze(0)\n",
        "\n",
        "        return mask\n",
        "    def forward(self, src, tgt, training=True):\n",
        "        \"\"\"\n",
        "        Standard translation forward pass (not used for pretraining).\n",
        "\n",
        "        This is the main translation pipeline that converts source language to target language.\n",
        "\n",
        "        Input dimensions:\n",
        "        - src: [batch_size, source_seq_len] (source language word IDs)\n",
        "        - tgt: [batch_size, target_seq_len] (target language word IDs)\n",
        "        - training: boolean (whether in training mode)\n",
        "\n",
        "        Output dimensions: [batch_size, target_seq_len, tgt_vocab_size]\n",
        "        - Probability distribution over target vocabulary for each position\n",
        "        \"\"\"\n",
        "\n",
        "        # STEP 1: Create masks to handle padding and causality\n",
        "\n",
        "        # Source padding mask: \"Which source tokens are real vs padding?\"\n",
        "        # Input: [batch_size, source_seq_len]\n",
        "        # Output: [batch_size, 1, 1, source_seq_len]\n",
        "        src_mask = self.create_padding_mask(src)\n",
        "\n",
        "        # Target padding mask: \"Which target tokens are real vs padding?\"\n",
        "        # Input: [batch_size, target_seq_len]\n",
        "        # Output: [batch_size, 1, 1, target_seq_len]\n",
        "        tgt_padding_mask = self.create_padding_mask(tgt).to(self.device)\n",
        "\n",
        "        # Look-ahead mask: \"Prevent seeing future target tokens\"\n",
        "        # Input: target_seq_len (integer)\n",
        "        # Output: [1, 1, target_seq_len, target_seq_len]\n",
        "        tgt_seq_len = tgt.size(1)\n",
        "        look_ahead_mask = self.create_look_ahead_mask(tgt_seq_len).to(self.device)\n",
        "\n",
        "        # Combined mask: \"Apply both padding and look-ahead restrictions\"\n",
        "        # Input: look_ahead_mask + tgt_padding_mask\n",
        "        # Output: [batch_size, 1, target_seq_len, target_seq_len]\n",
        "        combined_mask = torch.logical_and(look_ahead_mask, tgt_padding_mask)\n",
        "\n",
        "        # STEP 2: Process source through encoder\n",
        "        # Input: src=[batch_size, source_seq_len], src_mask=[batch_size, 1, 1, source_seq_len]\n",
        "        # Output: [batch_size, source_seq_len, d_model] (source understanding)\n",
        "        enc_output = self.encoder(src, src_mask)\n",
        "\n",
        "        # STEP 3: Process target through decoder (with encoder context)\n",
        "        # Input: tgt=[batch_size, target_seq_len], enc_output=[batch_size, source_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, d_model] (target understanding)\n",
        "        dec_output = self.decoder(tgt, enc_output, combined_mask, src_mask)\n",
        "\n",
        "        # STEP 4: Convert to vocabulary probabilities\n",
        "        # Input: [batch_size, target_seq_len, d_model]\n",
        "        # Output: [batch_size, target_seq_len, tgt_vocab_size] (word probabilities)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    def mlm_encode(self, input_ids):\n",
        "      \"\"\"\n",
        "      For encoder MLM pretraining (English):\n",
        "      - Pass input through encoder\n",
        "      - Project encoder output to English vocabulary size\n",
        "\n",
        "      This is used for pretraining the encoder on source language data.\n",
        "      Like teaching the encoder \"Given this English sentence with some words masked,\n",
        "      predict what the masked words should be.\"\n",
        "\n",
        "      Input dimensions: input_ids=[batch_size, seq_len] (source language with masked tokens)\n",
        "      Output dimensions: [batch_size, seq_len, src_vocab_size] (predictions for each position)\n",
        "      \"\"\"\n",
        "\n",
        "      # Validate input shape\n",
        "      # Think of this as checking: \"Are we receiving a 2D table of numbers?\"\n",
        "      # input_ids is like a spreadsheet where each row is a sentence and each column is a word position\n",
        "      assert input_ids.dim() == 2, f\"Encoder MLM input_ids must be 2D [batch, seq], got {input_ids.shape}\"\n",
        "\n",
        "      # Create padding mask for the input\n",
        "      # Problem: Sentences have different lengths, but we need fixed-size arrays\n",
        "      # Solution: Add padding (like empty spaces) and create a \"mask\" to ignore those spaces\n",
        "      #\n",
        "      # Input: [batch_size, seq_len] - A 2D array where each row is a sentence\n",
        "      # This function takes our 2D sentence array and creates a 4D \"attention mask\"\n",
        "      # Output: [batch_size, 1, 1, seq_len] - A 4D array that tells the model \"ignore padded positions\"\n",
        "      # The extra dimensions (1, 1) are added for broadcasting - like stretching a rubber sheet\n",
        "      # to fit over a larger surface\n",
        "      src_mask = self.create_padding_mask(input_ids)\n",
        "\n",
        "      # Process through encoder\n",
        "      # The encoder is like a smart translator that reads the input sentence and creates\n",
        "      # a \"rich understanding\" of each word in context\n",
        "      #\n",
        "      # Input: input_ids=[batch_size, seq_len] - Original sentences with word IDs\n",
        "      #        src_mask=[batch_size, 1, 1, seq_len] - Mask saying \"ignore padding\"\n",
        "      #\n",
        "      # The encoder transforms each word ID into a high-dimensional vector (d_model numbers)\n",
        "      # that captures the word's meaning in context\n",
        "      # Output: [batch_size, seq_len, d_model] - Each word becomes a vector of d_model numbers\n",
        "      # Think: instead of word ID \"5\", we now have a vector like [0.2, -0.1, 0.8, ...]\n",
        "      enc_output = self.encoder(input_ids, src_mask)\n",
        "\n",
        "      # Validate encoder output shape\n",
        "      # Safety check: Make sure the encoder didn't change the batch size or sequence length\n",
        "      # We should still have the same number of sentences and same number of words per sentence\n",
        "      assert enc_output.shape[:2] == input_ids.shape, f\"Encoder output shape {enc_output.shape} does not match input {input_ids.shape}\"\n",
        "\n",
        "      # Project to source vocabulary\n",
        "      # Now we have rich word representations, but we need to convert them back to word predictions\n",
        "      # This is like asking: \"Given this rich understanding, what word should go here?\"\n",
        "      #\n",
        "      # Input: [batch_size, seq_len, d_model] - Rich word representations\n",
        "      # The mlm_head_src is a learned transformation (like a lookup table with math)\n",
        "      # that maps each d_model-dimensional vector to vocabulary-sized predictions\n",
        "      # Output: [batch_size, seq_len, src_vocab_size] - Probability scores for each word in vocabulary\n",
        "      #\n",
        "      # For each position in each sentence, we get a score for every possible word\n",
        "      # Higher score = \"I think this word belongs here\"\n",
        "      return self.mlm_head_src(enc_output)\n",
        "\n",
        "    def decoder_mlm(self, input_ids):\n",
        "      \"\"\"\n",
        "      For decoder MLM pretraining (Bengali):\n",
        "      - Pass input through decoder (as a language model)\n",
        "      - Use dummy encoder output (zeros)\n",
        "      - Project decoder output to Bengali vocabulary size\n",
        "\n",
        "      This is used for pretraining the decoder on target language data.\n",
        "      Like teaching the decoder \"Given this Bengali sentence with some words masked,\n",
        "      predict what the masked words should be\" (without any source context).\n",
        "\n",
        "      Input dimensions: input_ids=[batch_size, seq_len] (target language with masked tokens)\n",
        "      Output dimensions: [batch_size, seq_len, tgt_vocab_size] (predictions for each position)\n",
        "      \"\"\"\n",
        "\n",
        "      # Extract basic information from input\n",
        "      # Think of this as asking: \"How many sentences do we have?\" and \"How long are they?\"\n",
        "      # input_ids is like a 2D table where rows=sentences, columns=word positions\n",
        "      batch_size, seq_len = input_ids.size()\n",
        "      device = input_ids.device  # Which computing device (CPU/GPU) are we using?\n",
        "\n",
        "      # Create dummy encoder output (all zeros)\n",
        "      # Problem: The decoder usually expects input from an encoder (like in translation)\n",
        "      # But here we're just doing language modeling (predict next word), so no encoder needed\n",
        "      # Solution: Create a \"fake\" encoder output filled with zeros\n",
        "      #\n",
        "      # This creates a 3D array filled with zeros:\n",
        "      # Input: batch_size, seq_len, d_model (dimensions we need)\n",
        "      # Output: [batch_size, seq_len, d_model] - Like a 3D box of zeros\n",
        "      # Think: For each sentence, for each word position, we have d_model zero values\n",
        "      dummy_enc = torch.zeros(batch_size, seq_len, self.decoder.d_model, device=device)\n",
        "\n",
        "      # Validate dummy encoder shape\n",
        "      # Safety check: Make sure our fake encoder output has the right dimensions\n",
        "      # Like checking if a box has the right length, width, and height\n",
        "      assert dummy_enc.shape == (batch_size, seq_len, self.decoder.d_model), f\"Dummy encoder shape {dummy_enc.shape} does not match (batch, seq, d_model)\"\n",
        "\n",
        "      # Create look-ahead mask: \"Don't peek at future words\"\n",
        "      # Problem: When predicting a word, we shouldn't see words that come after it\n",
        "      # (That would be cheating - like seeing the answer before solving the problem)\n",
        "      #\n",
        "      # Input: seq_len (just a number - how long our sentences are)\n",
        "      # This function creates a triangular mask - imagine a lower triangular matrix\n",
        "      # Output: [1, 1, seq_len, seq_len] - A 4D \"attention mask\"\n",
        "      # The mask says \"position i can only look at positions 0 to i, not i+1 to end\"\n",
        "      look_ahead_mask = self.create_look_ahead_mask(seq_len).to(device)\n",
        "\n",
        "      # Create padding mask: \"Ignore padding tokens\"\n",
        "      # Problem: Sentences have different lengths, but we need fixed-size arrays\n",
        "      # Solution: We added padding, now we need to ignore those padded positions\n",
        "      #\n",
        "      # Input: [batch_size, seq_len] - Our original sentence array\n",
        "      # Output: [batch_size, 1, 1, seq_len] - A mask saying \"ignore padded positions\"\n",
        "      # The extra dimensions (1, 1) are for broadcasting - like stretching to fit\n",
        "      padding_mask = self.create_padding_mask(input_ids).to(device)\n",
        "\n",
        "      # Combine both masks\n",
        "      # We need BOTH rules: \"don't look ahead\" AND \"ignore padding\"\n",
        "      # This is like combining two filters - both must be satisfied\n",
        "      #\n",
        "      # Input: look_ahead_mask [1, 1, seq_len, seq_len] + padding_mask [batch_size, 1, 1, seq_len]\n",
        "      # The logical_and operation combines them element-wise (like AND gate in logic)\n",
        "      # Output: [batch_size, 1, seq_len, seq_len] - Combined mask with both rules\n",
        "      combined_mask = torch.logical_and(look_ahead_mask, padding_mask)\n",
        "\n",
        "      # Validate mask shapes\n",
        "      # Safety checks: Make sure our masks have the right dimensions\n",
        "      # Like checking if puzzle pieces have the right shape before fitting them together\n",
        "      assert look_ahead_mask.shape[-2:] == (seq_len, seq_len), f\"Look ahead mask shape {look_ahead_mask.shape} does not match (1, 1, seq, seq)\"\n",
        "      assert padding_mask.shape[-1] == seq_len, f\"Padding mask shape {padding_mask.shape} does not match seq_len {seq_len}\"\n",
        "\n",
        "      # Process through decoder with dummy encoder output\n",
        "      # The decoder is like a sophisticated text predictor that learns context\n",
        "      # It takes the input words and the fake encoder output, following the mask rules\n",
        "      #\n",
        "      # Input: input_ids=[batch_size, seq_len] - Original sentences with word IDs\n",
        "      #        dummy_enc=[batch_size, seq_len, d_model] - Fake encoder output (all zeros)\n",
        "      #        combined_mask=[batch_size, 1, seq_len, seq_len] - Combined attention rules\n",
        "      #        None - No additional encoder mask needed\n",
        "      #\n",
        "      # The decoder transforms each word ID into a rich representation that understands:\n",
        "      # \"What should the next word be, given the context so far?\"\n",
        "      # Output: [batch_size, seq_len, d_model] - Rich word representations for predictions\n",
        "      dec_out = self.decoder(input_ids, dummy_enc, combined_mask, None)\n",
        "\n",
        "      # Validate decoder output shape\n",
        "      # Safety check: Make sure decoder didn't change batch size or sequence length\n",
        "      # We should still have the same number of sentences and words per sentence\n",
        "      assert dec_out.shape[:2] == input_ids.shape, f\"Decoder output shape {dec_out.shape} does not match input {input_ids.shape}\"\n",
        "\n",
        "      # Project to target vocabulary\n",
        "      # Now we have rich word representations, but we need actual word predictions\n",
        "      # This is like asking: \"Given this understanding, what word should come next?\"\n",
        "      #\n",
        "      # Input: [batch_size, seq_len, d_model] - Rich word representations from decoder\n",
        "      # The mlm_head_tgt is a learned transformation that maps each d_model-dimensional\n",
        "      # vector to vocabulary-sized predictions (like a smart lookup table)\n",
        "      # Output: [batch_size, seq_len, tgt_vocab_size] - Probability scores for each word\n",
        "      #\n",
        "      # For each position in each sentence, we get a score for every possible word\n",
        "      # in the target language vocabulary. Higher score = \"I think this word belongs here\"\n",
        "      return self.mlm_head_tgt(dec_out)"
      ],
      "metadata": {
        "id": "FogCeW8Dmbg7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset classes\n",
        "class PretrainDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for pre-training with masked language modeling (MLM).\n",
        "    - Randomly masks tokens in each sentence.\n",
        "    - Returns input_ids (masked) and labels (original, with -100 for unmasked).\n",
        "    \"\"\"\n",
        "\n",
        "    # This is the initialization function - it runs when you create a new instance of this class\n",
        "    # Think of it like setting up a new container with all the ingredients you need\n",
        "    # Input: sentences (list of strings), vocab (word-to-number converter), max_length (integer), mask_prob (float)\n",
        "    # Output: A configured dataset object ready to serve training data\n",
        "    def __init__(self, sentences, vocab, max_length, mask_prob=0.15):\n",
        "        # Store the list of sentences we want to train on\n",
        "        # Input: sentences - a Python list like [\"Hello world\", \"How are you\", ...]\n",
        "        self.sentences = sentences\n",
        "        # Store the vocabulary (dictionary that converts words to numbers)\n",
        "        # Input: vocab - an object that can convert \"hello\" -> 142, \"world\" -> 89, etc.\n",
        "        self.vocab = vocab\n",
        "        # Store the maximum length we want our sentences to be\n",
        "        # Input: max_length - integer like 128 (all sentences will be padded/truncated to this length)\n",
        "        self.max_length = max_length\n",
        "        # Store the probability of masking each word (15% by default)\n",
        "        # Input: mask_prob - float like 0.15 (15% chance to hide each word)\n",
        "        self.mask_prob = mask_prob\n",
        "\n",
        "    # This function tells us how many sentences we have in total\n",
        "    # Like counting how many items are in a box\n",
        "    # Input: None (called automatically by Python)\n",
        "    # Output: Integer representing total number of sentences\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    # This function takes a sentence and randomly hides some words\n",
        "    # It's like playing a fill-in-the-blank game\n",
        "    # Input: tokens - a list of integers like [2, 45, 123, 67, 3] (sentence converted to numbers)\n",
        "    # Output: masked_tokens (list of integers with some hidden), labels (list showing which were hidden)\n",
        "    def mask_tokens(self, tokens):\n",
        "        # Create a copy of the original sentence so we don't change it\n",
        "        # Input: tokens - list of integers like [2, 45, 123, 67, 3]\n",
        "        # Output: masked_tokens - identical copy at first, will be modified\n",
        "        masked_tokens = tokens.copy()\n",
        "        # Create a list to store the correct answers (what words were hidden)\n",
        "        # -100 is a special number meaning \"don't check this word\"\n",
        "        # Input: len(tokens) - integer length of sentence\n",
        "        # Output: labels - list of -100s, same length as tokens, like [-100, -100, -100, -100, -100]\n",
        "        labels = [-100] * len(tokens)\n",
        "\n",
        "        # Go through each word in the sentence\n",
        "        # This loop processes each position: 0, 1, 2, 3, ... len(tokens)-1\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Don't hide special words like padding, start/end markers, or already masked words\n",
        "            # Numbers 0,1,2,3,4 represent PAD, UNK, SOS, EOS, MASK tokens\n",
        "            # Input: token - single integer like 45\n",
        "            # Decision: skip if token is a special control token\n",
        "            if token in [0, 2, 3, 4]:\n",
        "                continue\n",
        "\n",
        "            # Randomly decide if we should hide this word (15% chance)\n",
        "            # Input: random.random() generates float between 0.0 and 1.0\n",
        "            # Decision: if random number < 0.15, then mask this word\n",
        "            if random.random() < self.mask_prob:\n",
        "                # Remember what the original word was (this becomes our \"correct answer\")\n",
        "                # Input: token - the original word ID like 45\n",
        "                # Output: labels[i] changes from -100 to 45 (the correct answer)\n",
        "                labels[i] = token\n",
        "\n",
        "                # 80% of the time, replace with a special MASK token\n",
        "                # Input: random.random() generates another float between 0.0 and 1.0\n",
        "                # Decision: if < 0.8, use MASK token\n",
        "                if random.random() < 0.8:\n",
        "                    # Replace word with special MASK token (usually token ID 4)\n",
        "                    # Input: vocab lookup returns integer ID for '<MASK>' token\n",
        "                    # Output: masked_tokens[i] becomes 4 (or whatever MASK token ID is)\n",
        "                    masked_tokens[i] = self.vocab.get_vocab().get('<MASK>', 4)\n",
        "                # 10% of the time, replace with a random word from our vocabulary\n",
        "                # (This checks 50% of the remaining 20%, which equals 10% overall)\n",
        "                elif random.random() < 0.5:\n",
        "                    # Replace with random word ID from vocabulary\n",
        "                    # Input: vocab size like 30000, generates random int from 5 to 29999\n",
        "                    # Output: masked_tokens[i] becomes random word ID like 15842\n",
        "                    masked_tokens[i] = random.randint(5, len(self.vocab.get_vocab()) - 1)\n",
        "                # 10% of the time, keep the original word (this helps the model learn better)\n",
        "                # No change needed - masked_tokens[i] stays as original token\n",
        "\n",
        "        # Return the modified sentence and the answer key\n",
        "        # Output: masked_tokens - list of integers with some words hidden/changed\n",
        "        #         labels - list of integers/(-100) showing which positions need to be predicted\n",
        "        return masked_tokens, labels\n",
        "\n",
        "    # This function gets called when we ask for a specific sentence by its position\n",
        "    # Like asking for the 5th item in a list\n",
        "    # Input: idx - integer index like 0, 1, 2, 3... (which sentence to get)\n",
        "    # Output: dictionary with 'input_ids' and 'labels' tensors\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the sentence at the requested position and remove extra spaces\n",
        "        # Input: idx - integer index to select sentence\n",
        "        # Output: sentence - string like \"Hello world how are you\"\n",
        "        sentence = self.sentences[idx].strip()\n",
        "\n",
        "        # Convert the sentence from words to numbers (tokenization)\n",
        "        # This is like giving each word a unique ID number\n",
        "        # Input: sentence - string like \"Hello world\"\n",
        "        # Output: tokens - list of integers like [142, 89] (word IDs)\n",
        "        tokens = self.vocab.encode(sentence)\n",
        "\n",
        "        # If the sentence is too long, cut it short\n",
        "        # We subtract 2 to leave room for start and end markers\n",
        "        # Input: tokens - list of integers, potentially any length\n",
        "        # Output: tokens - same list but truncated to max_length-2 if needed\n",
        "        if len(tokens) > self.max_length - 2:\n",
        "            tokens = tokens[:self.max_length - 2]\n",
        "\n",
        "        # Add special markers at the beginning and end of the sentence\n",
        "        # Like putting quotation marks around a sentence\n",
        "        # Input: tokens - list of word IDs like [142, 89]\n",
        "        # Processing: get SOS token ID (usually 2) and EOS token ID (usually 3)\n",
        "        # Output: tokens - list like [2, 142, 89, 3] (SOS + words + EOS)\n",
        "        sos = self.vocab.get_vocab().get('<SOS>')  # Start of sentence marker\n",
        "        eos = self.vocab.get_vocab().get('<EOS>')    # End of sentence marker\n",
        "        tokens = [sos] + tokens + [eos]\n",
        "\n",
        "        # Hide some words randomly and remember what they were\n",
        "        # Input: tokens - list of integers like [2, 142, 89, 3]\n",
        "        # Output: masked_tokens - same list but with some words hidden/changed\n",
        "        #         labels - list showing which positions need to be predicted\n",
        "        masked_tokens, labels = self.mask_tokens(tokens)\n",
        "\n",
        "        # Make sure all sentences are the same length by adding padding\n",
        "        # Like adding blank spaces to make all lines the same length\n",
        "        # Input: masked_tokens - list of variable length like [2, 4, 89, 3] (length 4)\n",
        "        #        labels - list of same length as masked_tokens\n",
        "        # Processing: add PAD tokens (usually 0) to reach max_length\n",
        "        # Output: padded_tokens - list of exactly max_length integers\n",
        "        #         padded_labels - list of exactly max_length integers/(-100)\n",
        "        pad = self.vocab.get_vocab().get('<PAD>')  # Padding token\n",
        "        padded_tokens = masked_tokens + [pad] * (self.max_length - len(masked_tokens))\n",
        "        padded_labels = labels + [-100] * (self.max_length - len(labels))\n",
        "\n",
        "        # Return the data in a format that PyTorch can understand\n",
        "        # Think of this as packaging the data in a specific container\n",
        "        # Input: padded_tokens - Python list of integers with length max_length\n",
        "        #        padded_labels - Python list of integers with length max_length\n",
        "        # Output: Dictionary with two PyTorch tensors:\n",
        "        #         'input_ids': 1D tensor of shape [max_length] containing word IDs\n",
        "        #         'labels': 1D tensor of shape [max_length] containing target answers\n",
        "        return {\n",
        "            'input_ids': torch.tensor(padded_tokens, dtype=torch.long),    # The sentence with some words hidden\n",
        "            'labels': torch.tensor(padded_labels, dtype=torch.long)        # The correct answers for the hidden words\n",
        "        }"
      ],
      "metadata": {
        "id": "n3DW8xQKm0Vw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for translation fine-tuning.\n",
        "    - Returns source and target sequences, both input and output forms.\n",
        "\n",
        "    This class handles paired sentences for translation training, like:\n",
        "    English: \"Hello world\" → Bengali: \"হ্যালো বিশ্ব\"\n",
        "    \"\"\"\n",
        "\n",
        "    # This is the initialization function - sets up the translation dataset\n",
        "    # Think of it like organizing two parallel lists of sentences for translation practice\n",
        "    # Input: src_sentences (list of strings in source language)\n",
        "    #        tgt_sentences (list of strings in target language)\n",
        "    #        src_vocab (word-to-number converter for source language)\n",
        "    #        tgt_vocab (word-to-number converter for target language)\n",
        "    #        max_length (integer - maximum sentence length)\n",
        "    # Output: A configured dataset object ready to serve translation pairs\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_length):\n",
        "        # Store the source sentences (e.g., English sentences)\n",
        "        # Input: src_sentences - Python list like [\"Hello world\", \"How are you\", ...]\n",
        "        self.src_sentences = src_sentences\n",
        "        # Store the target sentences (e.g., Bengali sentences)\n",
        "        # Input: tgt_sentences - Python list like [\"হ্যালো বিশ্ব\", \"আপনি কেমন আছেন\", ...]\n",
        "        # Note: src_sentences[i] and tgt_sentences[i] should be translations of each other\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        # Store the vocabulary for source language (converts source words to numbers)\n",
        "        # Input: src_vocab - object that converts \"hello\" -> 142, \"world\" -> 89, etc.\n",
        "        self.src_vocab = src_vocab\n",
        "        # Store the vocabulary for target language (converts target words to numbers)\n",
        "        # Input: tgt_vocab - object that converts \"হ্যালো\" -> 73, \"বিশ্ব\" -> 156, etc.\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        # Store the maximum length for both source and target sentences\n",
        "        # Input: max_length - integer like 128 (all sentences will be padded/truncated to this)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    # This function tells us how many translation pairs we have\n",
        "    # Like counting how many sentence pairs are in our training set\n",
        "    # Input: None (called automatically by Python)\n",
        "    # Output: Integer representing total number of translation pairs\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    # This function gets a specific translation pair by its position\n",
        "    # Like asking for the 5th English-Bengali sentence pair\n",
        "    # Input: idx - integer index like 0, 1, 2, 3... (which translation pair to get)\n",
        "    # Output: Dictionary with 'src', 'tgt_ip', and 'tgt_op' tensors\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the source sentence at the requested position\n",
        "        # Input: idx - integer index to select sentence pair\n",
        "        # Output: src_sentence - string like \"Hello world\" (source language)\n",
        "        src_sentence = self.src_sentences[idx]\n",
        "        # Get the corresponding target sentence\n",
        "        # Input: idx - same integer index\n",
        "        # Output: tgt_sentence - string like \"হ্যালো বিশ্ব\" (target language)\n",
        "        tgt_sentence = self.tgt_sentences[idx]\n",
        "\n",
        "        # Convert source sentence from words to numbers using source vocabulary\n",
        "        # Input: src_sentence - string like \"Hello world\"\n",
        "        # Output: src_encoded - list of integers like [142, 89] (source word IDs)\n",
        "        src_encoded = self.src_vocab.encode(src_sentence)\n",
        "\n",
        "        # Create target input sequence: add SOS (Start Of Sentence) token at beginning\n",
        "        # This is what we feed to the decoder: \"<SOS> হ্যালো বিশ্ব\"\n",
        "        # Input: tgt_sentence - string like \"হ্যালো বিশ্ব\"\n",
        "        # Processing: get SOS token ID (usually 2), encode target sentence to numbers\n",
        "        # Output: tgt_encoded_ip - list like [2, 73, 156] (SOS + target word IDs)\n",
        "        tgt_encoded_ip = [self.tgt_vocab.get_vocab().get('<SOS>')] + self.tgt_vocab.encode(tgt_sentence)\n",
        "\n",
        "        # Create target output sequence: add EOS (End Of Sentence) token at end\n",
        "        # This is what we expect the decoder to produce: \"হ্যালো বিশ্ব <EOS>\"\n",
        "        # Input: tgt_sentence - same string like \"হ্যালো বিশ্ব\"\n",
        "        # Processing: encode target sentence to numbers, get EOS token ID (usually 3)\n",
        "        # Output: tgt_encoded_op - list like [73, 156, 3] (target word IDs + EOS)\n",
        "        tgt_encoded_op = self.tgt_vocab.encode(tgt_sentence) + [self.tgt_vocab.get_vocab().get('<EOS>')]\n",
        "\n",
        "        # Truncate sequences if they're too long\n",
        "        # Like cutting off sentences that are longer than our maximum allowed length\n",
        "        # Input: src_encoded - list of integers, potentially any length\n",
        "        # Output: src_encoded - same list but truncated to max_length if needed\n",
        "        if len(src_encoded) > self.max_length:\n",
        "            src_encoded = src_encoded[:self.max_length]\n",
        "        # Input: tgt_encoded_ip - list of integers, potentially any length\n",
        "        # Output: tgt_encoded_ip - same list but truncated to max_length if needed\n",
        "        if len(tgt_encoded_ip) > self.max_length:\n",
        "            tgt_encoded_ip = tgt_encoded_ip[:self.max_length]\n",
        "        # Input: tgt_encoded_op - list of integers, potentially any length\n",
        "        # Output: tgt_encoded_op - same list but truncated to max_length if needed\n",
        "        if len(tgt_encoded_op) > self.max_length:\n",
        "            tgt_encoded_op = tgt_encoded_op[:self.max_length]\n",
        "\n",
        "        # Pad all sequences to the same length by adding padding tokens\n",
        "        # Like adding blank spaces to make all sentences the same length\n",
        "        # Input: source vocab to get PAD token ID (usually 0)\n",
        "        # Output: pad - integer representing padding token ID\n",
        "        pad = self.src_vocab.get_vocab().get('<PAD>')\n",
        "\n",
        "        # Pad source sequence\n",
        "        # Input: src_encoded - list of variable length like [142, 89] (length 2)\n",
        "        # Processing: add PAD tokens to reach max_length\n",
        "        # Output: src_padded - list of exactly max_length integers like [142, 89, 0, 0, 0, ...]\n",
        "        src_padded = src_encoded + [pad] * (self.max_length - len(src_encoded))\n",
        "\n",
        "        # Pad target input sequence\n",
        "        # Input: tgt_encoded_ip - list of variable length like [2, 73, 156] (length 3)\n",
        "        # Processing: add PAD tokens to reach max_length\n",
        "        # Output: tgt_encoded_ip - list of exactly max_length integers like [2, 73, 156, 0, 0, ...]\n",
        "        tgt_encoded_ip = tgt_encoded_ip + [pad] * (self.max_length - len(tgt_encoded_ip))\n",
        "\n",
        "        # Pad target output sequence\n",
        "        # Input: tgt_encoded_op - list of variable length like [73, 156, 3] (length 3)\n",
        "        # Processing: add PAD tokens to reach max_length\n",
        "        # Output: tgt_encoded_op - list of exactly max_length integers like [73, 156, 3, 0, 0, ...]\n",
        "        tgt_encoded_op = tgt_encoded_op + [pad] * (self.max_length - len(tgt_encoded_op))\n",
        "\n",
        "        # Return the data in a format that PyTorch can understand\n",
        "        # Think of this as packaging three related pieces of data together\n",
        "        # Input: src_padded - Python list of integers with length max_length\n",
        "        #        tgt_encoded_ip - Python list of integers with length max_length\n",
        "        #        tgt_encoded_op - Python list of integers with length max_length\n",
        "        # Output: Dictionary with three PyTorch tensors:\n",
        "        #         'src': 1D tensor of shape [max_length] - source sentence (English)\n",
        "        #         'tgt_ip': 1D tensor of shape [max_length] - target input (SOS + Bengali)\n",
        "        #         'tgt_op': 1D tensor of shape [max_length] - target output (Bengali + EOS)\n",
        "        return {\n",
        "            'src': torch.tensor(src_padded, dtype=torch.long),       # Source sentence for encoder\n",
        "            'tgt_ip': torch.tensor(tgt_encoded_ip, dtype=torch.long), # Target input for decoder (what to feed in)\n",
        "            'tgt_op': torch.tensor(tgt_encoded_op, dtype=torch.long)  # Target output for decoder (what to expect out)\n",
        "        }"
      ],
      "metadata": {
        "id": "0dJzDrGfo0bM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading functions\n",
        "def load_monolingual_data(file_path, max_sentences):\n",
        "    \"\"\"\n",
        "    Loads monolingual text data from a file.\n",
        "    Returns a list of sentences (strings).\n",
        "\n",
        "    This function reads a text file containing sentences in a single language\n",
        "    and converts it into a Python list for machine learning training.\n",
        "    Think of it like reading a book and extracting each sentence into a list.\n",
        "    \"\"\"\n",
        "\n",
        "    # Print status message to track progress\n",
        "    # Input: file_path - string representing file location like \"/data/english.txt\"\n",
        "    # Output: Prints informational message to console\n",
        "    print(f\"Loading monolingual data from {file_path}...\")\n",
        "\n",
        "    # Initialize empty container to store all sentences\n",
        "    # Input: None (initialization)\n",
        "    # Output: sentences - empty Python list [], will grow as we read the file\n",
        "    sentences = []\n",
        "\n",
        "    # Open the text file for reading\n",
        "    # Input: file_path - string path to text file\n",
        "    #        'r' - read mode (not write mode)\n",
        "    #        encoding='utf-8' - handles international characters (Bengali, Chinese, etc.)\n",
        "    # Output: f - file handle object that lets us read the file line by line\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        # Read the file one line at a time\n",
        "        # This loop processes each line: line 0, line 1, line 2, etc.\n",
        "        # Input: f - file handle that provides lines one by one\n",
        "        # Processing: enumerate() gives us both line number (i) and line content (line)\n",
        "        for i, line in enumerate(f):\n",
        "            # Stop reading if we've reached our sentence limit\n",
        "            # This prevents loading too much data into memory\n",
        "            # Input: i - current line number (starts at 0)\n",
        "            #        max_sentences - integer limit like 10000\n",
        "            # Decision: if we've read enough sentences, exit the loop\n",
        "            if i >= max_sentences:\n",
        "                break\n",
        "\n",
        "            # Clean up the line by removing whitespace and newlines\n",
        "            # Input: line - raw string from file like \"Hello world\\n\" or \"  Good morning  \\n\"\n",
        "            # Processing: strip() removes spaces, tabs, newlines from both ends\n",
        "            # Output: line - cleaned string like \"Hello world\" or \"Good morning\"\n",
        "            line = line.strip()\n",
        "\n",
        "            # Only keep non-empty lines (skip blank lines)\n",
        "            # Input: line - cleaned string (could be empty \"\" after stripping)\n",
        "            # Decision: if line has content, add it to our collection\n",
        "            if line:  # Skip empty lines\n",
        "                # Add this sentence to our growing list\n",
        "                # Input: line - non-empty string like \"Hello world\"\n",
        "                # Operation: append() adds one item to the end of the list\n",
        "                # Output: sentences list grows by 1 item\n",
        "                # Example: sentences changes from [\"Hi\", \"Bye\"] to [\"Hi\", \"Bye\", \"Hello world\"]\n",
        "                sentences.append(line)\n",
        "\n",
        "    # Print summary statistics about what we loaded\n",
        "    # Input: sentences - Python list of strings\n",
        "    # Output: Prints count and file path to console\n",
        "    print(f\"Loaded {len(sentences)} sentences from {file_path}\")\n",
        "\n",
        "    # Print first few sentences as a sample to verify correct loading\n",
        "    # Input: sentences - Python list of strings like [\"Hello\", \"World\", \"How are you\"]\n",
        "    # Processing: sentences[:5] takes first 5 items (or all items if less than 5)\n",
        "    # Output: Prints first 5 sentences to console for verification\n",
        "    print(f\"First 5 sentences(load_monolingual_data): {sentences[:5]}\")\n",
        "\n",
        "    # Return the final collection of sentences\n",
        "    # Input: sentences - Python list of strings\n",
        "    # Output: Returns list of strings like [\"Hello world\", \"How are you\", \"Good morning\", ...]\n",
        "    # Dimensions: 1D list with length = number of sentences loaded (up to max_sentences)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "-8lauAhWphct"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_translation_data(file_path, max_sentences):\n",
        "    \"\"\"\n",
        "    Loads parallel translation pairs from a CSV file.\n",
        "    Returns two lists: English sentences and Bengali sentences.\n",
        "\n",
        "    This function reads a CSV file where each row contains a sentence pair:\n",
        "    one column for English, one column for Bengali (the same meaning).\n",
        "    Think of it like reading a two-column dictionary where each row is:\n",
        "    | English Column | Bengali Column |\n",
        "    | \"Hello world\"  | \"হ্যালো বিশ্ব\"  |\n",
        "    | \"How are you\"  | \"আপনি কেমন আছেন\" |\n",
        "    \"\"\"\n",
        "\n",
        "    # Print status message to track progress\n",
        "    # Input: file_path - string representing CSV file location like \"/data/translations.csv\"\n",
        "    # Output: Prints informational message to console\n",
        "    print(f\"Loading translation data from {file_path}...\")\n",
        "\n",
        "    # Read the CSV file into a pandas DataFrame (like a spreadsheet in memory)\n",
        "    # Input: file_path - string path to CSV file\n",
        "    # Output: df - pandas DataFrame object (like a 2D table with rows and columns)\n",
        "    # Dimensions: [number_of_rows, number_of_columns] - typically [N, 2] for translation pairs\n",
        "    # Example: df might look like:\n",
        "    #          en                bn\n",
        "    #    0     \"Hello world\"     \"হ্যালো বিশ্ব\"\n",
        "    #    1     \"How are you\"     \"আপনি কেমন আছেন\"\n",
        "    #    2     \"Good morning\"    \"সুপ্রভাত\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Remove any rows that have missing data (empty cells)\n",
        "    # Input: df - pandas DataFrame that might have some empty cells (NaN values)\n",
        "    # Processing: dropna() removes entire rows where any column has missing data\n",
        "    # Output: df - cleaned DataFrame with only complete rows\n",
        "    # Dimensions: [number_of_complete_rows, number_of_columns] - fewer or same rows as before\n",
        "    # Example: if row 5 had missing Bengali translation, that entire row gets removed\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Keep only the first max_sentences rows (limit data size)\n",
        "    # Input: df - pandas DataFrame with potentially many rows\n",
        "    #        max_sentences - integer limit like 10000\n",
        "    # Processing: head(n) takes the first n rows from the DataFrame\n",
        "    # Output: df - truncated DataFrame with at most max_sentences rows\n",
        "    # Dimensions: [min(original_rows, max_sentences), number_of_columns]\n",
        "    # Example: if df had 50000 rows and max_sentences=10000, result has 10000 rows\n",
        "    df = df.head(max_sentences)\n",
        "\n",
        "    # Extract English sentences from the 'en' column\n",
        "    # Input: df - pandas DataFrame with columns including 'en'\n",
        "    # Processing: df['en'] selects just the 'en' column (pandas Series)\n",
        "    #            .tolist() converts pandas Series to Python list\n",
        "    # Output: english_sentences - Python list of strings\n",
        "    # Dimensions: 1D list with length = number of rows in df\n",
        "    # Example: [\"Hello world\", \"How are you\", \"Good morning\", ...]\n",
        "    english_sentences = df['en'].tolist()\n",
        "\n",
        "    # Extract Bengali sentences from the 'bn' column\n",
        "    # Input: df - same pandas DataFrame with columns including 'bn'\n",
        "    # Processing: df['bn'] selects just the 'bn' column (pandas Series)\n",
        "    #            .tolist() converts pandas Series to Python list\n",
        "    # Output: bengali_sentences - Python list of strings\n",
        "    # Dimensions: 1D list with length = number of rows in df (same as english_sentences)\n",
        "    # Example: [\"হ্যালো বিশ্ব\", \"আপনি কেমন আছেন\", \"সুপ্রভাত\", ...]\n",
        "    bengali_sentences = df['bn'].tolist()\n",
        "\n",
        "    # Print summary statistics about what we loaded\n",
        "    # Input: english_sentences - Python list of strings\n",
        "    # Output: Prints count to console (both lists should have same length)\n",
        "    print(f\"Loaded {len(english_sentences)} translation pairs\")\n",
        "\n",
        "    # Return both lists as a tuple\n",
        "    # Input: english_sentences - 1D Python list of strings\n",
        "    #        bengali_sentences - 1D Python list of strings (same length)\n",
        "    # Output: Returns tuple of two lists\n",
        "    # Dimensions: (list[length=N], list[length=N]) where N is number of translation pairs\n",
        "    # Important: english_sentences[i] and bengali_sentences[i] are translations of each other\n",
        "    # Example: english_sentences[0] = \"Hello world\", bengali_sentences[0] = \"হ্যালো বিশ্ব\"\n",
        "    return english_sentences, bengali_sentences"
      ],
      "metadata": {
        "id": "EO90PQ4Upx8x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pretrain_dataloaders(english_sentences, bengali_sentences, src_vocab, tgt_vocab, config):\n",
        "    \"\"\"\n",
        "    Creates DataLoaders for pretraining (MLM) on English and Bengali monolingual data.\n",
        "\n",
        "    Think of this like preparing two separate assembly lines for processing text:\n",
        "    - One line processes English sentences\n",
        "    - Another line processes Bengali sentences\n",
        "\n",
        "    Each \"assembly line\" (DataLoader) will:\n",
        "    1. Take raw sentences and convert them to numbers (tokens)\n",
        "    2. Randomly mask some words (like filling in blanks in a test)\n",
        "    3. Group sentences into batches for efficient processing\n",
        "\n",
        "    Args:\n",
        "        english_sentences: List of English text strings\n",
        "                          Input: [\"Hello world\", \"How are you\", ...]\n",
        "        bengali_sentences: List of Bengali text strings\n",
        "                          Input: [\"হ্যালো বিশ্ব\", \"কেমন আছেন\", ...]\n",
        "        src_vocab: Dictionary mapping English words to numbers\n",
        "                  Input: {\"hello\": 1, \"world\": 2, ...}\n",
        "        tgt_vocab: Dictionary mapping Bengali words to numbers\n",
        "                  Input: {\"হ্যালো\": 1, \"বিশ্ব\": 2, ...}\n",
        "        config: Settings dictionary containing:\n",
        "                - max_length: Maximum sentence length (e.g., 128 tokens)\n",
        "                - mask_prob: Probability of masking words (e.g., 0.15 = 15%)\n",
        "                - batch_size: How many sentences to process together (e.g., 32)\n",
        "\n",
        "    Returns:\n",
        "        english_loader: DataLoader that outputs batches of English data\n",
        "                       Output shape: (batch_size, max_length) - matrix of token IDs\n",
        "        bengali_loader: DataLoader that outputs batches of Bengali data\n",
        "                       Output shape: (batch_size, max_length) - matrix of token IDs\n",
        "    \"\"\"\n",
        "    print(\"Creating pre-training data loaders...\")\n",
        "\n",
        "    # STEP 1: Create datasets (think of these as \"recipe books\" for processing text)\n",
        "    # Each dataset knows how to:\n",
        "    # - Convert text sentences to number sequences (tokenization)\n",
        "    # - Randomly hide some words and mark them as [MASK] tokens\n",
        "    # - Pad/truncate sentences to exact same length for batch processing\n",
        "\n",
        "    # English dataset: transforms raw English text into training examples\n",
        "    # Input: List of strings → Output: Tokenized & masked sequences\n",
        "    english_dataset = PretrainDataset(\n",
        "        english_sentences,      # Raw text: [\"Hello world\", ...]\n",
        "        src_vocab,             # Word→number mapping: {\"hello\": 1, ...}\n",
        "        config['max_length'],  # Fixed sequence length: 128 tokens\n",
        "        config['mask_prob']    # Masking probability: 15%\n",
        "    )\n",
        "\n",
        "    # Bengali dataset: same process but for Bengali text\n",
        "    # Input: List of strings → Output: Tokenized & masked sequences\n",
        "    bengali_dataset = PretrainDataset(\n",
        "        bengali_sentences,      # Raw text: [\"হ্যালো বিশ্ব\", ...]\n",
        "        tgt_vocab,             # Word→number mapping: {\"হ্যালো\": 1, ...}\n",
        "        config['max_length'],  # Fixed sequence length: 128 tokens\n",
        "        config['mask_prob']    # Masking probability: 15%\n",
        "    )\n",
        "\n",
        "    # STEP 2: Create data loaders (think of these as \"batch processors\")\n",
        "    # DataLoader groups individual examples into batches for efficient GPU processing\n",
        "    # Instead of processing one sentence at a time, we process many together\n",
        "\n",
        "    # English loader: packages English examples into batches\n",
        "    # Input: Individual sequences (max_length,) → Output: Batches (batch_size, max_length)\n",
        "    # Example: 32 individual sequences of 128 tokens each → 1 batch of shape (32, 128)\n",
        "    english_loader = DataLoader(\n",
        "        english_dataset,           # Dataset to draw examples from\n",
        "        batch_size=config['batch_size'],  # How many examples per batch (e.g., 32)\n",
        "        shuffle=True              # Randomize order each epoch for better training\n",
        "    )\n",
        "\n",
        "    # Bengali loader: same batching process for Bengali data\n",
        "    # Input: Individual sequences (max_length,) → Output: Batches (batch_size, max_length)\n",
        "    bengali_loader = DataLoader(\n",
        "        bengali_dataset,           # Dataset to draw examples from\n",
        "        batch_size=config['batch_size'],  # How many examples per batch (e.g., 32)\n",
        "        shuffle=True              # Randomize order each epoch for better training\n",
        "    )\n",
        "\n",
        "    # STEP 3: Report statistics for monitoring\n",
        "    # These numbers help you understand your data volume\n",
        "    print(f\"English pre-training samples: {len(english_dataset)}\")  # Total English examples\n",
        "    print(f\"Bengali pre-training samples: {len(bengali_dataset)}\")  # Total Bengali examples\n",
        "\n",
        "    # FINAL OUTPUT: Two data loaders that will feed the neural network\n",
        "    # Each loader yields batches of shape (batch_size, max_length)\n",
        "    # Example: If batch_size=32 and max_length=128, each batch is a 32×128 matrix\n",
        "    # where each row is one sentence represented as token IDs\n",
        "    return english_loader, bengali_loader"
      ],
      "metadata": {
        "id": "BkH7kXI1p6Xz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_translation_dataloaders(english_sentences, bengali_sentences, src_vocab, tgt_vocab, config):\n",
        "    \"\"\"\n",
        "    Creates DataLoaders for translation fine-tuning on parallel data.\n",
        "\n",
        "    Think of this like preparing two training programs for a translator:\n",
        "    - Training program: Practice translating with immediate feedback\n",
        "    - Validation program: Take tests without feedback to measure progress\n",
        "\n",
        "    Unlike the pretraining function that processes languages separately,\n",
        "    this function works with PAIRED sentences (English ↔ Bengali translations)\n",
        "\n",
        "    Args:\n",
        "        english_sentences: List of English sentences\n",
        "                          Input: [\"Hello world\", \"How are you\", ...]\n",
        "        bengali_sentences: List of Bengali sentences (translations of English)\n",
        "                          Input: [\"হ্যালো বিশ্ব\", \"কেমন আছেন\", ...]\n",
        "                          Note: bengali_sentences[i] is translation of english_sentences[i]\n",
        "        src_vocab: Dictionary mapping English words to numbers\n",
        "                  Input: {\"hello\": 1, \"world\": 2, ...}\n",
        "        tgt_vocab: Dictionary mapping Bengali words to numbers\n",
        "                  Input: {\"হ্যালো\": 1, \"বিশ্ব\": 2, ...}\n",
        "        config: Settings dictionary containing:\n",
        "                - max_length: Maximum sentence length (e.g., 128 tokens)\n",
        "                - batch_size: How many sentence pairs to process together (e.g., 32)\n",
        "\n",
        "    Returns:\n",
        "        train_loader: DataLoader for training pairs\n",
        "                     Output shape: (batch_size, max_length) for source + target\n",
        "        val_loader: DataLoader for validation pairs\n",
        "                   Output shape: (batch_size, max_length) for source + target\n",
        "    \"\"\"\n",
        "    print(\"Creating translation data loaders...\")\n",
        "\n",
        "    # STEP 1: Split data into training and validation sets\n",
        "    # This is like dividing your study materials into \"practice problems\" and \"final exam\"\n",
        "    # Standard split: 90% for training, 10% for validation\n",
        "\n",
        "    # Calculate split point: 90% of total data\n",
        "    # Input: Total count → Output: Split index\n",
        "    # Example: 10,000 sentences → split_idx = 9,000\n",
        "    split_idx = int(0.9 * len(english_sentences))\n",
        "\n",
        "    # Training data: First 90% of sentence pairs\n",
        "    # Input: Full lists → Output: Subset lists (90% of original size)\n",
        "    # Example: 10,000 pairs → 9,000 training pairs\n",
        "    train_src = english_sentences[:split_idx]    # English sentences [0:9000]\n",
        "    train_tgt = bengali_sentences[:split_idx]    # Bengali sentences [0:9000]\n",
        "\n",
        "    # Validation data: Last 10% of sentence pairs\n",
        "    # Input: Full lists → Output: Subset lists (10% of original size)\n",
        "    # Example: 10,000 pairs → 1,000 validation pairs\n",
        "    val_src = english_sentences[split_idx:]      # English sentences [9000:10000]\n",
        "    val_tgt = bengali_sentences[split_idx:]      # Bengali sentences [9000:10000]\n",
        "\n",
        "    # STEP 2: Create datasets (think of these as \"exercise generators\")\n",
        "    # Each dataset knows how to:\n",
        "    # - Take a sentence pair (English, Bengali)\n",
        "    # - Convert both to number sequences using respective vocabularies\n",
        "    # - Pad/truncate to same length for batch processing\n",
        "    # - Package them as training examples\n",
        "\n",
        "    # Training dataset: converts sentence pairs to training examples\n",
        "    # Input: Two lists of strings → Output: Paired tokenized sequences\n",
        "    # Each example: (source_tokens, target_tokens) where both have shape (max_length,)\n",
        "    train_dataset = TranslationDataset(\n",
        "        train_src,                 # English sentences for training\n",
        "        train_tgt,                 # Bengali sentences for training\n",
        "        src_vocab,                 # English word→number mapping\n",
        "        tgt_vocab,                 # Bengali word→number mapping\n",
        "        config['max_length']       # Fixed sequence length: 128 tokens\n",
        "    )\n",
        "\n",
        "    # Validation dataset: same process but for validation pairs\n",
        "    # Input: Two lists of strings → Output: Paired tokenized sequences\n",
        "    val_dataset = TranslationDataset(\n",
        "        val_src,                   # English sentences for validation\n",
        "        val_tgt,                   # Bengali sentences for validation\n",
        "        src_vocab,                 # English word→number mapping\n",
        "        tgt_vocab,                 # Bengali word→number mapping\n",
        "        config['max_length']       # Fixed sequence length: 128 tokens\n",
        "    )\n",
        "\n",
        "    # STEP 3: Create data loaders (think of these as \"batch processors\")\n",
        "    # These group individual sentence pairs into batches for efficient processing\n",
        "    # Instead of translating one sentence at a time, we translate many in parallel\n",
        "\n",
        "    # Training loader: packages training pairs into batches\n",
        "    # Input: Individual pairs (max_length,) + (max_length,) → Output: Batches\n",
        "    # Output shape: Two tensors of (batch_size, max_length) each\n",
        "    # Example: 32 pairs → batch of 32 English + batch of 32 Bengali sequences\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,             # Dataset to draw pairs from\n",
        "        batch_size=config['batch_size'],  # How many pairs per batch (e.g., 32)\n",
        "        shuffle=True              # Randomize order each epoch for better learning\n",
        "    )\n",
        "\n",
        "    # Validation loader: same batching but without shuffling\n",
        "    # Input: Individual pairs (max_length,) + (max_length,) → Output: Batches\n",
        "    # Output shape: Two tensors of (batch_size, max_length) each\n",
        "    # shuffle=False ensures consistent validation results across runs\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,               # Dataset to draw pairs from\n",
        "        batch_size=config['batch_size'],  # How many pairs per batch (e.g., 32)\n",
        "        shuffle=False             # Keep same order for reproducible validation\n",
        "    )\n",
        "\n",
        "    # STEP 4: Report statistics for monitoring\n",
        "    # These numbers help you track your dataset sizes and training progress\n",
        "    print(f\"Translation training samples: {len(train_dataset)}\")      # e.g., 9,000 pairs\n",
        "    print(f\"Translation validation samples: {len(val_dataset)}\")      # e.g., 1,000 pairs\n",
        "\n",
        "    # FINAL OUTPUT: Two data loaders for training and validation\n",
        "    # Each loader yields batches with TWO components:\n",
        "    # 1. Source batch: (batch_size, max_length) - English sentences as token IDs\n",
        "    # 2. Target batch: (batch_size, max_length) - Bengali sentences as token IDs\n",
        "    #\n",
        "    # Example usage in training loop:\n",
        "    # for src_batch, tgt_batch in train_loader:\n",
        "    #     # src_batch shape: (32, 128) - 32 English sentences of 128 tokens each\n",
        "    #     # tgt_batch shape: (32, 128) - 32 Bengali sentences of 128 tokens each\n",
        "    #     # Model learns to translate src_batch → tgt_batch\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "9PuPELrjrR9N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Utility for early stopping during training to prevent overfitting.\n",
        "\n",
        "    Think of this like a fitness coach monitoring your workout progress:\n",
        "    - If you stop improving for several sessions, the coach stops the program\n",
        "    - This prevents you from overtraining and getting worse results\n",
        "\n",
        "    In machine learning terms:\n",
        "    - Monitors validation loss (how well model performs on unseen data)\n",
        "    - If model stops improving for 'patience' epochs, stops training\n",
        "    - Prevents overfitting (memorizing training data instead of learning patterns)\n",
        "\n",
        "    This is a STATEFUL object that remembers training history across epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, min_delta=0.000001):\n",
        "        \"\"\"\n",
        "        Initialize the early stopping monitor.\n",
        "\n",
        "        Args:\n",
        "            patience: How many epochs to wait without improvement before stopping\n",
        "                     Input: Integer (e.g., 5)\n",
        "                     Think of this as \"strikes\" - after 5 strikes, you're out\n",
        "            min_delta: Minimum change to qualify as an improvement\n",
        "                      Input: Float (e.g., 0.000001)\n",
        "                      Prevents stopping due to tiny random fluctuations\n",
        "\n",
        "        Internal State Variables:\n",
        "            counter: Tracks consecutive epochs without improvement\n",
        "                    Type: Integer, Range: [0, patience]\n",
        "            best_loss: Best validation loss seen so far\n",
        "                      Type: Float, Initial: infinity (worst possible)\n",
        "        \"\"\"\n",
        "        # CONFIGURATION: Set stopping criteria\n",
        "        self.patience = patience        # Max epochs to wait (e.g., 5)\n",
        "        self.min_delta = min_delta      # Minimum improvement threshold (e.g., 0.000001)\n",
        "\n",
        "        # STATE TRACKING: Initialize monitoring variables\n",
        "        self.counter = 0                # Consecutive epochs without improvement\n",
        "                                       # Range: [0, patience]\n",
        "                                       # 0 = just improved, patience = time to stop\n",
        "\n",
        "        self.best_loss = float('inf')   # Best validation loss seen so far\n",
        "                                       # Starts at infinity (worst possible)\n",
        "                                       # Gets updated when we see improvements\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        \"\"\"\n",
        "        Check if training should stop based on current validation loss.\n",
        "\n",
        "        This method is called after each epoch with the latest validation loss.\n",
        "        It acts like a \"judge\" deciding whether to continue or stop training.\n",
        "\n",
        "        Args:\n",
        "            val_loss: Current epoch's validation loss\n",
        "                     Input: Single float value (e.g., 2.345)\n",
        "                     Lower values = better model performance\n",
        "\n",
        "        Returns:\n",
        "            should_stop: Boolean indicating whether to stop training\n",
        "                        Output: True = stop training, False = continue\n",
        "\n",
        "        Logic Flow:\n",
        "            1. Compare current loss with best loss seen so far\n",
        "            2. If improved significantly → reset counter, update best\n",
        "            3. If not improved → increment counter\n",
        "            4. If counter reaches patience → return True (stop)\n",
        "        \"\"\"\n",
        "\n",
        "        # STEP 1: Check if current loss is significantly better than best loss\n",
        "        # We use \"best_loss - min_delta\" to avoid stopping on tiny random improvements\n",
        "        #\n",
        "        # Input dimensions:\n",
        "        # - val_loss: scalar float\n",
        "        # - self.best_loss: scalar float\n",
        "        # - self.min_delta: scalar float\n",
        "        # Output: boolean comparison result\n",
        "\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            # IMPROVEMENT DETECTED: Current loss is significantly better\n",
        "            # Example: best_loss=2.5, min_delta=0.001, val_loss=2.3\n",
        "            # Check: 2.3 < 2.5 - 0.001 = 2.499 ✓ (True)\n",
        "\n",
        "            # Update tracking state\n",
        "            self.best_loss = val_loss   # New best loss (scalar → scalar)\n",
        "            self.counter = 0            # Reset patience counter (int → 0)\n",
        "\n",
        "            # Model is still improving, continue training\n",
        "\n",
        "        else:\n",
        "            # NO IMPROVEMENT: Current loss is not significantly better\n",
        "            # Example: best_loss=2.5, min_delta=0.001, val_loss=2.51\n",
        "            # Check: 2.51 < 2.5 - 0.001 = 2.499 ✗ (False)\n",
        "\n",
        "            # Increment the \"strikes\" counter\n",
        "            self.counter += 1           # Increment counter (int → int + 1)\n",
        "\n",
        "            # Model is not improving, getting closer to stopping\n",
        "\n",
        "        # STEP 2: Decide whether to stop training\n",
        "        # Compare counter with patience threshold\n",
        "        # Input: self.counter (int), self.patience (int)\n",
        "        # Output: boolean (True = stop, False = continue)\n",
        "\n",
        "        should_stop = self.counter >= self.patience\n",
        "\n",
        "        # Examples:\n",
        "        # - counter=3, patience=5 → 3 >= 5 = False (continue)\n",
        "        # - counter=5, patience=5 → 5 >= 5 = True (stop)\n",
        "\n",
        "        return should_stop\n",
        "\n",
        "    # USAGE EXAMPLE:\n",
        "    # early_stopper = EarlyStopping(patience=5, min_delta=0.001)\n",
        "    #\n",
        "    # for epoch in range(100):\n",
        "    #     # ... training code ...\n",
        "    #     val_loss = validate_model()  # Returns scalar float\n",
        "    #\n",
        "    #     if early_stopper(val_loss):  # Pass scalar, get boolean\n",
        "    #         print(f\"Early stopping at epoch {epoch}\")\n",
        "    #         break\n",
        "    #\n",
        "    # State evolution example:\n",
        "    # Epoch 1: val_loss=3.0 → best_loss=3.0, counter=0, continue\n",
        "    # Epoch 2: val_loss=2.5 → best_loss=2.5, counter=0, continue\n",
        "    # Epoch 3: val_loss=2.6 → best_loss=2.5, counter=1, continue\n",
        "    # Epoch 4: val_loss=2.7 → best_loss=2.5, counter=2, continue\n",
        "    # ...\n",
        "    # Epoch 8: val_loss=2.8 → best_loss=2.5, counter=5, STOP!"
      ],
      "metadata": {
        "id": "p7pIMZ0CrYdN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === HELPER: BATCH ACCURACY ===\n",
        "def batch_accuracy(logits, labels, ignore_index):\n",
        "    \"\"\"\n",
        "    Compute token-level accuracy, ignoring positions with `ignore_index`.\n",
        "\n",
        "    Think of this like grading a multiple-choice test where some answers should be ignored:\n",
        "    - logits: The model's \"confidence scores\" for each possible answer\n",
        "    - labels: The correct answers (ground truth)\n",
        "    - ignore_index: Special marker for \"skip this question\" (like padding tokens)\n",
        "\n",
        "    The function converts confidence scores to actual predictions, then calculates\n",
        "    what percentage of predictions match the correct answers (excluding skipped ones).\n",
        "\n",
        "    Args:\n",
        "        logits: Model's raw prediction scores for each token\n",
        "               Shape: (batch_size, sequence_length, vocab_size)\n",
        "               Example: (32, 128, 50000) - 32 sentences, 128 tokens each, 50k possible words\n",
        "               Values: Real numbers (can be negative) representing confidence\n",
        "\n",
        "        labels: Correct token IDs (ground truth)\n",
        "               Shape: (batch_size, sequence_length)\n",
        "               Example: (32, 128) - 32 sentences, 128 tokens each\n",
        "               Values: Integers from 0 to vocab_size-1, plus ignore_index\n",
        "\n",
        "        ignore_index: Special token ID to ignore during accuracy calculation\n",
        "                     Type: Integer (commonly -100 or 0)\n",
        "                     Used for: Padding tokens, special tokens we don't want to evaluate\n",
        "\n",
        "    Returns:\n",
        "        acc: Token-level accuracy as a float between 0.0 and 1.0\n",
        "            Type: Float\n",
        "            Example: 0.85 means 85% of (non-ignored) tokens were predicted correctly\n",
        "    \"\"\"\n",
        "    # Disable gradient computation for efficiency (we're just measuring, not training)\n",
        "    # This is like telling the system \"don't track this for learning, just calculate\"\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # STEP 1: Convert confidence scores to actual predictions\n",
        "        # logits contains confidence scores for each possible word in vocabulary\n",
        "        # We pick the word with highest confidence score\n",
        "\n",
        "        # Find the index of maximum value along vocabulary dimension\n",
        "        # Input shape: (batch_size, sequence_length, vocab_size)\n",
        "        # Output shape: (batch_size, sequence_length)\n",
        "        # Example: (32, 128, 50000) → (32, 128)\n",
        "        # Operation: For each position, argmax picks the most confident word ID\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # STEP 2: Create mask to identify which positions to evaluate\n",
        "        # We want to ignore certain positions (like padding tokens)\n",
        "\n",
        "        # Create boolean mask: True where we should count, False where we should ignore\n",
        "        # Input shapes: labels (32, 128), ignore_index (scalar)\n",
        "        # Output shape: (32, 128) - same as labels\n",
        "        # Values: True/False for each token position\n",
        "        mask = labels != ignore_index\n",
        "\n",
        "        # STEP 3: Safety check to avoid division by zero\n",
        "        # If all tokens are ignored (very rare), return 0 accuracy\n",
        "\n",
        "        # Count how many positions we're actually evaluating\n",
        "        # Input shape: (32, 128) boolean mask\n",
        "        # Output: Scalar integer (total number of True values)\n",
        "        if mask.sum() == 0:\n",
        "            return 0.0  # avoid div/0 when all tokens are ignored (rare edge case)\n",
        "\n",
        "        # STEP 4: Calculate accuracy\n",
        "        # Find positions where prediction matches ground truth AND we should evaluate\n",
        "\n",
        "        # Check correctness: True where prediction equals ground truth\n",
        "        # Input shapes: preds (32, 128), labels (32, 128)\n",
        "        # Output shape: (32, 128) - boolean tensor\n",
        "        # Then combine with mask using logical AND (&)\n",
        "        correct = (preds == labels) & mask\n",
        "\n",
        "        # STEP 5: Compute final accuracy percentage\n",
        "        # Count correct predictions and divide by total evaluable positions\n",
        "\n",
        "        # Convert boolean tensor to count of True values\n",
        "        # Input: correct (32, 128) boolean tensor\n",
        "        # Output: Scalar integer (number of correct predictions)\n",
        "        num_correct = correct.sum().item()\n",
        "\n",
        "        # Convert boolean mask to count of True values\n",
        "        # Input: mask (32, 128) boolean tensor\n",
        "        # Output: Scalar integer (number of positions to evaluate)\n",
        "        num_total = mask.sum().item()\n",
        "\n",
        "        # Calculate accuracy as percentage\n",
        "        # Input: Two scalar integers\n",
        "        # Output: Float between 0.0 and 1.0\n",
        "        acc = num_correct / num_total\n",
        "\n",
        "        return acc\n",
        "\n",
        "# EXAMPLE WALKTHROUGH:\n",
        "# If we have a batch with:\n",
        "# - logits shape: (2, 4, 1000) - 2 sentences, 4 tokens each, 1000 vocab words\n",
        "# - labels: [[1, 5, 999, -100], [2, 8, 15, -100]] - correct token IDs\n",
        "# - ignore_index: -100 (padding tokens)\n",
        "#\n",
        "# Step 1: argmax gives predictions: [[1, 3, 999, 200], [2, 8, 12, 500]]\n",
        "# Step 2: mask identifies non-padding: [[True, True, True, False], [True, True, True, False]]\n",
        "# Step 3: We have 6 positions to evaluate (not 8, because 2 are padding)\n",
        "# Step 4: correct positions: [[True, False, True, False], [True, True, False, False]]\n",
        "# Step 5: accuracy = 4 correct / 6 total = 0.667 (66."
      ],
      "metadata": {
        "id": "mLevuWOZrcK0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Noam Learning-Rate Scheduler (from \"Attention Is All You Need\") ===\n",
        "def get_noam_scheduler(optimizer, d_model, warmup_steps):\n",
        "    \"\"\"\n",
        "    Implements the Noam learning rate schedule from the original Transformer paper.\n",
        "\n",
        "    Think of this like a sophisticated training plan for an athlete:\n",
        "    - Phase 1 (Warmup): Gradually increase training intensity from zero\n",
        "    - Phase 2 (Decay): Slowly decrease intensity as performance improves\n",
        "\n",
        "    This prevents the model from learning too aggressively at first (which can cause\n",
        "    instability) while ensuring it continues learning throughout training.\n",
        "\n",
        "    The learning rate curve looks like: /\\_____ (ramp up, then gradual decay)\n",
        "\n",
        "    Args:\n",
        "        optimizer: The optimization algorithm (like Adam, SGD) that updates model weights\n",
        "                  Type: torch.optim.Optimizer object\n",
        "                  Contains: Current learning rate, momentum, weight decay settings\n",
        "\n",
        "        d_model: Model dimension (size of hidden representations)\n",
        "                Type: Integer (commonly 512, 768, 1024)\n",
        "                Purpose: Used to scale the learning rate based on model size\n",
        "                Rationale: Larger models need smaller learning rates for stability\n",
        "\n",
        "        warmup_steps: Number of steps for the warmup phase\n",
        "                     Type: Integer (commonly 4000, 8000, 16000)\n",
        "                     Purpose: How long to gradually increase learning rate from 0\n",
        "\n",
        "    Returns:\n",
        "        scheduler: Learning rate scheduler object\n",
        "                  Type: torch.optim.lr_scheduler.LambdaLR\n",
        "                  Usage: Call scheduler.step() after each training step\n",
        "                  Effect: Automatically adjusts optimizer's learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    # INNER FUNCTION: Calculates learning rate multiplier for any given step\n",
        "    # This function will be called automatically by PyTorch's scheduler\n",
        "    def lr_lambda(step):\n",
        "        \"\"\"\n",
        "        Calculate learning rate multiplier for a given training step.\n",
        "\n",
        "        Args:\n",
        "            step: Current training step (starts at 0)\n",
        "                 Type: Integer\n",
        "                 Range: 0, 1, 2, 3, ... (increases throughout training)\n",
        "\n",
        "        Returns:\n",
        "            multiplier: Factor to multiply base learning rate by\n",
        "                       Type: Float\n",
        "                       Range: 0.0 to 1.0 (typically)\n",
        "        \"\"\"\n",
        "\n",
        "        # STEP 1: Adjust step counting (PyTorch starts at 0, but we need 1-based)\n",
        "        # Input: step (integer, 0-based)\n",
        "        # Output: step (integer, 1-based)\n",
        "        # Example: step=0 becomes step=1, step=1 becomes step=2\n",
        "        step += 1  # step counting starts at 0 in LambdaLR\n",
        "\n",
        "        # STEP 2: Calculate the core learning rate formula\n",
        "        # This implements the Noam schedule: lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
        "\n",
        "        # Calculate model size scaling factor\n",
        "        # Input: d_model (integer, e.g., 512)\n",
        "        # Output: scalar float (e.g., 512^(-0.5) = 0.044)\n",
        "        # Purpose: Larger models get smaller base learning rates\n",
        "        model_scale = d_model ** -0.5\n",
        "\n",
        "        # Calculate step-based component (the heart of the schedule)\n",
        "        # Two competing terms:\n",
        "        # 1. step^(-0.5): Decay term - gets smaller as training progresses\n",
        "        #    Input: step (integer) → Output: float\n",
        "        #    Example: step=100 → 100^(-0.5) = 0.1\n",
        "        #\n",
        "        # 2. step * (warmup_steps^(-1.5)): Warmup term - linear increase initially\n",
        "        #    Input: step (integer), warmup_steps (integer) → Output: float\n",
        "        #    Example: step=1000, warmup_steps=4000 → 1000 * (4000^(-1.5)) = 0.004\n",
        "        #\n",
        "        # min() chooses the smaller value:\n",
        "        # - During warmup: warmup term is smaller → linear increase\n",
        "        # - After warmup: decay term is smaller → gradual decrease\n",
        "        step_scale = min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
        "\n",
        "        # Combine model scaling with step scaling\n",
        "        # Input: Two floats → Output: Float\n",
        "        # This gives the raw learning rate value\n",
        "        scale = model_scale * step_scale\n",
        "\n",
        "        # STEP 3: Normalize the learning rate\n",
        "        # We want the peak learning rate (at warmup_steps) to be 1.0\n",
        "        # This makes it easier to set a reasonable base learning rate in the optimizer\n",
        "\n",
        "        # Calculate what the learning rate would be at the peak (end of warmup)\n",
        "        # Input: d_model (int), warmup_steps (int) → Output: float\n",
        "        # This is the maximum value the schedule will reach\n",
        "        normaliser = (d_model ** -0.5) * (warmup_steps ** -1.5)\n",
        "\n",
        "        # Normalize so peak learning rate = 1.0\n",
        "        # Input: scale (float), normaliser (float) → Output: float (0.0 to 1.0)\n",
        "        # At warmup_steps: scale = normaliser, so this returns 1.0\n",
        "        # Before warmup_steps: this returns < 1.0 (gradual increase)\n",
        "        # After warmup_steps: this returns < 1.0 (gradual decrease)\n",
        "        return scale / normaliser\n",
        "\n",
        "    # STEP 4: Create the actual scheduler object\n",
        "    # LambdaLR will call lr_lambda(step) at each step and multiply the base learning rate\n",
        "    # Input: optimizer (object), lr_lambda (function) → Output: scheduler (object)\n",
        "    #\n",
        "    # How it works:\n",
        "    # - Base learning rate in optimizer: e.g., 0.001\n",
        "    # - lr_lambda returns: e.g., 0.5\n",
        "    # - Actual learning rate used: 0.001 * 0.5 = 0.0005\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# EXAMPLE LEARNING RATE SCHEDULE:\n",
        "# Assume d_model=512, warmup_steps=4000, base_lr=0.001\n",
        "#\n",
        "# Step 1000: lr_lambda ≈ 0.25 → actual_lr = 0.001 * 0.25 = 0.00025\n",
        "# Step 4000: lr_lambda = 1.0  → actual_lr = 0.001 * 1.0  = 0.001    (peak)\n",
        "# Step 8000: lr_lambda ≈ 0.71 → actual_lr = 0.001 * 0.71 = 0.00071\n",
        "# Step 16000: lr_lambda ≈ 0.5 → actual_lr = 0.001 * 0.5  = 0.0005\n",
        "#\n",
        "# USAGE IN TRAINING LOOP:\n",
        "# scheduler = get_noam_scheduler(optimizer, d_model=512, warmup_steps=4000)\n",
        "# for step in range(num_steps):\n",
        "#     # ... training step ...\n",
        "#     optimizer.step()      # Update model weights\n",
        "#     scheduler.step()      # Update learning rate\n",
        "#     # Current LR: optimizer.param_groups[0]['lr']"
      ],
      "metadata": {
        "id": "WGGUdmibrnxa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === PRETRAIN MODEL: MLM on monolingual data ===\n",
        "def pretrain_model(model, english_sentences, bengali_sentences, src_vocab, tgt_vocab, config, overall_start_time=None):\n",
        "    \"\"\"\n",
        "    Pre-train the transformer model using Masked Language Modeling (MLM).\n",
        "\n",
        "    Think of this like teaching a student to fill in missing words in sentences\n",
        "    before they learn to translate between languages. This is a two-part process:\n",
        "\n",
        "    1. Encoder learns English: Given \"The cat [MASK] on the mat\" → predict \"sat\"\n",
        "    2. Decoder learns Bengali: Given \"বিড়াল [MASK] এ বসে\" → predict \"মাদুরে\"\n",
        "\n",
        "    This foundational learning helps the model understand both languages deeply\n",
        "    before attempting translation between them.\n",
        "\n",
        "    Args:\n",
        "        model: The transformer neural network to train\n",
        "              Type: PyTorch model object\n",
        "              Contains: Encoder, decoder, and prediction heads\n",
        "\n",
        "        english_sentences: List of English text for encoder training\n",
        "                          Input: [\"Hello world\", \"How are you\", ...]\n",
        "                          Size: Variable (e.g., 100,000 sentences)\n",
        "\n",
        "        bengali_sentences: List of Bengali text for decoder training\n",
        "                          Input: [\"হ্যালো বিশ্ব\", \"কেমন আছেন\", ...]\n",
        "                          Size: Variable (e.g., 100,000 sentences)\n",
        "\n",
        "        src_vocab: English vocabulary (word to number mapping)\n",
        "                  Input: {\"hello\": 1, \"world\": 2, ...}\n",
        "                  Size: ~50,000 words typically\n",
        "\n",
        "        tgt_vocab: Bengali vocabulary (word to number mapping)\n",
        "                  Input: {\"হ্যালো\": 1, \"বিশ্ব\": 2, ...}\n",
        "                  Size: ~50,000 words typically\n",
        "\n",
        "        config: Training configuration dictionary\n",
        "               Contains: batch_size, learning_rate, epochs, etc.\n",
        "\n",
        "        overall_start_time: Optional timestamp for global timing\n",
        "                           Type: Float (from time.time())\n",
        "\n",
        "    Returns:\n",
        "        model: The trained model with learned parameters\n",
        "        train_losses: List of training error values over time\n",
        "        val_losses: List of validation errors (empty in this version)\n",
        "        pretrain_accs: List of accuracy measurements during training\n",
        "    \"\"\"\n",
        "    print(\"=== Starting Pre-training Phase ===\")\n",
        "\n",
        "    # STEP 1: Setup computing environment\n",
        "    # Choose between GPU (fast) or CPU (slower but always available)\n",
        "    device = torch.device(config['device'])  # Usually 'cuda' or 'cpu'\n",
        "\n",
        "    # Move model to chosen device (like loading software on the right computer)\n",
        "    # Input: Model object → Output: Same model, but on GPU/CPU\n",
        "    model = model.to(device)\n",
        "\n",
        "    # STEP 2: Create training datasets\n",
        "    # These will convert raw text into training examples with masked words\n",
        "    print(\"Creating pre-training datasets...\")\n",
        "\n",
        "    # English dataset: Creates fill-in-the-blank exercises from English sentences\n",
        "    # Input: Raw sentences → Output: Masked sequences + labels\n",
        "    # Example: \"Hello world\" → input_ids: [1, 103, 2], labels: [-100, 2, -100]\n",
        "    # where 103 = [MASK] token, -100 = ignore this position\n",
        "    english_dataset = PretrainDataset(\n",
        "        english_sentences,                    # Raw English text\n",
        "        src_vocab,                           # English word→number mapping\n",
        "        config['max_length'],                # Max sequence length (e.g., 128)\n",
        "        config['mask_probability']           # Fraction of words to mask (e.g., 0.15)\n",
        "    )\n",
        "\n",
        "    # Bengali dataset: Same process for Bengali sentences\n",
        "    # Input: Raw sentences → Output: Masked sequences + labels\n",
        "    bengali_dataset = PretrainDataset(\n",
        "        bengali_sentences,                   # Raw Bengali text\n",
        "        tgt_vocab,                          # Bengali word→number mapping\n",
        "        config['max_length'],                # Max sequence length (e.g., 128)\n",
        "        config['mask_probability']           # Fraction of words to mask (e.g., 0.15)\n",
        "    )\n",
        "\n",
        "    # STEP 3: Create data loaders (batch processors)\n",
        "    # These group individual examples into batches for efficient training\n",
        "\n",
        "    # English loader: Packages English examples into batches\n",
        "    # Input: Individual sequences → Output: Batches of shape (batch_size, max_length)\n",
        "    # Example: 32 sequences of 128 tokens each → (32, 128) tensor\n",
        "    english_loader = DataLoader(\n",
        "        english_dataset,\n",
        "        batch_size=config['batch_size'],     # e.g., 32 examples per batch\n",
        "        shuffle=True                         # Randomize order each epoch\n",
        "    )\n",
        "\n",
        "    # Bengali loader: Same batching for Bengali data\n",
        "    bengali_loader = DataLoader(\n",
        "        bengali_dataset,\n",
        "        batch_size=config['batch_size'],     # e.g., 32 examples per batch\n",
        "        shuffle=True                         # Randomize order each epoch\n",
        "    )\n",
        "\n",
        "    # Report dataset sizes\n",
        "    print(f\"English pre-training samples: {len(english_dataset)}\")\n",
        "    print(f\"Bengali pre-training samples: {len(bengali_dataset)}\")\n",
        "\n",
        "    # STEP 4: Setup training components\n",
        "\n",
        "    # Loss function: Measures how wrong the model's predictions are\n",
        "    # CrossEntropyLoss compares predicted word probabilities vs correct answers\n",
        "    # ignore_index=-100: Skip these positions when calculating error\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    # Optimizer: The \"learning algorithm\" that updates model weights\n",
        "    # Adam is a sophisticated method that adapts learning speed automatically\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),                  # All model weights to update\n",
        "        lr=config['pretrain_learning_rate'], # Base learning rate (e.g., 0.0001)\n",
        "        betas=(0.9, 0.98),                  # Momentum parameters\n",
        "        eps=1e-9                            # Numerical stability constant\n",
        "    )\n",
        "\n",
        "    # Scheduler: Adjusts learning rate during training (Noam schedule)\n",
        "    # Starts slow, ramps up, then gradually decreases\n",
        "    scheduler = get_noam_scheduler(\n",
        "        optimizer,\n",
        "        config['d_model'],                   # Model dimension (e.g., 512)\n",
        "        config['warmup_steps']               # Warmup duration (e.g., 4000 steps)\n",
        "    )\n",
        "\n",
        "    # Early stopping: Prevents overfitting by stopping when performance plateaus\n",
        "    early_stopping = EarlyStopping(patience=config['patience'])\n",
        "\n",
        "    # STEP 5: Initialize tracking variables\n",
        "    # These lists store performance metrics over time\n",
        "    train_losses = []      # Training error at each epoch\n",
        "    val_losses = []        # Validation error (empty in this version)\n",
        "    pretrain_accs = []     # Accuracy measurements during training\n",
        "\n",
        "    # Record training start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # STEP 6: Main training loop\n",
        "    # Repeat the learning process for multiple epochs\n",
        "    for epoch in range(config['pretrain_epochs']):\n",
        "\n",
        "        # TIMING: Calculate elapsed time and check limits\n",
        "        time_elapsed = time.time() - start_time\n",
        "        global_time_elapsed = time.time() - overall_start_time if overall_start_time is not None else 0\n",
        "\n",
        "        print(f\"Time elapsed: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s (phase), {global_time_elapsed // 60:.0f}m {global_time_elapsed % 60:.0f}s (overall)\")\n",
        "\n",
        "        # Stop if training is taking too long\n",
        "        if time_elapsed > config['max_train_minutes'] * 60:\n",
        "            print(\"Time limit exceeded. Stopping pre-training.\")\n",
        "            break\n",
        "        if overall_start_time is not None and global_time_elapsed > config['max_global_minutes'] * 60:\n",
        "            print(\"Global time limit exceeded. Stopping pre-training.\")\n",
        "            break\n",
        "\n",
        "        # Set model to training mode (enables dropout, batch norm updates)\n",
        "        model.train()\n",
        "\n",
        "        # Initialize epoch tracking variables\n",
        "        epoch_train_loss = 0  # Cumulative error for this epoch\n",
        "        batch_count = 0       # Number of batches processed\n",
        "\n",
        "        # PHASE 1: Train encoder on English data\n",
        "        print(f\"Training encoder (English) - Epoch {epoch+1}\")\n",
        "        for batch in tqdm(english_loader, desc=f\"Encoder MLM (English) Epoch {epoch+1}\", disable=CONFIG['tqdm_disable']):\n",
        "\n",
        "            # Extract batch data and move to device (GPU/CPU)\n",
        "            # input_ids: sentences with some words replaced by [MASK] tokens\n",
        "            # labels: the original words that were masked\n",
        "            input_ids = batch['input_ids'].to(device)  # Shape: (batch_size, seq_length)\n",
        "            labels = batch['labels'].to(device)        # Shape: (batch_size, seq_length)\n",
        "\n",
        "            # DEBUG: Print shapes for understanding\n",
        "            #print(f\"input_ids shape: {input_ids.shape}\")  # e.g., (32, 128)\n",
        "            #print(f\"labels shape: {labels.shape}\")        # e.g., (32, 128)\n",
        "\n",
        "            # Clear gradients from previous step (like erasing a whiteboard)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # FORWARD PASS: Get model predictions\n",
        "            # Model tries to predict original words from masked sentences\n",
        "            # Input: (batch_size, seq_length) → Output: (batch_size, seq_length, vocab_size)\n",
        "            logits = model.mlm_encode(input_ids)\n",
        "            #print(f\"logits shape: {logits.shape}\")  # e.g., (32, 128, 50000)\n",
        "\n",
        "            # Verify shapes match for loss calculation\n",
        "            assert logits.shape[:2] == labels.shape, f\"Logits shape {logits.shape} and labels shape {labels.shape} do not match for loss\"\n",
        "\n",
        "            # LOSS CALCULATION: How wrong were the predictions?\n",
        "            # Reshape tensors for CrossEntropyLoss\n",
        "            # Input: logits (batch_size, seq_length, vocab_size) → (batch_size*seq_length, vocab_size)\n",
        "            # Input: labels (batch_size, seq_length) → (batch_size*seq_length,)\n",
        "            # Output: scalar loss value\n",
        "            loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),  # Flatten: (32*128, 50000)\n",
        "                labels.view(-1)                    # Flatten: (32*128,)\n",
        "            )\n",
        "\n",
        "            # BACKWARD PASS: Learn from mistakes\n",
        "            # Calculate gradients (how to improve each weight)\n",
        "            # PyTorch: automatic differentiation (autograd); get the context from 'logits'\n",
        "            loss.backward()\n",
        "\n",
        "            # UPDATE: Apply the learned improvements\n",
        "            optimizer.step()\n",
        "\n",
        "            # SCHEDULING: Adjust learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            # TRACKING: Record performance metrics\n",
        "            epoch_train_loss += loss.item()  # Add to cumulative loss\n",
        "            batch_count += 1\n",
        "\n",
        "            # Calculate accuracy (percentage of correct predictions)\n",
        "            # Input: logits (batch_size, seq_length, vocab_size), labels (batch_size, seq_length)\n",
        "            # Output: scalar accuracy (0.0 to 1.0)\n",
        "            acc = batch_accuracy(logits, labels, -100)\n",
        "            pretrain_accs.append(acc)\n",
        "\n",
        "        # PHASE 2: Train decoder on Bengali data\n",
        "        print(f\"Training decoder (Bengali) - Epoch {epoch+1}\")\n",
        "        for batch in tqdm(bengali_loader, desc=f\"Decoder MLM (Bengali) Epoch {epoch+1}\", disable=CONFIG['tqdm_disable']):\n",
        "\n",
        "            # Extract batch data and move to device\n",
        "            input_ids = batch['input_ids'].to(device)  # Shape: (batch_size, seq_length)\n",
        "            labels = batch['labels'].to(device)        # Shape: (batch_size, seq_length)\n",
        "\n",
        "            # DEBUG: Print shapes for understanding\n",
        "            #print(f\"input_ids shape: {input_ids.shape}\")\n",
        "            #print(f\"labels shape: {labels.shape}\")\n",
        "\n",
        "            # Clear gradients from previous step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # FORWARD PASS: Get decoder predictions\n",
        "            # Decoder tries to predict original Bengali words from masked sentences\n",
        "            # Input: (batch_size, seq_length) → Output: (batch_size, seq_length, vocab_size)\n",
        "            logits = model.decoder_mlm(input_ids)\n",
        "            #print(f\"logits shape: {logits.shape}\")\n",
        "\n",
        "            # Verify shapes match for loss calculation\n",
        "            assert logits.shape[:2] == labels.shape, f\"Logits shape {logits.shape} and labels shape {labels.shape} do not match for loss\"\n",
        "\n",
        "            # LOSS CALCULATION: How wrong were the predictions?\n",
        "            loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),  # Flatten for loss calculation\n",
        "                labels.view(-1)\n",
        "            )\n",
        "\n",
        "            # BACKWARD PASS: Learn from mistakes\n",
        "            loss.backward()\n",
        "\n",
        "            # UPDATE: Apply improvements\n",
        "            optimizer.step()\n",
        "\n",
        "            # SCHEDULING: Adjust learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            # TRACKING: Record performance metrics\n",
        "            epoch_train_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # Calculate accuracy\n",
        "            acc = batch_accuracy(logits, labels, -100)\n",
        "            pretrain_accs.append(acc)\n",
        "\n",
        "        # EPOCH SUMMARY: Calculate and report average performance\n",
        "        avg_train_loss = epoch_train_loss / batch_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        print(f\"Pre-train Epoch {epoch+1}/{config['pretrain_epochs']}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # EARLY STOPPING: Check if we should stop training\n",
        "        if config['apply_early_stop'] and early_stopping(avg_train_loss):\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # STEP 7: Save the trained model\n",
        "    # This preserves all the learned knowledge for later use\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),    # All learned parameters\n",
        "        'config': config,                          # Training configuration\n",
        "        'src_vocab': src_vocab,                    # English vocabulary\n",
        "        'tgt_vocab': tgt_vocab,                    # Bengali vocabulary\n",
        "        'pretrain_losses': (train_losses, val_losses),  # Training history\n",
        "        'pretrain_accs': pretrain_accs             # Accuracy history\n",
        "    }, 'pretrained_transformer.pth')\n",
        "\n",
        "    print(\"Pre-training completed!\")\n",
        "\n",
        "    # Return the trained model and performance metrics\n",
        "    return model, train_losses, val_losses, pretrain_accs\n",
        "\n",
        "# DIMENSIONAL FLOW SUMMARY:\n",
        "# 1. Raw text → Tokenized sequences: List[str] → (batch_size, seq_length)\n",
        "# 2. Masked sequences → Model predictions: (batch_size, seq_length) → (batch_size, seq_length, vocab_size)\n",
        "# 3. Predictions → Loss calculation: (batch_size, seq_length, vocab_size) → scalar\n",
        "# 4. Loss → Gradients → Weight updates: scalar → model parameters updated\n",
        "# 5. Repeat for thousands of batches and multiple epochs"
      ],
      "metadata": {
        "id": "EC66rUIGr3RQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === FINETUNE MODEL: Translation on parallel data ===\n",
        "def finetune_model(model, english_sentences, bengali_sentences, src_vocab, tgt_vocab, config, overall_start_time=None):\n",
        "    \"\"\"\n",
        "    Fine-tune the pretrained transformer model on parallel translation data (English→Bengali).\n",
        "\n",
        "    Think of this like training a pre-existing translator to get better at a specific language pair.\n",
        "    The model already knows general language patterns, now we teach it English→Bengali specifically.\n",
        "    \"\"\"\n",
        "    print(\"=== Starting Fine-tuning Phase ===\")\n",
        "\n",
        "    # Set up GPU/CPU processing - like choosing which processor to use for calculations\n",
        "    device = torch.device(config['device'])\n",
        "    model = model.to(device)  # Move model to GPU/CPU\n",
        "\n",
        "    # Split data: 90% for training, 10% for validation (like train/test split in traditional ML)\n",
        "    # This prevents overfitting - we need unseen data to check if model generalizes well\n",
        "    split_idx = int(0.9 * len(english_sentences))\n",
        "    train_src = english_sentences[:split_idx]        # Training English sentences\n",
        "    train_tgt = bengali_sentences[:split_idx]        # Training Bengali sentences\n",
        "    val_src = english_sentences[split_idx:]          # Validation English sentences\n",
        "    val_tgt = bengali_sentences[split_idx:]          # Validation Bengali sentences\n",
        "\n",
        "    # Create datasets - these convert text to numbers that neural networks can process\n",
        "    # Input: List of sentences (strings)\n",
        "    # Output: Tensors of token IDs with shape [num_samples, max_length]\n",
        "    train_dataset = TranslationDataset(train_src, train_tgt, src_vocab, tgt_vocab, config['max_length'])\n",
        "    val_dataset = TranslationDataset(val_src, val_tgt, src_vocab, tgt_vocab, config['max_length'])\n",
        "\n",
        "    # Create data loaders - these feed data to model in batches (like processing chunks instead of all at once)\n",
        "    # batch_size determines how many sentence pairs to process simultaneously\n",
        "    # Input: Dataset, Output: Batches of shape [batch_size, max_length]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    print(f\"Fine-tuning training samples: {len(train_dataset)}\")\n",
        "    print(f\"Fine-tuning validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Loss function - measures how wrong the model's predictions are\n",
        "    # CrossEntropyLoss: good for classification (picking next word from vocabulary)\n",
        "    # ignore_index=0: ignores padding tokens (empty spaces added to make all sentences same length)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    # Optimizer - the algorithm that updates model weights to reduce loss\n",
        "    # Adam: popular optimizer that adapts learning rate automatically\n",
        "    # lr: learning rate (how big steps to take when updating weights)\n",
        "    # betas, eps: Adam-specific hyperparameters for momentum and numerical stability\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['finetune_learning_rate'], betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "    # Learning rate scheduler - gradually changes learning rate during training\n",
        "    # Noam scheduler: starts low, increases, then decreases (helps with convergence)\n",
        "    scheduler = get_noam_scheduler(optimizer, config['d_model'], config['warmup_steps'])\n",
        "\n",
        "    # Early stopping - stops training if model stops improving (prevents overfitting)\n",
        "    early_stopping = EarlyStopping(patience=config['patience'])\n",
        "\n",
        "    # Lists to track training progress over time\n",
        "    train_losses = []    # How wrong model is on training data each epoch\n",
        "    val_losses = []      # How wrong model is on validation data each epoch\n",
        "    train_accs = []      # How accurate model is on training data each epoch\n",
        "    val_accs = []        # How accurate model is on validation data each epoch\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training loop - repeat for specified number of epochs (full passes through data)\n",
        "    for epoch in range(config['finetune_epochs']):\n",
        "        # Time tracking for monitoring training progress\n",
        "        time_elapsed = time.time() - start_time\n",
        "        global_time_elapsed = time.time() - overall_start_time if overall_start_time is not None else 0\n",
        "        print(f\"Time elapsed: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s (phase), {global_time_elapsed // 60:.0f}m {global_time_elapsed % 60:.0f}s (overall)\")\n",
        "\n",
        "        # Stop if training takes too long (computational budget limit)\n",
        "        if overall_start_time is not None and global_time_elapsed > config['max_global_minutes'] * 60:\n",
        "            print(\"Global time limit exceeded. Stopping fine-tuning.\")\n",
        "            break\n",
        "\n",
        "        # TRAINING PHASE\n",
        "        model.train()  # Set model to training mode (enables dropout, batch norm updates)\n",
        "        epoch_train_loss = 0  # Accumulate loss for this epoch\n",
        "        epoch_train_acc = 0   # Accumulate accuracy for this epoch\n",
        "        batch_cnt = 0         # Count batches processed\n",
        "\n",
        "        # Process each batch of training data\n",
        "        for batch in tqdm(train_loader, desc=f\"Fine-tune Epoch {epoch+1}\", disable=CONFIG['tqdm_disable']):\n",
        "            # Get batch data and move to GPU/CPU\n",
        "            # src: English sentences as token IDs, shape [batch_size, max_length]\n",
        "            # tgt_ip: Bengali input (shifted right for teacher forcing), shape [batch_size, max_length]\n",
        "            # tgt_op: Bengali output (ground truth), shape [batch_size, max_length]\n",
        "            src = batch['src'].to(device)\n",
        "            tgt_ip = batch['tgt_ip'].to(device)\n",
        "            tgt_op = batch['tgt_op'].to(device)\n",
        "\n",
        "            # Reset gradients from previous batch (PyTorch accumulates gradients)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: feed data through model\n",
        "            # Input: src [batch_size, src_len], tgt_ip [batch_size, tgt_len]\n",
        "            # Output: predictions [batch_size, tgt_len, vocab_size] - probability for each word in vocabulary\n",
        "            outputs = model(src, tgt_ip, training=True)\n",
        "\n",
        "            # Calculate loss: how different are predictions from correct answers\n",
        "            # Reshape to 2D for CrossEntropyLoss: [batch_size * tgt_len, vocab_size] vs [batch_size * tgt_len]\n",
        "            # This compares each predicted word probability distribution with the correct word ID\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_op.reshape(-1))\n",
        "\n",
        "            # Calculate accuracy: what percentage of words were predicted correctly\n",
        "            # Input: outputs [batch_size, tgt_len, vocab_size], tgt_op [batch_size, tgt_len]\n",
        "            # Output: scalar accuracy value (0.0 to 1.0)\n",
        "            acc = batch_accuracy(outputs, tgt_op, ignore_index=0)\n",
        "            epoch_train_acc += acc\n",
        "            batch_cnt += 1\n",
        "\n",
        "            # Backward pass: calculate gradients (how much each weight should change)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update model weights based on gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update learning rate according to schedule\n",
        "            scheduler.step()\n",
        "\n",
        "            # Accumulate loss for epoch average\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Calculate average metrics for this epoch\n",
        "        avg_train_loss = epoch_train_loss / batch_cnt\n",
        "        avg_train_acc = epoch_train_acc / batch_cnt\n",
        "\n",
        "        # VALIDATION PHASE - test model on unseen data\n",
        "        model.eval()  # Set model to evaluation mode (disables dropout, freezes batch norm)\n",
        "        epoch_val_loss = 0\n",
        "        epoch_val_acc = 0\n",
        "        val_batches = 0\n",
        "\n",
        "        # Don't calculate gradients for validation (saves memory and computation)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                # Same data processing as training\n",
        "                src = batch['src'].to(device)\n",
        "                tgt_ip = batch['tgt_ip'].to(device)\n",
        "                tgt_op = batch['tgt_op'].to(device)\n",
        "\n",
        "                # Forward pass only (no backprop)\n",
        "                # Input/Output dimensions same as training\n",
        "                outputs = model(src, tgt_ip, training=True)\n",
        "\n",
        "                # Calculate loss and accuracy same as training\n",
        "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_op.reshape(-1))\n",
        "                acc = batch_accuracy(outputs, tgt_op, ignore_index=0)\n",
        "\n",
        "                epoch_val_loss += loss.item()\n",
        "                epoch_val_acc += acc\n",
        "                val_batches += 1\n",
        "\n",
        "        # Calculate validation averages\n",
        "        avg_val_loss = epoch_val_loss / val_batches\n",
        "        avg_val_acc = epoch_val_acc / val_batches\n",
        "\n",
        "        # Store metrics for plotting/analysis later\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accs.append(avg_train_acc)\n",
        "        val_accs.append(avg_val_acc)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Fine-tune Epoch {epoch+1}/{config['finetune_epochs']}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Train Acc : {avg_train_acc:.4f}, Val Acc : {avg_val_acc:.4f}\")\n",
        "\n",
        "        # Check if model stopped improving (early stopping)\n",
        "        if config['apply_early_stop'] and early_stopping(avg_val_loss):\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Save the trained model and all training information\n",
        "    # This creates a checkpoint file with everything needed to use the model later\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),  # Model weights\n",
        "        'config': config,                        # Training configuration\n",
        "        'src_vocab': src_vocab,                  # English vocabulary (word→ID mapping)\n",
        "        'tgt_vocab': tgt_vocab,                  # Bengali vocabulary (word→ID mapping)\n",
        "        'finetune_losses': (train_losses, val_losses),  # Training history\n",
        "        'train_accs': train_accs,                # Training accuracy history\n",
        "        'val_accs': val_accs                     # Validation accuracy history\n",
        "    }, 'finetuned_transformer.pth')\n",
        "\n",
        "    print(\"Fine-tuning completed!\")\n",
        "    return model, train_losses, val_losses, train_accs, val_accs"
      ],
      "metadata": {
        "id": "ggkrbBq5_kEa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationInference:\n",
        "    \"\"\"\n",
        "    Inference class for translating English sentences to Bengali using the trained model.\n",
        "\n",
        "    Think of this as a \"live translator\" that takes your trained model and uses it to\n",
        "    translate new sentences. It's like having a translator API that processes one\n",
        "    sentence at a time.\n",
        "\n",
        "    The key difference from training: we don't know the target sentence beforehand,\n",
        "    so we generate it word-by-word (autoregressive decoding).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, src_vocab, tgt_vocab, config):\n",
        "        \"\"\"\n",
        "        Initialize the translator with a trained model and vocabularies.\n",
        "\n",
        "        Args:\n",
        "            model: The trained neural network (like a trained translator's brain)\n",
        "            src_vocab: English vocabulary (maps English words ↔ numbers)\n",
        "            tgt_vocab: Bengali vocabulary (maps Bengali words ↔ numbers)\n",
        "            config: Configuration settings (max length, device, etc.)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.src_vocab = src_vocab          # English word→ID and ID→word mappings\n",
        "        self.tgt_vocab = tgt_vocab          # Bengali word→ID and ID→word mappings\n",
        "        self.config = config\n",
        "        self.device = torch.device(config['device'])  # GPU or CPU\n",
        "        self.model.eval()  # Set model to evaluation mode (no training, no dropout)\n",
        "\n",
        "    def translate(self, sentence, max_length=None):\n",
        "        \"\"\"\n",
        "        Translate a single English sentence to Bengali.\n",
        "\n",
        "        This is like asking a human translator: \"What's the Bengali for 'Hello world'?\"\n",
        "        The translator processes it word by word, building the Bengali sentence incrementally.\n",
        "\n",
        "        Args:\n",
        "            sentence: English sentence as string (e.g., \"Hello world\")\n",
        "            max_length: Maximum allowed length for input/output\n",
        "\n",
        "        Returns:\n",
        "            translated_sentence: Bengali sentence as string\n",
        "        \"\"\"\n",
        "        if max_length is None:\n",
        "            max_length = self.config['max_length']\n",
        "\n",
        "        # STEP 1: ENCODE SOURCE SENTENCE\n",
        "        # Convert English sentence from text to numbers that the model understands\n",
        "        # Input: \"Hello world\" (string)\n",
        "        # Output: [45, 123, 67] (list of integers, each representing a word)\n",
        "        src_encoded = self.src_vocab.encode(sentence)\n",
        "\n",
        "        # Truncate if sentence is too long (like cutting off a long message)\n",
        "        if len(src_encoded) > max_length:\n",
        "            src_encoded = src_encoded[:max_length]\n",
        "\n",
        "        # STEP 2: PAD SOURCE SEQUENCE\n",
        "        # All inputs must be same length, so add zeros (padding) to shorter sequences\n",
        "        # Input: [45, 123, 67] (length 3)\n",
        "        # Output: [45, 123, 67, 0, 0, 0, 0, 0] (length max_length, padded with zeros)\n",
        "        src_padded = src_encoded + [0] * (max_length - len(src_encoded))\n",
        "\n",
        "        # Convert to PyTorch tensor and add batch dimension\n",
        "        # Input: [45, 123, 67, 0, 0, 0, 0, 0] (list)\n",
        "        # Output: [[45, 123, 67, 0, 0, 0, 0, 0]] (tensor with shape [1, max_length])\n",
        "        # The extra dimension is needed because models expect batches, even for single sentences\n",
        "        src_tensor = torch.tensor([src_padded], dtype=torch.long).to(self.device)\n",
        "\n",
        "        # STEP 3: INITIALIZE TARGET SEQUENCE\n",
        "        # Start translation with SOS (Start of Sentence) token\n",
        "        # This tells the model \"start generating a Bengali sentence now\"\n",
        "        # Input: empty target\n",
        "        # Output: [2] (where 2 is the ID for '<SOS>' token)\n",
        "        tgt_input = [self.tgt_vocab.get_vocab().get('<SOS>')]\n",
        "\n",
        "        # STEP 4: AUTOREGRESSIVE DECODING\n",
        "        # Generate Bengali sentence word by word (like a human translator thinking step by step)\n",
        "        # We don't calculate gradients during inference (saves memory and computation)\n",
        "        with torch.no_grad():\n",
        "            # Generate up to max_length words\n",
        "            for _ in range(max_length):\n",
        "                # Pad current target sequence to fixed length\n",
        "                # Input: [2, 156] (SOS + first Bengali word)\n",
        "                # Output: [2, 156, 0, 0, 0, 0, 0, 0] (padded to max_length)\n",
        "                tgt_padded = tgt_input + [0] * (max_length - len(tgt_input))\n",
        "\n",
        "                # Convert to tensor with batch dimension\n",
        "                # Input: [2, 156, 0, 0, 0, 0, 0, 0] (list)\n",
        "                # Output: [[2, 156, 0, 0, 0, 0, 0, 0]] (tensor with shape [1, max_length])\n",
        "                tgt_tensor = torch.tensor([tgt_padded], dtype=torch.long).to(self.device)\n",
        "\n",
        "                # STEP 5: FORWARD PASS\n",
        "                # Feed source sentence and current target to model\n",
        "                # Input: src_tensor [1, max_length], tgt_tensor [1, max_length]\n",
        "                # Output: predictions [1, max_length, vocab_size]\n",
        "                # vocab_size is the number of Bengali words the model knows\n",
        "                # Each position contains probability distribution over all possible next words\n",
        "                predictions = self.model(src_tensor, tgt_tensor, training=False)\n",
        "\n",
        "                # STEP 6: GET NEXT WORD PREDICTION\n",
        "                # Extract prediction for the next word position\n",
        "                # We want the prediction for position after the last word we generated\n",
        "                # Input: predictions [1, max_length, vocab_size]\n",
        "                # Output: next_token_logits [vocab_size] - probability scores for each Bengali word\n",
        "                next_token_logits = predictions[0, len(tgt_input)-1, :]\n",
        "\n",
        "                # Choose the word with highest probability (greedy decoding)\n",
        "                # Input: next_token_logits [vocab_size] (e.g., [0.1, 0.8, 0.05, 0.05, ...])\n",
        "                # Output: next_token (integer) - ID of most probable word (e.g., 1 for index with 0.8)\n",
        "                next_token = torch.argmax(next_token_logits).item()\n",
        "\n",
        "                # STEP 7: UPDATE TARGET SEQUENCE\n",
        "                # Add the predicted word to our growing Bengali sentence\n",
        "                # Input: tgt_input [2, 156], next_token 89\n",
        "                # Output: tgt_input [2, 156, 89] (SOS + word1 + word2)\n",
        "                tgt_input.append(next_token)\n",
        "\n",
        "                # STEP 8: CHECK FOR END OF SENTENCE\n",
        "                # Stop if model generates EOS (End of Sentence) token\n",
        "                # This is like the model saying \"I'm done with this sentence\"\n",
        "                if next_token == self.tgt_vocab.get_vocab().get('<EOS>'):\n",
        "                    break\n",
        "\n",
        "        # STEP 9: DECODE RESULT\n",
        "        # Convert the generated number sequence back to Bengali text\n",
        "        # Remove SOS token from beginning and EOS token from end (if present)\n",
        "        # Input: [2, 156, 89, 234, 3] (SOS + Bengali words + EOS)\n",
        "        # Output: [156, 89, 234] (just the Bengali words)\n",
        "        if tgt_input[-1] == self.tgt_vocab.get_vocab().get('<EOS>'):\n",
        "            result_tokens = tgt_input[1:-1]  # Remove SOS and EOS\n",
        "        else:\n",
        "            result_tokens = tgt_input[1:]    # Remove only SOS\n",
        "\n",
        "        # Convert token IDs back to Bengali text\n",
        "        # Input: [156, 89, 234] (list of word IDs)\n",
        "        # Output: \"নমস্কার বিশ্ব\" (Bengali text string)\n",
        "        translated_sentence = self.tgt_vocab.decode(result_tokens)\n",
        "\n",
        "        return translated_sentence"
      ],
      "metadata": {
        "id": "QrdLfhrIAGLu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer:\n",
        "    \"\"\"\n",
        "    Improved Byte Pair Encoding (BPE) Tokenizer using HuggingFace's tokenizers library.\n",
        "\n",
        "    WHAT IS A TOKENIZER?\n",
        "    Think of this as a \"text-to-numbers converter\" for neural networks.\n",
        "    Neural networks can't understand words like \"hello\" or \"নমস্কার\" - they only work with numbers.\n",
        "\n",
        "    The tokenizer's job is to:\n",
        "    1. Convert text → numbers (encoding): \"Hello world\" → [45, 123, 67]\n",
        "    2. Convert numbers → text (decoding): [45, 123, 67] → \"Hello world\"\n",
        "\n",
        "    WHAT IS BPE?\n",
        "    BPE is like a smart compression algorithm for text. Instead of having a word for every\n",
        "    possible combination, it learns the most common \"chunks\" (subwords) and builds words from them.\n",
        "\n",
        "    Example: \"unhappiness\" might become [\"un\", \"happy\", \"ness\"] → [234, 567, 890]\n",
        "    This way, even if the model never saw \"unhappiness\", it can understand it from its parts.\n",
        "\n",
        "    Handles both English and Bengali text properly with appropriate normalization.\n",
        "    Now uses ByteLevel pre-tokenizer and decoder to preserve spaces in output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=10000, language=\"mixed\"):\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Maximum number of unique tokens (like dictionary size)\n",
        "                       Input: integer (e.g., 10000)\n",
        "                       Effect: Determines how many unique word-pieces the tokenizer can learn\n",
        "            language: What language to optimize for (\"english\", \"bengali\", or \"mixed\")\n",
        "                     Input: string\n",
        "                     Effect: Changes how text is normalized (lowercase, accent removal, etc.)\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tokenizer = None  # Will hold the trained tokenizer object\n",
        "        self.language = language\n",
        "\n",
        "    def train(self, sentences):\n",
        "        \"\"\"\n",
        "        Train the BPE tokenizer on the given sentences.\n",
        "\n",
        "        This is like teaching the tokenizer what words and word-pieces exist in your language.\n",
        "        The tokenizer analyzes all your text and learns the most efficient way to break it down.\n",
        "\n",
        "        Args:\n",
        "            sentences: List of sentences to train on\n",
        "                      Input: List of strings, e.g., [\"Hello world\", \"How are you\", \"নমস্কার\"]\n",
        "                      Output: Trained tokenizer that can convert text ↔ numbers\n",
        "        \"\"\"\n",
        "        if not sentences:\n",
        "            raise ValueError(\"Cannot train tokenizer on empty sentence list\")\n",
        "\n",
        "        # STEP 1: PREPARE TRAINING DATA\n",
        "        # Write all sentences to a temporary file (HuggingFace tokenizers expect file input)\n",
        "        # Input: List of strings in memory\n",
        "        # Output: Text file with one sentence per line\n",
        "        with tempfile.NamedTemporaryFile(mode='w+', delete=False, encoding='utf-8') as f:\n",
        "            for sentence in sentences:\n",
        "                if sentence and sentence.strip():  # Skip empty sentences\n",
        "                    f.write(sentence.strip() + '\\n')\n",
        "            temp_file = f.name\n",
        "\n",
        "        try:\n",
        "            # STEP 2: INITIALIZE TOKENIZER\n",
        "            # Create a blank BPE tokenizer (like creating an empty dictionary)\n",
        "            # Input: None (fresh start)\n",
        "            # Output: Untrained tokenizer object with BPE algorithm\n",
        "            tokenizer = Tokenizer(models.BPE(unk_token=\"<UNK>\"))\n",
        "\n",
        "            # STEP 3: SET UP TEXT NORMALIZATION\n",
        "            # This is like setting \"text preprocessing rules\" - how to clean/standardize text\n",
        "            # Different languages need different preprocessing\n",
        "            if self.language == \"english\":\n",
        "                # For English: NFD normalization, lowercase, strip accents\n",
        "                # Input: \"Café\" → Output: \"cafe\" (normalized, lowercase, no accents)\n",
        "                tokenizer.normalizer = NormalizerSequence([\n",
        "                    NFD(),        # Normalize unicode characters (é → e + accent)\n",
        "                    Lowercase(),  # Convert to lowercase\n",
        "                    StripAccents() # Remove accent marks\n",
        "                ])\n",
        "            elif self.language == \"bengali\":\n",
        "                # For Bengali: Only NFD normalization (preserve case and Bengali characters)\n",
        "                # Input: \"নমস্কার\" → Output: \"নমস্কার\" (normalized unicode only)\n",
        "                tokenizer.normalizer = NFD()\n",
        "            else:  # mixed or default\n",
        "                # For mixed languages: Only NFD to handle both properly\n",
        "                # Input: Mixed text → Output: Unicode-normalized text\n",
        "                tokenizer.normalizer = NFD()\n",
        "\n",
        "            # STEP 4: SET UP PRE-TOKENIZATION\n",
        "            # This splits text into \"chunks\" before BPE processing\n",
        "            # ByteLevel preserves spaces and handles any character (including emojis, special chars)\n",
        "            # Input: \"Hello world\" → Output: [\"Hello\", \" world\"] (space preserved)\n",
        "            tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "\n",
        "            # STEP 5: DEFINE SPECIAL TOKENS\n",
        "            # These are like \"reserved words\" that have special meaning\n",
        "            # Input: List of special token strings\n",
        "            # Output: Reserved token IDs (usually 0, 1, 2, 3, 4)\n",
        "            special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\", \"<MASK>\"]\n",
        "            # <PAD>: Padding (fill empty space)\n",
        "            # <UNK>: Unknown word (words not in vocabulary)\n",
        "            # <SOS>: Start of Sentence\n",
        "            # <EOS>: End of Sentence\n",
        "            # <MASK>: Masked token (for some training techniques)\n",
        "\n",
        "            # STEP 6: SET UP TRAINER\n",
        "            # Configure how the BPE algorithm will learn\n",
        "            # Input: Training parameters\n",
        "            # Output: Trainer object that knows how to build vocabulary\n",
        "            trainer = trainers.BpeTrainer(\n",
        "                vocab_size=self.vocab_size,     # Maximum vocabulary size\n",
        "                special_tokens=special_tokens,   # Reserve these tokens\n",
        "                min_frequency=1,                 # Minimum times a subword must appear\n",
        "                show_progress=True              # Show progress bar during training\n",
        "            )\n",
        "\n",
        "            # STEP 7: TRAIN THE TOKENIZER\n",
        "            # This is the core learning step - BPE algorithm analyzes all text\n",
        "            # Input: Text file with sentences\n",
        "            # Process: Counts character pairs, merges most frequent ones iteratively\n",
        "            # Output: Vocabulary mapping (token_string → token_id)\n",
        "            #\n",
        "            # Example process:\n",
        "            # 1. Start with individual characters: \"h\", \"e\", \"l\", \"l\", \"o\"\n",
        "            # 2. Find most frequent pair: \"l\" + \"l\" → \"ll\"\n",
        "            # 3. Merge them: \"h\", \"e\", \"ll\", \"o\"\n",
        "            # 4. Repeat until vocab_size reached\n",
        "            tokenizer.train([temp_file], trainer)\n",
        "\n",
        "            # STEP 8: SET UP DECODER\n",
        "            # This handles converting token IDs back to text\n",
        "            # ByteLevel decoder properly handles spaces and special characters\n",
        "            # Input: List of token IDs, e.g., [45, 123, 67]\n",
        "            # Output: Properly formatted text with spaces preserved\n",
        "            tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "            \"\"\"\n",
        "            # STEP 9: SET UP POST-PROCESSOR\n",
        "            # This automatically adds special tokens to sequences\n",
        "            # Template: \"<SOS> $A <EOS>\" means wrap every sentence with start/end tokens\n",
        "            # Input: Raw sentence\n",
        "            # Output: Sentence with SOS and EOS tokens added\n",
        "            tokenizer.post_processor = processors.TemplateProcessing(\n",
        "                single=\"<SOS> $A <EOS>\",  # Template for single sentence\n",
        "                special_tokens=[\n",
        "                    (\"<SOS>\", self.get_special_token_id(\"<SOS>\", tokenizer)),\n",
        "                    (\"<EOS>\", self.get_special_token_id(\"<EOS>\", tokenizer)),\n",
        "                ]\n",
        "            )\n",
        "            \"\"\"\n",
        "            # Store the trained tokenizer\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "        finally:\n",
        "            # CLEANUP: Remove temporary file\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "\n",
        "    def get_special_token_id(self, token, tokenizer):\n",
        "        \"\"\"\n",
        "        Helper method to get special token ID.\n",
        "\n",
        "        Input: Token string (e.g., \"<SOS>\")\n",
        "        Output: Token ID (e.g., 2)\n",
        "\n",
        "        Like looking up a word in a dictionary to get its page number.\n",
        "        \"\"\"\n",
        "        vocab = tokenizer.get_vocab()  # Get the word→ID mapping dictionary\n",
        "        return vocab.get(token, vocab.get(\"<UNK>\"))  # Return ID or default to UNK\n",
        "\n",
        "    def encode(self, sentence, add_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Encode a sentence to token IDs.\n",
        "\n",
        "        This is the main \"text → numbers\" conversion function.\n",
        "\n",
        "        Args:\n",
        "            sentence: Input sentence string\n",
        "                     Input: \"Hello world\" (string)\n",
        "                     Output: [45, 123, 67] (list of integers)\n",
        "            add_special_tokens: Whether to add SOS/EOS tokens\n",
        "                               Input: True/False\n",
        "                               Effect: [2, 45, 123, 67, 3] vs [45, 123, 67]\n",
        "\n",
        "        Returns:\n",
        "            List of token IDs\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"BPE Tokenizer not trained yet. Call train() first.\")\n",
        "\n",
        "        if not sentence or not sentence.strip():\n",
        "            return []  # Empty input → empty output\n",
        "\n",
        "        # ENCODING PROCESS:\n",
        "        # Input: \"Hello world\" (string)\n",
        "        # Step 1: Normalize → \"hello world\" (if English)\n",
        "        # Step 2: Pre-tokenize → [\"hello\", \" world\"] (preserve spaces)\n",
        "        # Step 3: Apply BPE → [\"he\", \"llo\", \" wor\", \"ld\"] (subword units)\n",
        "        # Step 4: Convert to IDs → [45, 123, 67, 89] (numbers)\n",
        "        # Step 5: Add special tokens if requested → [2, 45, 123, 67, 89, 3]\n",
        "        encoding = self.tokenizer.encode(sentence.strip(), add_special_tokens=add_special_tokens)\n",
        "        return encoding.ids  # Return just the list of numbers\n",
        "\n",
        "    def decode(self, ids, skip_special_tokens=True):\n",
        "        \"\"\"\n",
        "        Decode token IDs back to text.\n",
        "\n",
        "        This is the main \"numbers → text\" conversion function.\n",
        "\n",
        "        Args:\n",
        "            ids: List of token IDs\n",
        "                Input: [45, 123, 67] (list of integers)\n",
        "                Output: \"Hello world\" (string)\n",
        "            skip_special_tokens: Whether to skip special tokens in output\n",
        "                               Input: True/False\n",
        "                               Effect: \"Hello world\" vs \"<SOS> Hello world <EOS>\"\n",
        "\n",
        "        Returns:\n",
        "            Decoded string\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"BPE Tokenizer not trained yet. Call train() first.\")\n",
        "\n",
        "        if not ids:\n",
        "            return \"\"  # Empty input → empty output\n",
        "\n",
        "        # STEP 1: FILTER SPECIAL TOKENS (if requested)\n",
        "        # Remove special tokens from the ID list before decoding\n",
        "        # This prevents output like \"<SOS> Hello world <EOS>\" when you just want \"Hello world\"\n",
        "        if skip_special_tokens:\n",
        "            # Get IDs of all special tokens\n",
        "            # Input: Special token strings\n",
        "            # Output: Set of special token IDs, e.g., {0, 1, 2, 3, 4}\n",
        "            special_ids = {\n",
        "                self.tokenizer.token_to_id(\"<PAD>\"),\n",
        "                self.tokenizer.token_to_id(\"<UNK>\"),\n",
        "                self.tokenizer.token_to_id(\"<SOS>\"),\n",
        "                self.tokenizer.token_to_id(\"<EOS>\"),\n",
        "                self.tokenizer.token_to_id(\"<MASK>\")\n",
        "            }\n",
        "            # Remove None values (tokens that don't exist)\n",
        "            special_ids = {id for id in special_ids if id is not None}\n",
        "\n",
        "            # Filter out special token IDs from the input\n",
        "            # Input: [2, 45, 123, 67, 3] (includes SOS=2, EOS=3)\n",
        "            # Output: [45, 123, 67] (only content tokens)\n",
        "            ids = [id for id in ids if id not in special_ids]\n",
        "\n",
        "        try:\n",
        "            # STEP 2: DECODE TO TEXT\n",
        "            # DECODING PROCESS:\n",
        "            # Input: [45, 123, 67] (list of token IDs)\n",
        "            # Step 1: Convert IDs to subwords → [\"he\", \"llo\", \" world\"]\n",
        "            # Step 2: Merge subwords → \"hello world\"\n",
        "            # Step 3: Handle spaces and special characters properly\n",
        "            # Output: \"hello world\" (final string)\n",
        "            decoded = self.tokenizer.decode(ids)\n",
        "            return decoded.strip()  # Remove leading/trailing whitespace\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Decode error for IDs {ids}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"\n",
        "        Get the vocabulary dictionary.\n",
        "\n",
        "        Returns the complete word→ID mapping that the tokenizer learned.\n",
        "\n",
        "        Input: None\n",
        "        Output: Dictionary like {\"hello\": 45, \"world\": 123, \"<SOS>\": 2, ...}\n",
        "\n",
        "        This is like getting the complete dictionary/codebook the tokenizer uses.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"BPE Tokenizer not trained yet. Call train() first.\")\n",
        "        return self.tokenizer.get_vocab()\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        \"\"\"\n",
        "        Get the actual vocabulary size.\n",
        "\n",
        "        Input: None\n",
        "        Output: Integer (actual number of tokens learned, e.g., 8547)\n",
        "\n",
        "        This might be different from the requested vocab_size if there wasn't enough text.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            return 0\n",
        "        return len(self.get_vocab())\n",
        "\n",
        "    def token_to_id(self, token):\n",
        "        \"\"\"\n",
        "        Convert token to ID.\n",
        "\n",
        "        Input: Token string (e.g., \"hello\")\n",
        "        Output: Token ID (e.g., 45)\n",
        "\n",
        "        Like looking up a word in a dictionary to get its page number.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"BPE Tokenizer not trained yet.\")\n",
        "        return self.tokenizer.token_to_id(token)\n",
        "\n",
        "    def id_to_token(self, id):\n",
        "        \"\"\"\n",
        "        Convert ID to token.\n",
        "\n",
        "        Input: Token ID (e.g., 45)\n",
        "        Output: Token string (e.g., \"hello\")\n",
        "\n",
        "        Like looking up a page number in a dictionary to get the word.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"BPE Tokenizer not trained yet.\")\n",
        "        return self.tokenizer.id_to_token(id)\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"\n",
        "        Save the trained tokenizer to disk.\n",
        "\n",
        "        Input: File path (e.g., \"my_tokenizer.json\")\n",
        "        Output: Tokenizer saved to file\n",
        "\n",
        "        Like saving your custom dictionary so you can use it later.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"No trained tokenizer to save.\")\n",
        "        self.tokenizer.save(filepath)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        \"\"\"\n",
        "        Load a trained tokenizer from disk.\n",
        "\n",
        "        Input: File path (e.g., \"my_tokenizer.json\")\n",
        "        Output: Tokenizer loaded and ready to use\n",
        "\n",
        "        Like loading a previously saved dictionary.\n",
        "        \"\"\"\n",
        "        self.tokenizer = Tokenizer.from_file(filepath)"
      ],
      "metadata": {
        "id": "ViONpkG8AUqI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === MAIN FUNCTION FOR TWO-STAGE TRAINING ===\n",
        "\"\"\"\n",
        "Main function to orchestrate the entire training and inference process:\n",
        "\n",
        "OVERVIEW FOR SOFTWARE DEVELOPERS:\n",
        "This is a machine learning pipeline that trains a \"Transformer\" model to translate\n",
        "English text to Bengali. Think of it like training a very sophisticated autocomplete\n",
        "system that can convert meaning between languages.\n",
        "\n",
        "THE TWO-STAGE PROCESS:\n",
        "1. PRE-TRAINING: The model learns general language patterns by predicting missing\n",
        "    words in sentences (like a fill-in-the-blanks game)\n",
        "2. FINE-TUNING: The model learns specific translation patterns using paired\n",
        "    English-Bengali sentences\n",
        "\n",
        "STEPS:\n",
        "1. Load monolingual data (separate English and Bengali text files)\n",
        "2. Build vocabularies (maps words to numbers that computers can process)\n",
        "3. Create and pretrain the model (MLM - Masked Language Modeling)\n",
        "4. Fine-tune the model on translation pairs\n",
        "5. Save the model and vocabs\n",
        "6. Run inference on a new English sentence\n",
        "\"\"\"\n",
        "# === Overall start time ===\n",
        "overall_start_time = time.time()\n",
        "print(\"=== English-Bengali Transformer: Pre-training and Fine-tuning ===\")\n",
        "print(f\"Configuration: {CONFIG}\")\n",
        "\n",
        "# === STEP 1: LOAD MONOLINGUAL DATA ===\n",
        "# INPUT: Text files with one sentence per line\n",
        "# OUTPUT: Lists of strings (sentences)\n",
        "# PURPOSE: Get raw text data to train language understanding\n",
        "english_sentences = load_monolingual_data(CONFIG['english_file'], CONFIG['max_pretrain_sentences'])\n",
        "bengali_sentences = load_monolingual_data(CONFIG['bengali_file'], CONFIG['max_pretrain_sentences'])\n",
        "\n",
        "# Error handling - exit if data loading fails\n",
        "if not english_sentences or not bengali_sentences:\n",
        "    print(\"Error: Could not load monolingual data. Please check 'english.txt' and 'bengali.txt'.\")\n",
        "\n",
        "\n",
        "# === STEP 2: BUILD VOCABULARIES USING BPE (Byte Pair Encoding) ===\n",
        "# CONCEPT: Computers can't process words directly - they need numbers\n",
        "# BPE breaks words into subword pieces (like \"unhappy\" → \"un\" + \"happy\")\n",
        "# This helps handle rare words and different word forms\n",
        "\n",
        "if CONFIG.get('shared_bpe_vocab', False):\n",
        "    # SHARED VOCABULARY APPROACH:\n",
        "    # INPUT: Combined English + Bengali sentences (list of strings)\n",
        "    # OUTPUT: One tokenizer that handles both languages\n",
        "    # DIMENSIONS: vocab_size × embedding_dim mapping table\n",
        "    print(\"\\n--- Training SHARED BPE Tokenizer on English + Bengali sentences ---\")\n",
        "    shared_sentences = english_sentences + bengali_sentences  # Concatenate lists\n",
        "    shared_bpe = BPETokenizer(vocab_size=CONFIG['vocab_size'])  # Create tokenizer\n",
        "    shared_bpe.train(shared_sentences)  # Learn subword patterns from data\n",
        "    print(\"Shared BPE vocab size:\", len(shared_bpe.get_vocab()))\n",
        "    src_vocab = shared_bpe  # Source language tokenizer\n",
        "    tgt_vocab = shared_bpe  # Target language tokenizer (same as source)\n",
        "else:\n",
        "    # SEPARATE VOCABULARY APPROACH:\n",
        "    # INPUT: English sentences (list of strings)\n",
        "    # OUTPUT: English-specific tokenizer\n",
        "    # DIMENSIONS: vocab_size × embedding_dim mapping table for English\n",
        "    print(\"\\n--- Training BPE Tokenizer on English sentences ---\")\n",
        "    bpe_tokenizer_en = BPETokenizer(vocab_size=CONFIG['vocab_size'], language=\"english\")\n",
        "    bpe_tokenizer_en.train(english_sentences)  # Learn English subword patterns\n",
        "    print(\"English BPE vocab size:\", len(bpe_tokenizer_en.get_vocab()))\n",
        "\n",
        "    # INPUT: Bengali sentences (list of strings)\n",
        "    # OUTPUT: Bengali-specific tokenizer\n",
        "    # DIMENSIONS: vocab_size × embedding_dim mapping table for Bengali\n",
        "    print(\"\\n--- Training BPE Tokenizer on Bengali sentences ---\")\n",
        "    bpe_tokenizer_bn = BPETokenizer(vocab_size=CONFIG['vocab_size'], language=\"bengali\")\n",
        "    bpe_tokenizer_bn.train(bengali_sentences)  # Learn Bengali subword patterns\n",
        "    print(\"Bengali BPE vocab size:\", len(bpe_tokenizer_bn.get_vocab()))\n",
        "\n",
        "    src_vocab = bpe_tokenizer_en  # Source language tokenizer\n",
        "    tgt_vocab = bpe_tokenizer_bn  # Target language tokenizer\n",
        "\n",
        "# === STEP 3: CREATE TRANSFORMER MODEL ===\n",
        "# CONCEPT: A Transformer is like a very sophisticated pattern matching system\n",
        "# It has an \"encoder\" (understands input) and \"decoder\" (generates output)\n",
        "# Think of it as: Encoder reads English, Decoder writes Bengali\n",
        "\n",
        "print(\"Creating transformer model...\")\n",
        "# INPUT DIMENSIONS:\n",
        "# - src_vocab_size: Number of unique English subwords (typically 8000-50000)\n",
        "# - tgt_vocab_size: Number of unique Bengali subwords (typically 8000-50000)\n",
        "# - d_model: Internal vector size (like 512 or 768) - bigger = more capacity\n",
        "# - num_heads: Parallel attention mechanisms (like 8 or 16)\n",
        "# - max_length: Maximum sentence length in tokens (like 256 or 512)\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=len(src_vocab.get_vocab()),    # English vocabulary size\n",
        "    tgt_vocab_size=len(tgt_vocab.get_vocab()),    # Bengali vocabulary size\n",
        "    d_model=CONFIG['d_model'],                    # Hidden dimension (e.g., 512)\n",
        "    num_encoder_layers=CONFIG['num_encoder_layers'],  # Encoder depth (e.g., 6)\n",
        "    num_decoder_layers=CONFIG['num_decoder_layers'],  # Decoder depth (e.g., 6)\n",
        "    num_heads=CONFIG['num_heads'],                # Attention heads (e.g., 8)\n",
        "    dff=CONFIG['dff'],                           # Feed-forward dimension (e.g., 2048)\n",
        "    max_length=CONFIG['max_length'],             # Max sequence length (e.g., 256)\n",
        "    dropout_rate=CONFIG['dropout_rate']          # Regularization rate (e.g., 0.1)\n",
        ")\n",
        "\n",
        "# === STEP 4: PRE-TRAINING PHASE ===\n",
        "# CONCEPT: Like teaching a child language by having them fill in missing words\n",
        "# INPUT: Individual sentences with some words randomly masked\n",
        "# OUTPUT: Model that understands language patterns\n",
        "# VECTOR OPERATIONS:\n",
        "# - Input: [batch_size, sequence_length] integers (token IDs)\n",
        "# - Embedding: [batch_size, sequence_length, d_model] floats\n",
        "# - Transformer layers: [batch_size, sequence_length, d_model] → [batch_size, sequence_length, d_model]\n",
        "# - Output: [batch_size, sequence_length, vocab_size] probabilities\n",
        "\n",
        "print(\"\\n=== PRE-TRAINING PHASE ===\")\n",
        "model, pretrain_train_losses, pretrain_val_losses, pretrain_accs = pretrain_model(\n",
        "    model,                    # The neural network model\n",
        "    english_sentences,        # List of English sentences for training\n",
        "    bengali_sentences,        # List of Bengali sentences for training\n",
        "    src_vocab,               # English tokenizer\n",
        "    tgt_vocab,               # Bengali tokenizer\n",
        "    CONFIG,                  # Training configuration\n",
        "    overall_start_time=overall_start_time\n",
        ")\n",
        "\n",
        "# === STEP 5: LOAD TRANSLATION DATA ===\n",
        "# INPUT: CSV file with English-Bengali sentence pairs\n",
        "# OUTPUT: Two aligned lists of sentences\n",
        "# PURPOSE: Get paired data for translation training\n",
        "english_pairs, bengali_pairs = load_translation_data(CONFIG['translation_file'], CONFIG['max_translation_pairs'])\n",
        "\n",
        "# Error handling - exit if translation data loading fails\n",
        "if not english_pairs or not bengali_pairs:\n",
        "    print(\"Error: Could not load translation data. Please check 'english_to_bangla.csv'.\")\n",
        "\n",
        "\n",
        "# === STEP 6: FINE-TUNING PHASE ===\n",
        "# CONCEPT: Now teach the model to translate by showing it English-Bengali pairs\n",
        "# INPUT: Paired sentences (English sentence → Bengali sentence)\n",
        "# OUTPUT: Model that can translate English to Bengali\n",
        "# VECTOR OPERATIONS:\n",
        "# - Encoder Input: [batch_size, src_seq_len] → [batch_size, src_seq_len, d_model]\n",
        "# - Decoder Input: [batch_size, tgt_seq_len] → [batch_size, tgt_seq_len, d_model]\n",
        "# - Cross-attention: Decoder attends to encoder output\n",
        "# - Final Output: [batch_size, tgt_seq_len, tgt_vocab_size] translation probabilities\n",
        "\n",
        "print(\"\\n=== FINE-TUNING PHASE ===\")\n",
        "model, finetune_train_losses, finetune_val_losses, train_accs, val_accs = finetune_model(\n",
        "    model,                    # Pre-trained model from step 4\n",
        "    english_pairs,            # List of English sentences\n",
        "    bengali_pairs,            # List of corresponding Bengali sentences\n",
        "    src_vocab,               # English tokenizer\n",
        "    tgt_vocab,               # Bengali tokenizer\n",
        "    CONFIG,                  # Training configuration\n",
        "    overall_start_time=overall_start_time\n",
        ")\n",
        "\n",
        "# === STEP 8: GENERATE TRAINING VISUALIZATION PLOTS ===\n",
        "# CONCEPT: Create graphs to visualize how well the training went\n",
        "# These help you understand if the model learned properly\n",
        "\n",
        "# 1. Pretraining Loss Plot\n",
        "# MEANING: Lower loss = better performance at predicting masked words\n",
        "# X-axis: Training epochs (complete passes through data)\n",
        "# Y-axis: Loss value (lower is better)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(pretrain_train_losses, label='Pretrain Loss')\n",
        "plt.title('Pretraining Loss vs Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('pretrain_loss.png')\n",
        "\n",
        "# 2. Pretraining Accuracy Plot\n",
        "# MEANING: Higher accuracy = better at predicting masked words\n",
        "# X-axis: Training batches (small chunks of data processed together)\n",
        "# Y-axis: Accuracy percentage (higher is better)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(pretrain_accs, label='Pretrain Accuracy')\n",
        "plt.title('Pretraining Accuracy vs Batch')\n",
        "plt.xlabel('Batch (across epochs)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('pretrain_accuracy.png')\n",
        "\n",
        "# 3. Fine-tuning Loss Plot\n",
        "# MEANING: Shows how well the model learns to translate\n",
        "# Train Loss: Performance on training data\n",
        "# Val Loss: Performance on validation data (unseen during training)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(finetune_train_losses, label='Train Loss')\n",
        "plt.plot(finetune_val_losses, label='Val Loss')\n",
        "plt.title('Fine-tuning Loss vs Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('finetune_loss.png')\n",
        "\n",
        "# 4. Fine-tuning Accuracy Plot\n",
        "# MEANING: Translation accuracy on training vs validation data\n",
        "# Gap between lines indicates overfitting (memorizing vs generalizing)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Val Accuracy')\n",
        "plt.title('Fine-tuning Accuracy vs Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('finetune_accuracy.png')\n",
        "\n",
        "# Display plots briefly then close them to free memory\n",
        "plt.show(block=False)  # Non-blocking display\n",
        "plt.pause(15)          # Show for 15 seconds\n",
        "plt.close('all')       # Clean up memory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6IuxWwAqBUoF",
        "outputId": "52359d23-8023-419b-aade-15af69023e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== English-Bengali Transformer: Pre-training and Fine-tuning ===\n",
            "Configuration: {'vocab_size': 18000, 'd_model': 512, 'dff': 2048, 'num_heads': 8, 'num_encoder_layers': 6, 'num_decoder_layers': 6, 'dropout_rate': 0.1, 'max_length': 200, 'batch_size': 64, 'pretrain_learning_rate': 0.0001, 'finetune_learning_rate': 5e-05, 'pretrain_epochs': 500, 'finetune_epochs': 500, 'apply_early_stop': False, 'patience': 5, 'english_file': 'EBook_of_The_Bhagavad-Gita_English.txt', 'bengali_file': 'EBook_of_The_Bhagavad-Gita_Bengali.txt', 'translation_file': 'english_to_bangla.csv', 'max_pretrain_sentences': 39000, 'max_translation_pairs': 39, 'mask_probability': 0.15, 'device': 'cpu', 'max_train_minutes': 45, 'max_global_minutes': 100, 'shared_bpe_vocab': False, 'warmup_steps': 4000, 'tqdm_disable': False}\n",
            "Loading monolingual data from EBook_of_The_Bhagavad-Gita_English.txt...\n",
            "Loaded 3054 sentences from EBook_of_The_Bhagavad-Gita_English.txt\n",
            "First 5 sentences(load_monolingual_data): ['CHAPTER I', 'Dhritirashtra:', 'Ranged thus for battle on the sacred plain--', 'On Kurukshetra--say, Sanjaya! say', 'What wrought my people, and the Pandavas?']\n",
            "Loading monolingual data from EBook_of_The_Bhagavad-Gita_Bengali.txt...\n",
            "Loaded 10575 sentences from EBook_of_The_Bhagavad-Gita_Bengali.txt\n",
            "First 5 sentences(load_monolingual_data): ['গীতা প্রতিটি মানুষেরই ধর্মশান্তর। - মহর্ষি বেদব্যাস', 'শ্রীকৃষ্তকালীন মহর্ষি বেদব্যাসের পূর্বে কোন শাস্ত্র লিপিবদ্ধ ছিল না।', 'শ্রতজ্ঞানের এই পরম্পরা ভঙ্গ করে তিনি চার বেদ, ব্রন্মসূত্র, মহাভারত, ভাগবত', 'এবং গীতার মত গ্রন্থগুলিতে পূর্বসঞ্চিত ভৌতিক এবং আধ্যাত্মিক জ্ঞানরাশিকে', 'সঙ্কলিত করে শেষে নির্ণয় করলেন যে, সবেপিনিষদো গাবো দোঞ্ধা']\n",
            "\n",
            "--- Training BPE Tokenizer on English sentences ---\n",
            "English BPE vocab size: 6958\n",
            "\n",
            "--- Training BPE Tokenizer on Bengali sentences ---\n",
            "Bengali BPE vocab size: 4800\n",
            "Creating transformer model...\n",
            "\n",
            "=== PRE-TRAINING PHASE ===\n",
            "=== Starting Pre-training Phase ===\n",
            "Creating pre-training datasets...\n",
            "English pre-training samples: 3054\n",
            "Bengali pre-training samples: 10575\n",
            "Time elapsed: 0m 0s (phase), 0m 15s (overall)\n",
            "Training encoder (English) - Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoder MLM (English) Epoch 1: 100%|██████████| 48/48 [35:11<00:00, 43.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training decoder (Bengali) - Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Decoder MLM (Bengali) Epoch 1:   1%|          | 1/166 [00:59<2:42:26, 59.07s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === STEP 7: SAVE THE TRAINED MODEL ===\n",
        "# CONCEPT: Like saving a trained program to disk so you can use it later\n",
        "# OUTPUT: A file containing all the learned parameters and settings\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),  # All the learned weights/parameters\n",
        "    'config': CONFIG,                        # Training configuration\n",
        "    'src_vocab': src_vocab,                  # English tokenizer\n",
        "    'tgt_vocab': tgt_vocab,                  # Bengali tokenizer\n",
        "    'pretrain_losses': (pretrain_train_losses, pretrain_val_losses),  # Training history\n",
        "    'finetune_losses': (finetune_train_losses, finetune_val_losses),  # Training history\n",
        "    'pretrain_accs': pretrain_accs,          # Accuracy history\n",
        "    'train_accs': train_accs,                # Training accuracy\n",
        "    'val_accs': val_accs                     # Validation accuracy\n",
        "}, 'en_bn_transformer_pretrained_finetuned.pth')\n",
        "print(\"Model saved as 'en_bn_transformer_pretrained_finetuned.pth'\")\n"
      ],
      "metadata": {
        "id": "0n2BHGeTKB9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === STEP 9: TRANSLATION INFERENCE ===\n",
        "# CONCEPT: Now use the trained model to translate new English sentences\n",
        "# This simulates loading a saved model and using it in production\n",
        "\n",
        "\n",
        "print(\"\\n=== Translation Inference ===\")\n",
        "# Load the saved model (simulating a fresh start)\n",
        "# INPUT: Saved model file\n",
        "# OUTPUT: Restored model and tokenizers\n",
        "checkpoint = torch.load('en_bn_transformer_pretrained_finetuned.pth',\n",
        "                        map_location=CONFIG['device'],\n",
        "                        weights_only=False)\n",
        "\n",
        "# Restore the model's learned parameters\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "src_vocab = checkpoint['src_vocab']  # English tokenizer\n",
        "tgt_vocab = checkpoint['tgt_vocab']  # Bengali tokenizer\n",
        "\n",
        "# Create inference object for easy translation\n",
        "# INPUT: English text string\n",
        "# OUTPUT: Bengali text string\n",
        "# VECTOR OPERATIONS IN INFERENCE:\n",
        "# 1. Tokenize: \"Hello world\" → [15, 247, 2] (token IDs)\n",
        "# 2. Embed: [15, 247, 2] → [3, 512] (batch_size=1, seq_len=3, d_model=512)\n",
        "# 3. Encode: [3, 512] → [3, 512] (encoder output)\n",
        "# 4. Decode: Generate Bengali tokens one by one using encoder output\n",
        "# 5. Detokenize: [45, 123, 67, 2] → \"নমস্কার বিশ্ব\" (Bengali text)\n",
        "translator = TranslationInference(model, src_vocab, tgt_vocab, CONFIG)"
      ],
      "metadata": {
        "id": "G8KC9DAIJowz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n=== Testing Translations ===\")\n",
        "# Test dataset: Mix of simple and complex sentences\n",
        "# PURPOSE: Evaluate model performance on different types of input\n",
        "test_sentences = [\n",
        "    \"a child in a pink dress is climbing up a set of stairs in an entry way .\",\n",
        "    \"a girl going into a wooden building .\",\n",
        "    \"a dog is running in the snow\",\n",
        "    \"a dog running\",\n",
        "    \"Hello, how are you?\",\n",
        "    \"a man in an orange hat starring at something .\",\n",
        "    \"I love you.\",\n",
        "    \"a little girl climbing into a wooden playhouse .\",\n",
        "    \"What is your name?\",\n",
        "    \"two dogs of different breeds looking at each other on the road .\",\n",
        "    \"Good morning.\",\n",
        "    \"Thank you very much.\",\n",
        "    \"Hello, how are you?\",\n",
        "    \"I love you.\",\n",
        "    \"What is your name?\",\n",
        "    \"Good morning.\",\n",
        "    \"Thank you very much.\",\n",
        "    \"The weather is nice today.\"\n",
        "]\n",
        "\n",
        "# BATCH PROCESSING LOOP:\n",
        "# For each test sentence, perform the complete translation pipeline\n",
        "for sentence in test_sentences:\n",
        "    # TRANSLATION PIPELINE FOR EACH SENTENCE:\n",
        "    #\n",
        "    # STEP 1: INPUT PREPROCESSING\n",
        "    # INPUT: Raw English string (e.g., \"Hello, how are you?\")\n",
        "    # DIMENSIONS: Variable length string\n",
        "\n",
        "    # STEP 2: TOKENIZATION (inside translator.translate())\n",
        "    # INPUT: \"Hello, how are you?\" (string)\n",
        "    # OUTPUT: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102] (token IDs)\n",
        "    # DIMENSIONS: [sequence_length] where sequence_length varies per sentence\n",
        "\n",
        "    # STEP 3: EMBEDDING LOOKUP (inside model)\n",
        "    # INPUT: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102] (token IDs)\n",
        "    # OUTPUT: [1, 8, 512] (batch_size=1, sequence_length=8, d_model=512)\n",
        "    # OPERATION: Each token ID gets mapped to a 512-dimensional vector\n",
        "\n",
        "    # STEP 4: ENCODER PROCESSING (inside model)\n",
        "    # INPUT: [1, 8, 512] (embedded English tokens)\n",
        "    # OUTPUT: [1, 8, 512] (encoded representations)\n",
        "    # OPERATION: Self-attention + feed-forward layers process the input\n",
        "    # Each layer maintains the same dimensions: [batch_size, seq_len, d_model]\n",
        "\n",
        "    # STEP 5: DECODER PROCESSING (inside model)\n",
        "    # INPUT: [1, 8, 512] (encoder output) + [1, tgt_len, 512] (partial Bengali)\n",
        "    # OUTPUT: [1, tgt_len, bengali_vocab_size] (next token probabilities)\n",
        "    # OPERATION: Cross-attention between English and Bengali representations\n",
        "    # This happens iteratively: generate one Bengali token at a time\n",
        "\n",
        "    # STEP 6: TOKEN GENERATION (inside translator.translate())\n",
        "    # INPUT: [1, tgt_len, bengali_vocab_size] (probabilities for each position)\n",
        "    # OUTPUT: [tgt_len] (selected token IDs)\n",
        "    # OPERATION: Pick highest probability token at each position\n",
        "    # Example: [45, 123, 67, 89, 2] (where 2 is the end-of-sequence token)\n",
        "\n",
        "    # STEP 7: DETOKENIZATION (inside translator.translate())\n",
        "    # INPUT: [45, 123, 67, 89, 2] (Bengali token IDs)\n",
        "    # OUTPUT: \"আপনি কেমন আছেন?\" (Bengali text string)\n",
        "    # OPERATION: Convert token IDs back to readable text\n",
        "\n",
        "    translation = translator.translate(sentence)\n",
        "\n",
        "    # DISPLAY RESULTS:\n",
        "    # Show original English and translated Bengali side by side\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"Bengali: {translation}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # PERFORMANCE CONSIDERATION:\n",
        "    # Each translation involves:\n",
        "    # - Forward pass through encoder: O(sequence_length²) due to self-attention\n",
        "    # - Iterative decoding: O(output_length × sequence_length) for cross-attention\n",
        "    # - Memory usage: ~(sequence_length × d_model × batch_size) floats\n",
        "\n",
        "# === STEP 11: PERFORMANCE MONITORING ===\n",
        "# CONCEPT: Track total training and inference time for optimization\n",
        "# Important for production deployment planning\n",
        "\n",
        "overall_time_elapsed = time.time() - overall_start_time\n",
        "print(f\"\\nTotal elapsed time: {overall_time_elapsed // 60:.0f}m {overall_time_elapsed % 60:.0f}s\")\n",
        "\n",
        "# TIME BREAKDOWN ANALYSIS:\n",
        "# - Data loading: Usually fast (I/O bound)\n",
        "# - Tokenizer training: Medium (CPU bound, depends on corpus size)\n",
        "# - Model pretraining: Slow (GPU/CPU bound, depends on model size)\n",
        "# - Model fine-tuning: Medium (GPU/CPU bound, smaller dataset)\n",
        "# - Inference: Fast (single forward passes)\n",
        "\n",
        "print(\"\\n=== TRAINING COMPLETE ===\")\n",
        "print(\"Model is ready for production use!\")\n",
        "print(\"You can now use the 'translator' object to translate English to Bengali.\")"
      ],
      "metadata": {
        "id": "n-Bnwc0fCTMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}